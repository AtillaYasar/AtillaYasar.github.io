<html>
  <body>
    <head>
      <link rel="stylesheet" href="tw_conversion_alignment.css"></link>
    </head>
    
    <p>
      <img src="tw_screenshots/alignment.png"><p id="tammy-alignment">
      <p style="color:purple"> tammy-alignment </p>
      
      
      
      </p><p id="speed-vs-complexity">
      <p style="color:purple"> speed-vs-complexity </p>
      
      
      
      </p><p id="transformative-ai">
      <p style="color:purple"> transformative-ai </p>
      
      
      
      </p><p id="general-structure">
      <p style="color:purple"> general-structure </p>
      2 worlds: high and low path dependence, and the article explores "how the inductive biases could play out"
      
      [in low path dependence]
      And we're going to imagine that, actually, the inductive biases in machine learning are not very path dependent
      so each world has a different set of inductive biases? 
      no.
      the inductive biases are either path dependent or not. and whether they are or not, defines which world you are in.
      
      
      
      
      
      </p><p id="weights-vs-activations">
      <p style="color:purple"> weights-vs-activations </p>
      
      
      
      </p><p id="knowledge-types">
      <p style="color:purple"> knowledge-types </p>
      [[proxies-->proxies]]
      [[world-model-->world-model]]
      [[training-objective-->training-objective]]
      
      
      </p><p id="training-objective">
      <p style="color:purple"> training-objective </p>
      
      
      
      </p><p id="indistinguishable">
      <p style="color:purple"> indistinguishable </p>
      [[corrigible-alignment-->corrigible-alignment]]
      [[proxy-alignment-->internal-alignment]]
      [[deceptive-alignment-->deceptive-alignment]]
      
      
      </p><p id="core-concepts">
      <p style="color:purple"> core-concepts </p>
      [[low-path-dependence-->low-path-dependence]]
      [[high-path-dependence-->high-path-dependence]]
      
      [[pointer-->pointer]]
      [[overhang-->training-overhang]]
      [[tradeoff-->tradeoff]]
      [[inner-alignment-->inner-alignment]]
      [[corrigible-alignment-->corrigible-alignment]]
      [[knowledge-types-->knowledge-types]]
      [[proxy-alignment-->internal-alignment]]
      [[deceptive-alignment-->deceptive-alignment]]
      [[inductive-bias-->inductive-bias]]
      [[path-dependence-->path-dependence]]
      [[speed-vs-complexity-->speed-vs-complexity]]
      
      
      
      </p><p id="training-overhang">
      <p style="color:purple"> training-overhang </p>
      
      
      
      </p><p id="pointer">
      <p style="color:purple"> pointer </p>
      
      
      
      </p><p id="world-model">
      <p style="color:purple"> world-model </p>
      
      
      
      </p><p id="proxies">
      <p style="color:purple"> proxies </p>
      
      
      
      </p><p id="deceptive-alignment">
      <p style="color:purple"> deceptive-alignment </p>
      blaise pascal
      he doesnt really care about the bible, but he just wants to avoid hell. so hell do what it says and appear christian.
      
      
      
      [[inner-alignment-->inner-alignment]
      [[outer-alignment-->outer-alignment]
      [[mesa-optimization-->mesa-optimization]
      
      https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment
      slides:
      https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit
      
      
      </p><p id="path-dependence">
      <p style="color:purple"> path-dependence </p>
      
      
      
      </p><p id="internal-alignment">
      <p style="color:purple"> internal-alignment </p>
      jesus christ
      he is literally god. knows exactly what god and the bible wants.
      
      
      </p><p id="corrigible-alignment">
      <p style="color:purple"> corrigible-alignment </p>
      martin luther
      doesnt care about the church or what people say, only the bible. 
      
      "call the Martin Luthers corrigibly aligned, because they want to figure out what you want, and then do that"
      ok why are they unique in figuring out what you want?
      oh because jesus is a copy of god, he wants the same things, he doesnt have to figure them out
      
      so he wants to figure out the training objective, not necessarily what you want. hmm.
      
      
      </p><p id="inductive-bias">
      <p style="color:purple"> inductive-bias </p>
      inductive bias is about.. how will the model learn???
      assumptions about how to generalize, and what to make of things it sees.
      
      "A classical example of an inductive bias is Occam's razor,"
      "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1]"
      
      some common inductive biases
      - Maximum margin: when drawing a boundary between two classes, attempt to maximize the width of the boundary. This is the bias used in support vector machines. The assumption is that distinct classes tend to be separated by wide boundaries.
      - Minimum features: unless there is good evidence that a feature is useful, it should be deleted. This is the assumption behind feature selection algorithms.
      etc
      
       So in the high path dependence world, the correct way to think about the inductive biases in machine learning, is to think: well, we have to understand particular paths that your model might take through model space—maybe first you might get one thing
      
      So this is one way to think about inductive biases, where it really matters the particular path you take through model space, and how difficult that path is
      
       You give it a bunch of data, and it's always going to find the simplest way to fit that data. In that situation, what matters is the data that you gave it and some basic understanding of simplicity, a set of inductive biases that your training process came with.
      
      One way to think about this is: your model space is so high-dimensional that your training process can essentially access the whole manifold of minimal loss solutions, and then it just picks the one that's the simplest according to some set of inductive biases.
      
      ^^ so, "some understanding of simplicity" is what your inductive bias says "simplicity" means?
      
      And we're going to imagine that, actually, the inductive biases in machine learning are not very path dependent
      
      "for low path dependence worlds, inductive biases are some comination of simplicity and speed"
      
      "approximate the simplicity priors by looking at the simplest one in each class"
      
      
      </p><p id="what-is-AI-thinking">
      <p style="color:purple"> what-is-AI-thinking </p>
      
      
      
      </p><p id="mesa-optimization">
      <p style="color:purple"> mesa-optimization </p>
      " a mesa-objective, which is just something that it’s trying to optimize for"
      
      
      </p><p id="outer-alignment">
      <p style="color:purple"> outer-alignment </p>
      
      
      
      </p><p id="value-learning">
      <p style="color:purple"> value-learning </p>
      
      
      
      </p><p id="alignment-minetest">
      <p style="color:purple"> alignment-minetest </p>
      in pins: https://docs.google.com/document/d/1Oz8neKkXYaWzmRg9ZeYqlFIgYENUkHLhcviDG2hQRiE/edit?usp=sharing
      
      "The goal of this project is to provide a rich and easily moddable environment that alignment researchers can use to test many aspects of alignment and alignment techniques."
      
      
      </p><p id="aesthetic-models">
      <p style="color:purple"> aesthetic-models </p>
      
      
      
      </p><p id="accelerating-alignment">
      <p style="color:purple"> accelerating-alignment </p>
      In terms of concrete tools, we're thinking about things like:
      (HITL = human in the loop)
      - HITL interface to explore LM generations about alignment work
      - HITL interface to help alignment researchers write more quickly (e.g. expand bullet points, rewrite text, etc.)
      - Tools to automatically generate alignment forum content like posts/comments from particular users and on particular topics (possibly integrated with LW/AF via the website, and extension, or a mirror)
      - A mirror LW/AF website filled with autogenerated content (like https://reddit.com/r/SubSimulatorGPT2/)
      
      
      </p><p id="agent-foundations">
      <p style="color:purple"> agent-foundations </p>
      in channel description:
      Decision Theory, Embedded Agency, Corrigibility, etc. Introduction (Warning: Math): 
      https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh
      
      
      </p><p id="prosaic-alignment">
      <p style="color:purple"> prosaic-alignment </p>
      
      
      
      </p><p id="EAI-channels">
      <p style="color:purple"> EAI-channels </p>
      [[prosaic-alignment-->prosaic-alignment]]
      [[agent-foundations-->agent-foundations]]
      [[accelerating-alignment-->accelerating-alignment]]
      [[aesthetic-models-->aesthetic-models]]
      [[alignment-minetest-->alignment-minetest]]
      
      
      </p><p id="oracle-AI">
      <p style="color:purple"> oracle-AI </p>
      
      
      
      </p><p id="general-alignment">
      <p style="color:purple"> general-alignment </p>
      
      
      
      </p><p id="to-read">
      <p style="color:purple"> to-read </p>
      
      
      
      </p><p id="decision-theory">
      <p style="color:purple"> decision-theory </p>
      
      
      
      </p><p id="too-niche-or-too-hard">
      <p style="color:purple"> too-niche-or-too-hard </p>
      
      
      
      </p><p id="failing-strategies">
      <p style="color:purple"> failing-strategies </p>
      
      
      
      </p><p id="AIXI">
      <p style="color:purple"> AIXI </p>
      
      
      
      </p><p id="teaching-AI-values">
      <p style="color:purple"> teaching-AI-values </p>
      
      
      
      </p><p id="complexity-of-value">
      <p style="color:purple"> complexity-of-value </p>
      
      
      
      </p><p id="embedded-agency">
      <p style="color:purple"> embedded-agency </p>
      
      
      
      </p><p id="agency-theory">
      <p style="color:purple"> agency-theory </p>
      
      
      
      </p><p id="RLHF">
      <p style="color:purple"> RLHF </p>
      
      
      
      </p><p id="interpretability">
      <p style="color:purple"> interpretability </p>
      
      
      
      </p><p id="understanding-AI">
      <p style="color:purple"> understanding-AI </p>
      
      
      
      </p><p id="goodharts-law">
      <p style="color:purple"> goodharts-law </p>
      
      
      
      </p><p id="reward-functions">
      <p style="color:purple"> reward-functions </p>
      
      </p>
    </p>
    
    <div class="sidenav">
      <tw_conversion_winning>
        <a href="#alignment-minetest">alignment-minetest</a>
        <a href="#aesthetic-models">aesthetic-models</a>
        <a href="#accelerating-alignment">accelerating-alignment</a>
        <a href="#agent-foundations">agent-foundations</a>
        <a href="#AIXI">AIXI</a>
        <a href="#agency-theory">agency-theory</a>
        <a href="#core-concepts">core-concepts</a>
        <a href="#corrigible-alignment">corrigible-alignment</a>
        <a href="#complexity-of-value">complexity-of-value</a>
        <a href="#deceptive-alignment">deceptive-alignment</a>
        <a href="#decision-theory">decision-theory</a>
        <a href="#EAI-channels">EAI-channels</a>
        <a href="#embedded-agency">embedded-agency</a>
        <a href="#failing-strategies">failing-strategies</a>
        <a href="#general-structure">general-structure</a>
        <a href="#general-alignment">general-alignment</a>
        <a href="#goodharts-law">goodharts-law</a>
        <a href="#indistinguishable">indistinguishable</a>
        <a href="#internal-alignment">internal-alignment</a>
        <a href="#inductive-bias">inductive-bias</a>
        <a href="#interpretability">interpretability</a>
        <a href="#knowledge-types">knowledge-types</a>
        <a href="#mesa-optimization">mesa-optimization</a>
        <a href="#outer-alignment">outer-alignment</a>
        <a href="#oracle-AI">oracle-AI</a>
        <a href="#pointer">pointer</a>
        <a href="#proxies">proxies</a>
        <a href="#path-dependence">path-dependence</a>
        <a href="#prosaic-alignment">prosaic-alignment</a>
        <a href="#RLHF">RLHF</a>
        <a href="#reward-functions">reward-functions</a>
        <a href="#speed-vs-complexity">speed-vs-complexity</a>
        <a href="#tammy-alignment">tammy-alignment</a>
        <a href="#transformative-ai">transformative-ai</a>
        <a href="#training-objective">training-objective</a>
        <a href="#training-overhang">training-overhang</a>
        <a href="#to-read">to-read</a>
        <a href="#too-niche-or-too-hard">too-niche-or-too-hard</a>
        <a href="#teaching-AI-values">teaching-AI-values</a>
        <a href="#understanding-AI">understanding-AI</a>
        <a href="#value-learning">value-learning</a>
        <a href="#weights-vs-activations">weights-vs-activations</a>
        <a href="#world-model">world-model</a>
        <a href="#what-is-AI-thinking">what-is-AI-thinking</a>
      </tw_conversion_winning>
    </div>
  </body>
</html>