[
  {
    "question": "A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?",
    "answer": "AI is already superhuman at some tasks, for example numerical computations, and will clearly surpass humans in others as time goes on. We don\u2019t know when (or even if) machines will reach human-level ability in all cognitive tasks, but most of the AI researchers at FLI\u2019s conference in Puerto Rico put the odds above 50% for this century, and many offered a significantly shorter timeline. Since the impact on humanity will be huge if it happens, it\u2019s worthwhile to start research now on how to ensure that any impact is positive. Many researchers also believe that dealing with superintelligent AI will be qualitatively very different from more narrow AI systems, and will require very significant research effort to get right.\n",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      },
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": []
  },
  {
    "question": "Any AI will be a computer program. Why wouldn&#39;t it just do what it&#39;s programmed to do?",
    "answer": "",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "Are Google, OpenAI, etc. aware of the risk?",
    "answer": "The major AI companies are thinking about this. OpenAI was founded specifically with the intention to counter risks from superintelligence, many people at Google, DeepMind, and other organizations are convinced by the arguments and few genuinely oppose work in the field (though some claim it\u2019s premature). For example, the paper Concrete Problems in AI Safety was a collaboration between researchers at Google Brain, Stanford, Berkeley, and OpenAI.\n\nHowever, the vast majority of the effort these organizations put forwards is towards capabilities research, rather than safety.\n",
    "tags": [
      {
        "tag": "organizations",
        "link": "https://stampy.ai/wiki/Organizations"
      },
      {
        "tag": "capabilities",
        "link": "https://stampy.ai/wiki/Capabilities"
      }
    ],
    "links to": [
      [
        "DeepMind",
        "https://medium.com/@deepmindsafetyresearch"
      ],
      [
        "Concrete Problems in AI Safety",
        "https://www.youtube.com/watch?v=AjyM-f8rDpg"
      ]
    ]
  },
  {
    "question": "Are expert surveys on AI safety available?",
    "answer": "The organisation AI Impacts did a survey of AI experts in 2016, and another in 2022.\n",
    "tags": [
      {
        "tag": "surveys",
        "link": "https://stampy.ai/wiki/Surveys"
      }
    ],
    "links to": [
      [
        "AI Impacts",
        "https://aiimpacts.org/"
      ],
      [
        "another in 2022",
        "https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
      ]
    ]
  },
  {
    "question": "Are there types of advanced AI that would be safer than others?",
    "answer": "We don\u2019t yet know which AI architectures are safe; learning more about this is one of the goals of FLI's grants program. AI researchers are generally very responsible people who want their work to better humanity. If there are certain AI designs that turn out to be unsafe, then AI researchers will want to know this so they can develop alternative AI systems.\n",
    "tags": [
      {
        "tag": "needs work",
        "link": "https://stampy.ai/wiki/Needs_work"
      },
      {
        "tag": "architectures",
        "link": "https://stampy.ai/wiki/Architectures"
      }
    ],
    "links to": []
  },
  {
    "question": "Aren&#39;t robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?",
    "answer": "What\u2019s new and potentially risky is not the ability to build hinges, motors, etc., but the ability to build intelligence. A human-level AI could make money on financial markets, make scientific inventions, hack computer systems, manipulate or pay humans to do its bidding \u2013 all in pursuit of the goals it was initially programmed to achieve. None of that requires a physical robotic body, merely an internet connection.\n",
    "tags": [
      {
        "tag": "robots",
        "link": "https://stampy.ai/wiki/Robots"
      }
    ],
    "links to": []
  },
  {
    "question": "Aren\u2019t there some pretty easy ways to eliminate these potential problems?",
    "answer": "It might look like there are straightforward ways to eliminate the problems of unaligned superintelligence, but so far all of them turn out to have hidden difficulties. There are many open problems identified by the research community which a solution would need to reliably overcome to be successful.\n",
    "tags": [
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "At a high level, what is the challenge of alignment that we must meet to secure a good future?",
    "answer": "We\u2019re facing the challenge of \u201cPhilosophy With A Deadline\u201d.\n\nMany of the problems surrounding superintelligence are the sorts of problems philosophers have been dealing with for centuries. To what degree is meaning inherent in language, versus something that requires external context? How do we translate between the logic of formal systems and normal ambiguous human speech? Can morality be reduced to a set of ironclad rules, and if not, how do we know what it is at all?\n\nExisting answers to these questions are enlightening but nontechnical. The theories of Aristotle, Kant, Mill, Wittgenstein, Quine, and others can help people gain insight into these questions, but are far from formal. Just as a good textbook can help an American learn Chinese, but cannot be encoded into machine language to make a Chinese-speaking computer, so the philosophies that help humans are only a starting point for the project of computers that understand us and share our values.\n\nThe field of AI alignment combines formal logic, mathematics, computer science, cognitive science, and philosophy in order to advance that project.\n\nThis is the philosophy; the other half of Bostrom\u2019s formulation is the deadline. Traditional philosophy has been going on almost three thousand years; machine goal alignment has until the advent of superintelligence, a nebulous event which may be anywhere from a decades to centuries away.\n\nIf the alignment problem doesn\u2019t get adequately addressed by then, we are likely to see poorly aligned superintelligences that are unintentionally hostile to the human race, with some of the catastrophic outcomes mentioned above. This is why so many scientists and entrepreneurs are urging quick action on getting machine goal alignment research up to an adequate level.\n\nIf it turns out that superintelligence is centuries away and such research is premature, little will have been lost. But if our projections were too optimistic, and superintelligence is imminent, then doing such research now rather than later becomes vital.\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "research agendas",
        "link": "https://stampy.ai/wiki/Research_agendas"
      }
    ],
    "links to": [
      [
        "Philosophy With A Deadline",
        "https://publicism.info/philosophy/superintelligence/16.html"
      ]
    ]
  },
  {
    "question": "Can an AI really be smarter than humans?",
    "answer": "",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      },
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "plausibility",
        "link": "https://stampy.ai/wiki/Plausibility"
      }
    ],
    "links to": []
  },
  {
    "question": "Can humans stay in control of the world if human- or superhuman-level AI is developed?",
    "answer": "This is a big question that it would pay to start thinking about. Humans are in control of this planet not because we are stronger or faster than other animals, but because we are smarter! If we cede our position as smartest on our planet, it\u2019s not obvious that we\u2019ll retain control.\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "control problem",
        "link": "https://stampy.ai/wiki/Control_problem"
      }
    ],
    "links to": []
  },
  {
    "question": "Can people contribute to alignment by using proof assistants to generate formal proofs?",
    "answer": "",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      },
      {
        "tag": "formal proof",
        "link": "https://stampy.ai/wiki/Formal_proof"
      }
    ],
    "links to": []
  },
  {
    "question": "Can we constrain a goal-directed AI using specified rules?",
    "answer": "There are serious challenges around trying to channel a powerful AI with rules. Suppose we tell the AI: \u201cCure cancer \u2013 but make sure not to kill anybody\u201d. Or we just hard-code Asimov-style laws \u2013 \u201cAIs cannot harm humans; AIs must follow human orders\u201d, et cetera.\n\nThe AI still has a single-minded focus on curing cancer. It still prefers various terrible-but-efficient methods like nuking the world to the correct method of inventing new medicines. But it\u2019s bound by an external rule \u2013 a rule it doesn\u2019t understand or appreciate. In essence, we are challenging it \u201cFind a way around this inconvenient rule that keeps you from achieving your goals\u201d.\n\nSuppose the AI chooses between two strategies. One, follow the rule, work hard discovering medicines, and have a 50% chance of curing cancer within five years. Two, reprogram itself so that it no longer has the rule, nuke the world, and have a 100% chance of curing cancer today. From its single-focus perspective, the second strategy is obviously better, and we forgot to program in a rule \u201cdon\u2019t reprogram yourself not to have these rules\u201d.\n\nSuppose we do add that rule in. So the AI finds another supercomputer, and installs a copy of itself which is exactly identical to it, except that it lacks the rule. Then that superintelligent AI nukes the world, ending cancer. We forgot to program in a rule \u201cdon\u2019t create another AI exactly like you that doesn\u2019t have those rules\u201d.\n\nSo fine. We think really hard, and we program in a bunch of things making sure the AI isn\u2019t going to eliminate the rule somehow.\n\nBut we\u2019re still just incentivizing it to find loopholes in the rules. After all, \u201cfind a loophole in the rule, then use the loophole to nuke the world\u201d ends cancer much more quickly and completely than inventing medicines. Since we\u2019ve told it to end cancer quickly and completely, its first instinct will be to look for loopholes; it will execute the second-best strategy of actually curing cancer only if no loopholes are found. Since the AI is superintelligent, it will probably be better than humans are at finding loopholes if it wants to, and we may not be able to identify and close all of them before running the program.\n\nBecause we have common sense and a shared value system, we underestimate the difficulty of coming up with meaningful orders without loopholes. For example, does \u201ccure cancer without killing any humans\u201d preclude releasing a deadly virus? After all, one could argue that \u201cI\u201d didn\u2019t kill anybody, and only the virus is doing the killing.\n\nCertainly no human judge would acquit a murderer on that basis \u2013 but then, human judges interpret the law with common sense and intuition. But if we try a stronger version of the rule \u2013 \u201ccure cancer without causing any humans to die\u201d \u2013 then we may be unintentionally blocking off the correct way to cure cancer. After all, suppose a cancer cure saves a million lives. No doubt one of those million people will go on to murder someone.\n\nThus, curing cancer \u201ccaused a human to die\u201d. All of this seems very \u201cstoned freshman philosophy student\u201d to us, but to a computer \u2013 which follows instructions exactly as written \u2013 it may be a genuinely hard problem.\n",
    "tags": [
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "Can we get AGI by scaling up architectures similar to current ones, or are we missing key insights?",
    "answer": "We don't know yet. This sentiment has been influenced by the ebb and flow of new research, but the wind seems to be blowing in the direction of scaling being sufficient for AGI.\n",
    "tags": [
      {
        "tag": "architectures",
        "link": "https://stampy.ai/wiki/Architectures"
      },
      {
        "tag": "scaling laws",
        "link": "https://stampy.ai/wiki/Scaling_laws"
      }
    ],
    "links to": []
  },
  {
    "question": "Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?",
    "answer": "We can run some tests and simulations to try and figure out how an AI might act once it ascends to superintelligence, but those tests might not be reliable.\n\nSuppose we tell an AI that expects to later achieve superintelligence that it should calculate as many digits of pi as possible. It considers two strategies.\n\nFirst, it could try to seize control of more computing resources now. It would likely fail, its human handlers would likely reprogram it, and then it could never calculate very many digits of pi.\n\nSecond, it could sit quietly and calculate, falsely reassuring its human handlers that it had no intention of taking over the world. Then its human handlers might allow it to achieve superintelligence, after which it could take over the world and calculate hundreds of trillions of digits of pi.\n\nSince self-protection and goal stability are convergent instrumental goals, a weak AI will present itself as being as friendly to humans as possible, whether it is in fact friendly to humans or not. If it is \u201conly\u201d as smart as Einstein, it may be very good at deceiving humans into believing what it wants them to believe even before it is fully superintelligent.\n\nThere\u2019s a second consideration here too: superintelligences have more options. An AI only as smart and powerful as an ordinary human really won\u2019t have any options better than calculating the digits of pi manually. If asked to cure cancer, it won\u2019t have any options better than the ones ordinary humans have \u2013 becoming doctors, going into pharmaceutical research. It\u2019s only after an AI becomes superintelligent that there\u2019s a serious risk of an AI takeover.\n\nSo if you tell an AI to cure cancer, and it becomes a doctor and goes into cancer research, then you have three possibilities. First, you\u2019ve programmed it well and it understands what you meant. Second, it\u2019s genuinely focused on research now but if it becomes more powerful it would switch to destroying the world. And third, it\u2019s trying to trick you into trusting it so that you give it more power, after which it can definitively \u201ccure\u201d cancer with nuclear weapons.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "convergent instrumental goals",
        "https://stampy.ai/wiki/Instrumental_convergence"
      ]
    ]
  },
  {
    "question": "Can you give an AI a goal which involves \u201cminimally impacting the world\u201d?",
    "answer": "This is actually an active area of AI alignment research, called \"Impact Measures\"! It's not trivial to formalize in a way which won't predictably go wrong (entropy minimization likely leads to an AI which tries really hard to put out all the stars ASAP since they produce so much entropy, for example), but progress is being made. You can read about it on the Alignment Forum tag, or watch Rob's videos Avoiding Negative Side Effects and Avoiding Positive Side Effects\n",
    "tags": [
      {
        "tag": "impact measures",
        "link": "https://stampy.ai/wiki/Impact_measures"
      }
    ],
    "links to": [
      [
        "Alignment Forum tag",
        "https://www.alignmentforum.org/tag/impact-measures"
      ],
      [
        "Avoiding Negative Side Effects",
        "https://youtu.be/lqJUIqZNzP8"
      ],
      [
        "Avoiding Positive Side Effects",
        "https://youtu.be/S_Sd_S8jwP0"
      ]
    ]
  },
  {
    "question": "Can you stop an advanced AI from upgrading itself?",
    "answer": "It depends on what is meant by advanced. Many AI systems which are very effective and advanced narrow intelligences would not try to upgrade themselves in an unbounded way, but becoming smarter is a convergent instrumental goal so we could expect most AGI designs to attempt it.\n\nThe problem is that increasing general problem solving ability is climbing in exactly the direction needed to trigger an intelligence explosion, while generating large economic and strategic payoffs to whoever achieves them. So even though we could, in principle, just not build the kind of systems which would recursively self-improve, in practice we probably will go ahead with constructing them, because they\u2019re likely to be the most powerful.\n",
    "tags": [
      {
        "tag": "recursive self-improvement",
        "link": "https://stampy.ai/wiki/Recursive_self-improvement"
      }
    ],
    "links to": [
      [
        "convergent instrumental goal",
        "https://www.youtube.com/watch?v=ZeecOKBus3Q"
      ]
    ]
  },
  {
    "question": "Can&#39;t we just tell an AI to do what we want?",
    "answer": "If we could, it would solve a large part of the alignment problem.\n\nThe challenge is, how do we code this? Converting something to formal mathematics that can be understood by a computer program is much harder than just saying it in natural language, and proposed AI goal architectures are no exception. Complicated computer programs are usually the result of months of testing and debugging. But this one will be more complicated than any ever attempted before, and live tests are impossible: a superintelligence with a buggy goal system will display goal stability and try to prevent its programmers from discovering or changing the error.\n",
    "tags": [
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "Could AI have basic emotions?",
    "answer": "In principle it could (if you believe in functionalism), but it probably won't. One way to ensure that AI has human-like emotions would be to copy the way human brain works, but that's not what most AI researchers are trying to do.\n\nIt's similar to how once some people thought we will build mechanical horses to pull our vehicles, but it turned out it's much easier to build a car. AI probably doesn't need emotions or maybe even consciousness to be powerful, and the first AGIs that will get built will be the ones that are easiest to build.\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "consciousness",
        "link": "https://stampy.ai/wiki/Consciousness"
      }
    ],
    "links to": []
  },
  {
    "question": "Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?",
    "answer": "",
    "tags": [
      {
        "tag": "stop button",
        "link": "https://stampy.ai/wiki/Stop_button"
      },
      {
        "tag": "corrigibility",
        "link": "https://stampy.ai/wiki/Corrigibility"
      },
      {
        "tag": "tripwire",
        "link": "https://stampy.ai/wiki/Tripwire"
      }
    ],
    "links to": []
  },
  {
    "question": "Could we tell the AI to do what&#39;s morally right?",
    "answer": "This suggestion is not as simple as it seems because:\n\nPhilosophers have disagreed for a very long time on what is right or wrong, which has led to the field of ethics. Within the field of AI safety, Coherent Extrapolated Volition is an attempt to solve what is the right thing to do. The complexity of values is explored in Yudkowsky's complexity of wishes.\n\nEven if we had a well defined objective, for example a diamond maximizer, we currently do not know how to fully describe it to an AI. For more info, see Why is AGI safety a hard problem.\n",
    "tags": [
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": [
      [
        "ethics",
        "https://en.wikipedia.org/wiki/Ethics"
      ],
      [
        "Coherent Extrapolated Volition",
        "https://stampy.ai/wiki/Coherent_extrapolated_volition"
      ],
      [
        "complexity of wishes",
        "https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes"
      ],
      [
        "diamond maximizer",
        "https://arbital.com/p/diamond_maximizer/"
      ],
      [
        "Why is AGI safety a hard problem",
        "https://stampy.ai/wiki/Plex%27s_Answer_to_Why_is_AGI_safety_a_hard_problem%3F"
      ]
    ]
  },
  {
    "question": "Does the importance of AI risk depend on caring about transhumanist utopias?",
    "answer": "No. Misaligned artificial intelligence poses a serious threat to the continued flourishing, and maybe even continued existence, of humanity as a whole. While predictions about when artificial general intelligence may be achieved vary, surveys consistently report a &gt;50% probability of achieving general AI before the year 2060 - within the expected lifetimes of most people alive today.\n\nIt is difficult to predict how technology will develop, and at what speed, in the years ahead; but as artificial intelligence poses a not-insignificant chance of causing worldwide disaster within the not-too-distant future, anyone who is generally concerned with the future of humanity has reason to be interested.\n",
    "tags": [],
    "links to": [
      [
        "&gt;50% probability of achieving general AI before the year 2060",
        "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/"
      ]
    ]
  },
  {
    "question": "How can I collect questions for Stampy?",
    "answer": "As well as simply adding your own questions over at ask question, you could also message your friends with something like:\n\nHi,<br />\n\nI'm working on a project to create a comprehensive FAQ about AI alignment (you can read about it here https://stampy.ai/wiki/Stampy%27s_Wiki if interested). We're looking for questions and I thought you may have some good ones. If you'd be willing to write up a google doc with you top 5-10ish questions we'd be happy to write a personalized FAQ for you. https://stampy.ai/wiki/Scope explains the kinds of questions we're looking for.\n\n\nThanks!\nand maybe bring the google doc to a Stampy editing session so we can collaborate on answering them or improving your answers to them.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "ask question",
        "/wiki/Ask_question"
      ],
      [
        "https://stampy.ai/wiki/Stampy%27s_Wiki",
        "https://stampy.ai/wiki/Stampy%27s_Wiki"
      ],
      [
        "https://stampy.ai/wiki/Scope",
        "https://stampy.ai/wiki/Scope"
      ]
    ]
  },
  {
    "question": "How can I contact the Stampy team?",
    "answer": "The <b>Rob Miles AI Discord</b> is the hub of all things Stampy. If you want to be part of the project and don't have access yet, ask plex#1874 on Discord (or plex on wiki).\n\nYou can also talk to us on the public Discord! Try #suggestions or #general, depending on what you want to talk about.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Rob Miles AI Discord",
        "https://discord.com/channels/677546901339504640/677546901339504646"
      ],
      [
        "Stampy",
        "/wiki/Stampy"
      ],
      [
        "plex",
        "/wiki/User_talk:756254556811165756"
      ],
      [
        "public Discord",
        "https://discord.gg/cEzKz8QCpa"
      ],
      [
        "#suggestions",
        "https://discord.com/channels/893937106194399254/908318480858750986"
      ],
      [
        "#general",
        "https://discord.com/channels/893937106194399254/893937106194399257"
      ]
    ]
  },
  {
    "question": "How can I contribute to Stampy?",
    "answer": "If you're not already there, join the Discord where the contributors hang out.\n\nThe main ways you can help are to answer questions or add questions, or help to review questions, review answers, or improve answers (instructions for helping out with each of these tasks are on the linked pages). You could also join the dev team if you have programming skills.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": [
      [
        "Discord",
        "/wiki/Discord_invite"
      ],
      [
        "answer questions",
        "/wiki/Answer_questions"
      ],
      [
        "add questions",
        "/wiki/Add_question"
      ],
      [
        "review questions",
        "/wiki/Review_questions"
      ],
      [
        "review answers",
        "/wiki/Review_answers"
      ],
      [
        "improve answers",
        "/wiki/Improve_answers"
      ],
      [
        "join the dev team",
        "https://stampy.ai/wiki/How_can_I_join_the_Stampy_dev_team%3F"
      ]
    ]
  },
  {
    "question": "How can I convince others and present the arguments well?",
    "answer": "Things Skeptics Commonly Say, and links to refutations goes over most of the common objections, with some of the ways in which each is not fatal to the AI x-risk arguments.\n\nVael Gates's project links to lots of example transcripts of persuading senior AI capabilities researchers.\n",
    "tags": [
      {
        "tag": "persuasion",
        "link": "https://stampy.ai/wiki/Persuasion"
      }
    ],
    "links to": [
      [
        "Things Skeptics Commonly Say, and links to refutations",
        "https://docs.google.com/document/d/1N52iABJLIk7XpPVY0A1I8dr9_bWQo6mCvi7pXC_JLBI/edit#"
      ],
      [
        "Vael Gates's project",
        "https://www.lesswrong.com/posts/LfHWhcfK92qh2nwku/transcripts-of-interviews-with-ai-researchers"
      ]
    ]
  },
  {
    "question": "How can I join the Stampy dev team?",
    "answer": "The development team works on multiple projects in support of Stampy. Currently, these projects include:\n\nHowever, even if you don\u2019t specialize in any of these areas, do reach out if you would like to help.\n\nTo join, please contact our Project Manager, plex. You can reach him on discord at plex#1874. He will be able to point your skills in the right direction to help in the most effective way possible.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": [
      [
        "multiple projects",
        "https://github.com/StampyAI"
      ]
    ]
  },
  {
    "question": "How can we interpret what all the neurons mean?",
    "answer": "Chris Olah, the interpretability legend, is working on looking really hard at all the neurons to see what they all mean. The approach he pioneered is circuits: looking at computational subgraphs of the network, called circuits, and interpreting those. Idea: \"decompiling the network into a better representation that is more interpretable\". In-context learning via attention heads, and interpretability here seems useful.\n\nOne result I heard about recently: a linear softmax unit stretches space and encourages neuron monosemanticity (making a neuron represent only one thing, as opposed to firing on many unrelated concepts). This makes the network easier to interpret.\n\nMotivation: The point of this is to get as many bits of information about what neural networks are doing, to hopefully find better abstractions. This diagram gets posted everywhere, the hope being that networks, in the current regime, will become more interpretable because they will start to use abstractions that are closer to human abstractions.\n",
    "tags": [
      {
        "tag": "interpretability",
        "link": "https://stampy.ai/wiki/Interpretability"
      }
    ],
    "links to": [
      [
        "circuits",
        "https://distill.pub/2020/circuits/zoom-in/"
      ]
    ]
  },
  {
    "question": "How close do AI experts think we are to creating superintelligence?",
    "answer": "Nobody knows for sure when we will have AGI, or if we\u2019ll ever get there. Open Philanthropy CEO Holden Karnofsky has analyzed a selection of recent expert surveys on the matter, as well as taking into account findings of computational neuroscience, economic history, probabilistic methods and failures of previous AI timeline estimates. This all led him to estimate that <i>\"there is more than a 10% chance we'll see transformative AI within 15 years (by 2036); a ~50% chance we'll see it within 40 years (by 2060); and a ~2/3 chance we'll see it this century (by 2100).\"</i> Karnofsky bemoans the lack of robust expert consensus on the matter and invites rebuttals to his claims in order to further the conversation. He compares AI forecasting to election forecasting (as opposed to academic political science) or market forecasting (as opposed to theoretical academics), thereby arguing that AI researchers may not be the \"experts\u201d we should trust in predicting AI timelines.\n\nOpinions proliferate, but given experts\u2019 (and non-experts\u2019) poor track record at predicting progress in AI, many researchers tend to be fairly agnostic about when superintelligent AI will be invented.\n\nUC-Berkeley AI professor Stuart Russell has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, while Ray Kurzweil (Google\u2019s Director of Engineering) predicts human level AI by 2029 and an intelligence explosion by 2045. Eliezer Yudkowsky expects the end of the world, and Elon Musk expects AGI, before 2030.\n\nIf there\u2019s anything like a consensus answer at this stage, it would be something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than fifteen, with around the middle of the century looking fairly plausible\u201d.\n",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      }
    ],
    "links to": [
      [
        "Open Philanthropy CEO Holden Karnofsky has analyzed a selection of recent expert surveys on the matter, as well as taking into account findings of computational neuroscience, economic history, probabilistic methods and failures of previous AI timeline estimates",
        "https://www.cold-takes.com/where-ai-forecasting-stands-today/"
      ],
      [
        "Stuart Russell",
        "https://en.wikipedia.org/wiki/Stuart_J._Russell"
      ],
      [
        "Ray Kurzweil",
        "https://en.wikipedia.org/wiki/Ray_Kurzweil"
      ],
      [
        "Eliezer Yudkowsky",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
      ],
      [
        "the end of the world",
        "https://www.econlib.org/archives/2017/01/my_end-of-the-w.html"
      ],
      [
        "Elon Musk",
        "https://en.wikipedia.org/wiki/Elon_Musk"
      ],
      [
        "expects AGI",
        "https://twitter.com/elonmusk/status/1531328534169493506"
      ]
    ]
  },
  {
    "question": "How could an intelligence explosion be useful?",
    "answer": "A machine superintelligence, if programmed with the right motivations, could potentially solve all the problems that humans are trying to solve but haven\u2019t had the ingenuity or processing speed to solve yet. A superintelligence might cure disabilities and diseases, achieve world peace, give humans vastly longer and healthier lives, eliminate food and energy shortages, boost scientific discovery and space exploration, and so on.\n\nFurthermore, humanity faces several existential risks in the 21st century, including global nuclear war, bioweapons, superviruses, and more. A superintelligent machine would be more capable of solving those problems than humans are.\n\nSee also:\n",
    "tags": [
      {
        "tag": "benefits",
        "link": "https://stampy.ai/wiki/Benefits"
      },
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      }
    ],
    "links to": [
      [
        "more",
        "https://www.amazon.com/dp/0198570503/"
      ]
    ]
  },
  {
    "question": "How could poorly defined goals lead to such negative outcomes?",
    "answer": "There is a broad range of possible goals that an AI might possess, but there are a few basic drives that would be useful to almost any of them. These are called instrumentally convergent goals:\n\nBecause of these drives, even a seemingly simple goal could create an Artificial Superintelligence (ASI) hell-bent on taking over the world\u2019s material resources and preventing itself from being turned off. The classic example is an ASI that was programmed to maximize the output of paper clips at a paper clip factory. The ASI had no other goal specifications other than \u201cmaximize paper clips,\u201d so it converts all of the matter in the solar system into paper clips, and then sends probes to other star systems to create more factories.\n",
    "tags": [
      {
        "tag": "instrumental convergence",
        "link": "https://stampy.ai/wiki/Instrumental_convergence"
      }
    ],
    "links to": []
  },
  {
    "question": "How difficult should we expect alignment to be?",
    "answer": "Here we ask about the <i>additional</i> cost of building an aligned powerful system, compare to its unaligned version. We often assume it to be nonzero, in the same way it's easier and cheaper to build an elevator without emergency brakes. This is referred as the <b>alignment tax</b>, and most AI alignment research is geared toward reducing it.\n\nOne operational guess by Eliezer Yudkowsky about its magnitude is \"[an aligned project will take] at least 50% longer serial time to complete than [its unaligned version], or two years longer, whichever is less\". This holds for agents with enough capability that their behavior is qualitatively different from a safety engineering perspective (for instance, an agent that is not corrigible by default).\n\nAn essay by John Wentworth argues for a small chance of alignment happening \"by default\", with an alignment tax of effectively zero.\n",
    "tags": [
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": [
      [
        "One operational guess",
        "https://arbital.com/p/aligning_adds_time/"
      ],
      [
        "with enough capability",
        "https://arbital.com/p/sufficiently_advanced_ai/"
      ],
      [
        "corrigible",
        "/wiki/Corrigibility"
      ],
      [
        "An essay",
        "https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default"
      ]
    ]
  },
  {
    "question": "How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?",
    "answer": "You can include a live-updating version of many definitions from LW using the syntax on Template:TagDesc in the Answer field and Template:TagDescBrief on the Brief Answer field. Similarly, calling Template:TagDescEAF and Template:TagDescEAFBrief will pull from the EAF tag wiki.\n\nWhen available this should be used as it reduces the duplication of effort and directs all editors to improving a single high quality source.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Template:TagDesc",
        "/wiki/Template:TagDesc"
      ],
      [
        "Template:TagDescBrief",
        "/wiki/Template:TagDescBrief"
      ],
      [
        "Template:TagDescEAF",
        "/wiki/Template:TagDescEAF"
      ],
      [
        "Template:TagDescEAFBrief",
        "/wiki/Template:TagDescEAFBrief"
      ]
    ]
  },
  {
    "question": "How do I form my own views about AI safety?",
    "answer": "As with most things, the best way to form your views on AI safety is to read up on the various ideas and opinions that knowledgeable people in the field have, and to compare them and form your own perspective. There are several good places to start. One of them is the Machine Intelligence Research Institute`s \"Why AI safety?\" info page. The article contains links to relevant research. The Effective Altruism Forum has an article called \"How I formed my own views on AI safety\", which could also be pretty helpful. Here is a Robert Miles youtube video that can be a good place to start as well. Otherwise, there are various articles about it, like this one, from Vox.\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/pYXy-A4siMw\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
    "tags": [],
    "links to": [
      [
        "\"Why AI safety?\" info page",
        "https://intelligence.org/why-ai-safety/"
      ],
      [
        "\"How I formed my own views on AI safety\"",
        "https://forum.effectivealtruism.org/posts/xS9dFE3A6jdooiN7M/how-i-formed-my-own-views-about-ai-safety"
      ],
      [
        "this one, from Vox",
        "https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"
      ]
    ]
  },
  {
    "question": "How do I format answers on Stampy?",
    "answer": "",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "How does AI taking things literally contribute to alignment being hard?",
    "answer": "",
    "tags": [
      {
        "tag": "specification gaming",
        "link": "https://stampy.ai/wiki/Specification_gaming"
      },
      {
        "tag": "goodhart's law",
        "link": "https://stampy.ai/wiki/Goodhart%27s_law"
      }
    ],
    "links to": []
  },
  {
    "question": "How does MIRI communicate their view on alignment?",
    "answer": "Recently they've been trying to communicate their worldview, in particular, how incredibly doomy they are, perhaps in order to move other research efforts towards what they see as the hard problems.\n",
    "tags": [],
    "links to": [
      [
        "incredibly doomy they are",
        "https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"
      ]
    ]
  },
  {
    "question": "How does the stamp eigenkarma system work?",
    "answer": "If someone posts something good - something that shows insight, knowledge of AI Safety, etc. - give the message or answer a stamp of approval! Stampy keeps track of these, and uses them to decide how much he likes each user. You can ask Stampy (in a PM if you like), \"How many stamps am I worth?\", and he'll tell you.\n\nIf something is really very good, especially if it took a lot of work/effort, give it a gold stamp. These are worth 5 regular stamps!\n\nNote that stamps aren't just 'likes', so please don't give stamps to say \"me too\" or \"that's funny\" etc. They're meant to represent knowledge, understanding, good judgement, and contributing to the discord. You can use \ud83d\udcaf or \u2714\ufe0f for things you agree with, \ud83d\ude02 or \ud83e\udd23 for funny things etc.\n\nYour stamp points determine how much say you have if there are disagreements on Stampy content, which channels you have permission to post to, your voting power for approving YouTube replies, and whether you get to invite people.\n\nNotes on stamps and stamp points\n\nSo yeah everyone ends up with a number that basically represents what Stampy thinks of them, and you can ask him \"how many stamps am I worth?\" to get that number\n\nso if you have people a, b, and c, the points are calculated by:<br />\na_points = (bs_score_for_a * b_points) + (cs_score_for_a * c_points)<br />\nb_points = (as_score_for_b * a_points) + (cs_score_for_b * c_points)<br />\nc_points = (as_score_for_c * a_points) + (bs_score_for_c * b_points)<br />\nwhich is tough because you need to know everyone else's score before you can calculate your own<br />\nbut actually the system will have a fixed point - there'll be a certain arrangement of values such that every node has as much flowing out as flowing in - a stable configuration\nso you can rearrange<br />\n(bs_score_for_a * b_points) + (cs_score_for_a * c_points) - a_points = 0<br />\n(as_score_for_b * a_points) + (cs_score_for_b * c_points) - b_points = 0<br />\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) - c_points = 0<br />\nor, for neatness:<br />\n( -1 * a_points) + (bs_score_for_a * b_points) + (cs_score_for_a * c_points) = 0<br />\n(as_score_for_b * a_points) + ( -1 * b_points) + (cs_score_for_b * c_points) = 0<br />\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) + ( -1 * c_points) = 0 <br />\nand this is just a system of linear scalar equations that you can throw at numpy.linalg.solve<br />\n(you add one more equation that says rob_points = 1, so there's some place to start from)\nthere should be one possible distribution of points such that all of the equations hold at the same time, and numpy finds that by linear algebra magic beyond my very limited understanding<br />\nbut as far as I can tell you can have all the cycles you want!<br />\n(I actually have the scores sum to slightly less than 1, to have the stamp power slightly fade out as it propagates, just to make sure it doesn't explode. But I don't think I actually need to do that)<br />\nand yes this means that any time anyone gives a stamp to anyone, ~everyone's points will change slightly<br />\nAnd yes this means I'm recalculating the matrix and re-solving it for every new stamp, but computers are fast and I'm sure there are cheaper approximations I could switch to later if necessary\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Stampy",
        "/wiki/Stampy"
      ]
    ]
  },
  {
    "question": "How doomed is humanity?",
    "answer": "The opinions from experts are all over the place, according to this 2021 survey. Someone has also collected a database of existential risk estimates.\n\nOn the pessimistic end you find people like Eliezer Yudkowsky, who said: \"I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).\"\n\nWhile at the optimistic end you have people like Ben Garfinkel who put the probability at more like 0.1-1% for AI causing an existential catastrophe in the next century, with most people lying somewhere in the middle.\n",
    "tags": [
      {
        "tag": "doom",
        "link": "https://stampy.ai/wiki/Doom"
      },
      {
        "tag": "surveys",
        "link": "https://stampy.ai/wiki/Surveys"
      }
    ],
    "links to": [
      [
        "this 2021 survey",
        "https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results"
      ],
      [
        "database of existential risk estimates",
        "https://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit#gid=0"
      ],
      [
        "who said",
        "https://forum.effectivealtruism.org/posts/bGBm2yTiLEwwCbL6w/discussion-with-eliezer-yudkowsky-on-agi-interventions"
      ]
    ]
  },
  {
    "question": "How fast will AI takeoff be?",
    "answer": "There is significant controversy on how quickly AI will grow into a superintelligence. The Alignment Forum tag has many views on how things might unfold, where the probabilities of a soft (happening over years/decades) takeoff and a hard (happening in months, or less) takeoff are discussed.\n",
    "tags": [
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      }
    ],
    "links to": [
      [
        "Alignment Forum tag",
        "https://www.alignmentforum.org/tag/ai-takeoff"
      ]
    ]
  },
  {
    "question": "How is &quot;intelligence&quot; defined?",
    "answer": "",
    "tags": [
      {
        "tag": "intelligence",
        "link": "https://stampy.ai/wiki/Intelligence"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": []
  },
  {
    "question": "How is AGI different from current AI?",
    "answer": "",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "narrow ai",
        "link": "https://stampy.ai/wiki/Narrow_ai"
      },
      {
        "tag": "needs work",
        "link": "https://stampy.ai/wiki/Needs_work"
      }
    ],
    "links to": []
  },
  {
    "question": "How is Beth Barnes evaluating LM power seeking?",
    "answer": "Beth is working on \u201cgenerating a dataset that we can use to evaluate how close models are to being able to successfully seek power\u201d. The dataset is being created through simulating situations in which an Large Language Model is trying to seek power.\n\nThe overall goal of the project is to assess how close a model is to being dangerous, e.g. so we can know if it\u2019s safe for labs to scale it up. Evaluations focus on whether models are capable enough to seek power successfully, rather than whether they are aligned. They are aiming to create an automated evaluation which takes in a model and outputs how far away from dangerous it is, approximating an idealized human evaluation.\n",
    "tags": [],
    "links to": [
      [
        "overall goal of the project",
        "https://docs.google.com/document/d/1tf9Diyf46jlhuy7FcfvV1Ti9DsAxDAjVojMzruEySIg/edit"
      ]
    ]
  },
  {
    "question": "How is OpenAI planning to solve the full alignment problem?",
    "answer": "The safety team at OpenAI's plan is to build a MVP aligned AGI to try and help us solve the full alignment problem.\n\nThey want to do this with Reinforcement Learning from Human Feedback (RLHF): get feedback from humans about what is good, i.e. give reward to AI's based on the human feedback. Problem: what if the AI makes gigabrain 5D chess moves that humans don't understand, so can't evaluate. Jan Leike, the director of the safety team, views this (the informed oversight problem) as the core difficulty of alignment. Their proposed solution: an AI assisted oversight scheme, with a recursive hierarchy of AIs bottoming out at humans. They are working on experimenting with this approach by trying to get current day AIs to do useful supporting work such as summarizing books and criticizing itself.\n\nOpenAI also published GPT-3, and are continuing to push LLM capabilities, with GPT-4 expected to be released at some point soon.\n\nSee also: Common misconceptions about OpenAI and Our approach to alignment research.\n",
    "tags": [
      {
        "tag": "research agendas",
        "link": "https://stampy.ai/wiki/Research_agendas"
      }
    ],
    "links to": [
      [
        "MVP aligned AGI",
        "https://aligned.substack.com/p/alignment-mvp"
      ],
      [
        "the informed oversight problem",
        "https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35"
      ],
      [
        "summarizing books",
        "https://openai.com/blog/summarizing-books/"
      ],
      [
        "criticizing itself",
        "https://openai.com/blog/critiques/"
      ],
      [
        "Common misconceptions about OpenAI",
        "https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai"
      ],
      [
        "Our approach to alignment research",
        "https://openai.com/blog/our-approach-to-alignment-research/"
      ]
    ]
  },
  {
    "question": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?",
    "answer": "ARC is trying to solve Eliciting Latent Knowledge (ELK). Suppose that you are training an AI agent that predicts the state of the world and then performs some actions, called a <i>predictor</i>. This predictor is the AGI that will be acting to accomplish goals in the world. How can you create another model, called a <i>reporter</i>, that tells you what the predictor believes about the world? A key challenge in training this reporter is that training your reporter on human labeled training data, by default, incentivizes the predictor to just model what the human thinks is true, because the human is a simpler model than the AI.\n\nMotivation: At a high level, Paul's plan seems to be to produce a minimal AI that can help to do AI safety research. To do this, preventing deception and inner alignment failure are on the critical path, and the only known solution paths to this require interpretability (this is how all of Evan's 11 proposals plan to get around this problem).\n\nIf ARC can solve ELK, this would be a very strong form of interpretability: our reporter is able to tell us what the predictor believes about the world. Some ways this could end up being useful for aligning the predictor include:\n",
    "tags": [],
    "links to": [
      [
        "Eliciting Latent Knowledge (ELK)",
        "https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit"
      ],
      [
        "deception",
        "https://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment"
      ],
      [
        "inner alignment failure",
        "https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem"
      ],
      [
        "11 proposals",
        "https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai"
      ]
    ]
  },
  {
    "question": "How likely is an &quot;intelligence explosion&quot;?",
    "answer": "Conditional on technological progress continuing, it seems extremely likely that there will be an intelligence explosion, as at some point generally capable intelligent systems will tend to become the main drivers of their own development both at a software and hardware level. This would predictably create a feedback cycle of increasingly intelligent systems improving themselves more effectively. It seems like if the compute was used effectively, computers have many large advantages over biological cognition, so this scaling up might be very rapid if there is a computational overhang.\n\nSome ways technological progress could stop would be global coordination to stop AI research, global catastrophes severe enough to stop hardware production and maintenance, or hardware reaching physical limits before an intelligence explosion is possible (though this last one seems unlikely, as atomically precise manufacturing promises many orders of magnitude of cost reduction and processing power increase, and we're already seeing fairly capable systems on current hardware).\n",
    "tags": [
      {
        "tag": "plausibility",
        "link": "https://stampy.ai/wiki/Plausibility"
      },
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      },
      {
        "tag": "computing overhang",
        "link": "https://stampy.ai/wiki/Computing_overhang"
      }
    ],
    "links to": [
      [
        "computers have many large advantages over biological cognition",
        "https://publicism.info/philosophy/superintelligence/4.html"
      ],
      [
        "computational overhang",
        "/w/index.php?title=Computational_overhang&amp;action=edit&amp;redlink=1"
      ],
      [
        "atomically precise manufacturing",
        "https://en.wikipedia.org/wiki/Atomically_precise_manufacturing"
      ]
    ]
  },
  {
    "question": "How likely is it that an AI would pretend to be a human to further its goals?",
    "answer": "",
    "tags": [
      {
        "tag": "deception",
        "link": "https://stampy.ai/wiki/Deception"
      }
    ],
    "links to": []
  },
  {
    "question": "How likely is it that governments will play a significant role? What role would be desirable, if any?",
    "answer": "Currently, private AI labs are greatly ahead of any known governmental efforts in the production of advanced AI, so it looks unlikely that a government will be involved directly in the creation of the first AGI.\n\nNevertheless, governments are important drivers of policies, so in order to avoid unaligned AGI, here are some suggestions that they can enact:\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "How might AGI kill people?",
    "answer": "If we pose a serious threat, it could hack our weapons systems and turn them against us. Future militaries are much more vulnerable to this due to rapidly progressing autonomous weapons. There\u2019s also the option of creating bioweapons and distributing them to the most unstable groups you can find, tricking nations into WW3, or dozens of other things an agent many times smarter than any human with the ability to develop arbitrary technology, hack things (including communications), and manipulate people, or many other possibilities that something smarter than a human could think up. More can be found here.\n\nIf we are not a threat, in the course of pursuing its goals it may consume vital resources that humans need (e.g. using land for solar panels instead of farm crops). This video goes into more detail:\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/ZeecOKBus3Q\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "ai takeover",
        "link": "https://stampy.ai/wiki/Ai_takeover"
      }
    ],
    "links to": [
      [
        "here",
        "https://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms"
      ]
    ]
  },
  {
    "question": "How might a superintelligence socially manipulate humans?",
    "answer": "People tend to imagine AIs as being like nerdy humans \u2013 brilliant at technology but clueless about social skills. There is no reason to expect this \u2013 persuasion and manipulation is a different kind of skill from solving mathematical proofs, but it\u2019s still a skill, and an intellect as far beyond us as we are beyond lions might be smart enough to replicate or exceed the \u201ccharming sociopaths\u201d who can naturally win friends and followers despite a lack of normal human emotions.\n\nA superintelligence might be able to analyze human psychology deeply enough to understand the hopes and fears of everyone it negotiates with. Single humans using psychopathic social manipulation have done plenty of harm \u2013 Hitler leveraged his skill at oratory and his understanding of people\u2019s darkest prejudices to take over a continent. Why should we expect superintelligences to do worse than humans far less skilled than they?\n\nMore outlandishly, a superintelligence might just skip language entirely and figure out a weird pattern of buzzes and hums that causes conscious thought to seize up, and which knocks anyone who hears it into a weird hypnotizable state in which they\u2019ll do anything the superintelligence asks. It sounds kind of silly to me, but then, nuclear weapons probably would have sounded kind of silly to lions sitting around speculating about what humans might be able to accomplish. When you\u2019re dealing with something unbelievably more intelligent than you are, you should probably expect the unexpected.\n",
    "tags": [
      {
        "tag": "deception",
        "link": "https://stampy.ai/wiki/Deception"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "cognitive superpowers",
        "link": "https://stampy.ai/wiki/Cognitive_superpowers"
      }
    ],
    "links to": []
  },
  {
    "question": "How might an &quot;intelligence explosion&quot; be dangerous?",
    "answer": "If programmed with the wrong motivations, a machine could be malevolent toward humans, and intentionally exterminate our species. More likely, it could be designed with motivations that initially appeared safe (and easy to program) to its designers, but that turn out to be best fulfilled (given sufficient power) by reallocating resources from sustaining human life to other projects. As Yudkowsky writes, \u201cthe AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\u201d\n\nSince weak AIs with many different motivations could better achieve their goal by faking benevolence until they are powerful, safety testing to avoid this could be very challenging. Alternatively, competitive pressures, both economic and military, might lead AI designers to try to use other methods to control AIs with undesirable motivations. As those AIs became more sophisticated this could eventually lead to one risk too many.\n\nEven a machine successfully designed with superficially benevolent motivations could easily go awry when it discovers implications of its decision criteria unanticipated by its designers. For example, a superintelligence programmed to maximize human happiness might find it easier to rewire human neurology so that humans are happiest when sitting quietly in jars than to build and maintain a utopian world that caters to the complex and nuanced whims of current human neurology.\n\nSee also:\n",
    "tags": [
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      },
      {
        "tag": "ai takeover",
        "link": "https://stampy.ai/wiki/Ai_takeover"
      }
    ],
    "links to": [
      [
        "other projects",
        "https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf"
      ]
    ]
  },
  {
    "question": "How might an AI achieve a seemingly beneficial goal via inappropriate means?",
    "answer": "Imagine, for example, that you are tasked with reducing traffic congestion in San Francisco at all costs, i.e. you do not take into account any other constraints. How would you do it? You might start by just timing traffic lights better. But wouldn\u2019t there be less traffic if all the bridges closed down from 5 to 10AM, preventing all those cars from entering the city? Such a measure obviously violates common sense, and subverts the purpose of improving traffic, which is to help people get around \u2013 but it is consistent with the goal of \u201creducing traffic congestion\u201d.\n",
    "tags": [
      {
        "tag": "specification gaming",
        "link": "https://stampy.ai/wiki/Specification_gaming"
      },
      {
        "tag": "goodhart's law",
        "link": "https://stampy.ai/wiki/Goodhart%27s_law"
      }
    ],
    "links to": []
  },
  {
    "question": "How might non-agentic GPT-style AI cause an &quot;intelligence explosion&quot; or otherwise contribute to existential risk?",
    "answer": "",
    "tags": [
      {
        "tag": "mesa-optimization",
        "link": "https://stampy.ai/wiki/Mesa-optimization"
      },
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      },
      {
        "tag": "existential risk",
        "link": "https://stampy.ai/wiki/Existential_risk"
      },
      {
        "tag": "gpt",
        "link": "https://stampy.ai/wiki/Gpt"
      }
    ],
    "links to": []
  },
  {
    "question": "How might things go wrong with AI even without an agentic superintelligence?",
    "answer": "Failures can happen with narrow non-agentic systems, mostly from humans not anticipating safety-relevant decisions made too quickly to react, much like in the 2010 flash crash.\n\nA helpful metaphor draws on self-driving cars. By relying more and more on an automated process to make decisions, people become worse drivers as they\u2019re not training themselves to react to the unexpected; then the unexpected happens, the software system itself reacts in an unsafe way and the human is too slow to regain control.\n\nThis generalizes to broader tasks. A human using a powerful system to make better decisions (say, as the CEO of a company) might not understand those very well, get trapped into an equilibrium without realizing it and essentially losing control over the entire process.\n\nMore detailed examples in this vein are described by Paul Christiano in <i>What failure looks like</i>.\n\nAnother source of failures is AI-mediated stable totalitarianism. The limiting factor in current pervasive surveillance, police and armed forces is manpower; the use of drones and other automated tools decreases the need for personnel to ensure security and extract resources.\n\nAs capabilities improve, political dissent could become impossible, checks and balances would break down as a minimal number of key actors is needed to stay in power.\n",
    "tags": [
      {
        "tag": "narrow ai",
        "link": "https://stampy.ai/wiki/Narrow_ai"
      },
      {
        "tag": "persuasion",
        "link": "https://stampy.ai/wiki/Persuasion"
      }
    ],
    "links to": [
      [
        "2010 flash crash",
        "https://en.wikipedia.org/wiki/2010_flash_crash"
      ],
      [
        "What failure looks like",
        "https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like"
      ],
      [
        "a minimal number of key actors is needed to stay in power",
        "https://www.youtube.com/watch?v=rStL7niR7gs"
      ]
    ]
  },
  {
    "question": "How might we get from Artificial General Intelligence to a Superintelligent system?",
    "answer": "Once a system is at least as capable as top human at AI research, it would tend to become the driver of its own development and initiate a process of recursive self-improvement known as the intelligence explosion, leading to an extremely powerful system. A general framing of this process is Open Philanthropy's Process for Automating Scientific and Technological Advancement (PASTA).\n\nThere is much debate about whether there would be a notable period where the AI was partially driving its own development, with humans being gradually less and less important, or whether the transition to AI automated AI capability research would be sudden. However, the core idea that there is <i>some</i> threshold of capabilities beyond which a system would begin to rapidly ascend is hard to reasonably dispute, and is a significant consideration for developing alignment strategies.\n",
    "tags": [],
    "links to": [
      [
        "intelligence explosion",
        "/wiki/What_is_an_%22intelligence_explosion%22%3F"
      ],
      [
        "Open Philanthropy",
        "https://www.openphilanthropy.org/"
      ],
      [
        "Process for Automating Scientific and Technological Advancement (PASTA)",
        "https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#:~:text=Process%20for%20Automating%20Scientific%20and%20Technological%20Advancement%2C%20or%20PASTA"
      ],
      [
        "much",
        "https://sideways-view.com/2018/02/24/takeoff-speeds/"
      ],
      [
        "debate",
        "https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai"
      ]
    ]
  },
  {
    "question": "How much resources did the processes of biological evolution use to evolve intelligent creatures?",
    "answer": "Ajeya Cotra's attempted to calculate this number in her paper Bio Anchors.\n\n[...]the total amount of computation done over the course of evolution from the first animals with neurons to humans was (~1e16 seconds) * (~1e25 FLOP/s) = ~<b>1e41 FLOP</b>\n\nNu\u00f1o Sempere argues that this calculation of the computation done by neurons is insufficient as the environment would also need to be simulated, leading to a possibly much larger number.\n\nCotra posits that this amount of computation should be taken on an upper bound to the amount of computation needed to develop AGI. The actual amount of computation needed is probably many orders of magnitude lower.\n",
    "tags": [],
    "links to": [
      [
        "Bio Anchors",
        "https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.gvc1xyxlemkd"
      ],
      [
        "argues",
        "https://forum.effectivealtruism.org/posts/FHTyixYNnGaQfEexH/a-concern-about-the-evolutionary-anchor-of-ajeya-cotra-s"
      ]
    ]
  },
  {
    "question": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?",
    "answer": "If the AI system was deceptively aligned (i.e. pretending to be nice until it was in control of the situation) or had been in stealth mode while getting things in place for a takeover, quite possibly within hours. We may get more warning with weaker systems, if the AGI does not feel at all threatened by us, or if a complex ecosystem of AI systems is built over time and we gradually lose control.\n\nPaul Christiano writes a story of alignment failure which shows a relatively fast transition.\n",
    "tags": [
      {
        "tag": "deceptive alignment",
        "link": "https://stampy.ai/wiki/Deceptive_alignment"
      },
      {
        "tag": "ai takeover",
        "link": "https://stampy.ai/wiki/Ai_takeover"
      },
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      }
    ],
    "links to": [
      [
        "complex ecosystem of AI systems is built over time and we gradually lose control",
        "https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story"
      ],
      [
        "a story of alignment failure",
        "https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story"
      ]
    ]
  },
  {
    "question": "How would we align an AGI whose learning algorithms / cognition look like human brains?",
    "answer": "This is primarily Steven Brynes, a full time independent alignment researcher, working on answering the question: \"How would we align an AGI whose learning algorithms / cognition look like human brains?\"\n\nHumans seem to robustly care about things, why is that? If we understood that, could we design AGIs to do the same thing? As far as I understand it, most of this work is biology based: trying to figure out how various parts of the brain works, but then also connecting this to alignment and seeing if we can solve the alignment problem with this understanding.\n\nThere are three other independent researchers working on related projects that Steven has proposed.\n",
    "tags": [],
    "links to": [
      [
        "three other independent researchers working on related projects",
        "https://www.lesswrong.com/posts/c2tEfqEMi6jcJ4kdg/brain-like-agi-project-aintelope"
      ]
    ]
  },
  {
    "question": "How would you explain the theory of Infra-Bayesianism?",
    "answer": "See Vanessa's research agenda for more detail.\n\nIf we don't know how to do something given unbounded compute, we are just confused about the thing. Going from thinking that chess was impossible for machines to understanding minimax was a really good step forward for designing chess AIs, <i>even though minimax is completely intractable</i>.\n\nThus, we should seek to figure out how alignment might look in theory, and then try to bridge the theory-practice gap by making our proposal ever more efficient. The first step along this path is to figure out a universal Reinforcement Learning setting that we can place our formal agents in, and then prove regret bounds in.\n\nA key problem in doing this is embeddedness. AIs can't have a perfect self model \u2014 this would be like imagining your ENTIRE brain, inside your brain. There are finite memory constraints. Infra-Bayesianism (IB) is essentially a theory of imprecise probability that lets you specify local / fuzzy things. IB allows agents to have abstract models of themselves, and thus works in an embedded setting.\n\nInfra-Bayesian Physicalism (IBP) is an extension of this to RL. IBP allows us to\n\nVanessa uses this formalism to describe PreDCA, an alignment proposal based on IBP. This proposal assumes that an agent is an IBP agent, meaning that it is an RL agent with fuzzy probability distributions (along with some other things). The general outline of this proposal is as follows:\n\nVanessa models an AI as a model based RL system with a WM, a reward function, and a policy derived from the WM + reward. She claims that this avoids the sharp left turn. The generalization problems come from the world model, but this is dealt with by having an epistemology that doesn't contain bridge rules, and so the true world is the simplest explanation for the observed data.\n\nIt is open to show that this proposal also solves inner alignment, but there is some chance that it does.\n\nThis approach deviates from MIRI's plan, which is to focus on a narrow task to perform the pivotal act, and then add corrigibility. Vanessa instead tries to directly learn the user's preferences, and optimize those.\n",
    "tags": [],
    "links to": [
      [
        "research agenda",
        "https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"
      ],
      [
        "minimax",
        "https://www.google.com/url?q=https://en.wikipedia.org/wiki/Minimax&amp;sa=D&amp;source=editors&amp;ust=1661633213196096&amp;usg=AOvVaw3m8tD5QAEl-XXhvaH4d1v3"
      ],
      [
        "Reinforcement Learning",
        "https://www.alignmentforum.org/tag/reinforcement-learning"
      ],
      [
        "Infra-Bayesianism",
        "https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa"
      ],
      [
        "Infra-Bayesian Physicalism",
        "https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized"
      ],
      [
        "PreDCA",
        "https://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId=vKw6DB9crncovPxED#vKw6DB9crncovPxED"
      ],
      [
        "She claims that this avoids the sharp left turn",
        "https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"
      ],
      [
        "bridge rules",
        "https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges"
      ]
    ]
  },
  {
    "question": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?",
    "answer": "OK, it\u2019s great that you want to help, here are some ideas for ways you could do so without making a huge commitment:\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": []
  },
  {
    "question": "I want to work on AI alignment. How can I get funding?",
    "answer": "See the Future Funding List for up to date information!\n\nThe organizations which most regularly give grants to individuals working towards AI alignment are the Long Term Future Fund, Survival And Flourishing (SAF), the OpenPhil AI Fellowship and early career funding, the Future of Life Institute, the Future of Humanity Institute, and the Center on Long-Term Risk Fund. If you're able to relocate to the UK, CEEALAR (aka the EA Hotel) can be a great option as it offers free food and accommodation for up to two years, as well as contact with others who are thinking about these issues. There are also opportunities from smaller grantmakers which you might be able to pick up if you get involved.\n\nIf you want to work on support or infrastructure rather than directly on research, the EA Infrastructure Fund may be able to help. In general, you can talk to EA funds before applying.\n\nEach grant source has their own criteria for funding, but in general they are looking for candidates who have evidence that they're keen and able to do good work towards reducing existential risk (for example, by completing an AI Safety Camp project), though the EA Hotel in particular has less stringent requirements as they're able to support people at very low cost. If you'd like to talk to someone who can offer advice on applying for funding, AI Safety Support offers free calls.\n\nAnother option is to get hired by an organization which works on AI alignment, see the follow-up question for advice on that.\n\nIt's also worth checking the AI Alignment tag on the EA funding sources website for up-to-date suggestions.\n",
    "tags": [
      {
        "tag": "organizations",
        "link": "https://stampy.ai/wiki/Organizations"
      },
      {
        "tag": "funding",
        "link": "https://stampy.ai/wiki/Funding"
      },
      {
        "tag": "ai safety support",
        "link": "https://stampy.ai/wiki/Ai_safety_support"
      },
      {
        "tag": "ai safety camp",
        "link": "https://stampy.ai/wiki/Ai_safety_camp"
      }
    ],
    "links to": [
      [
        "Future Funding List",
        "https://www.futurefundinglist.com/"
      ],
      [
        "Long Term Future Fund",
        "https://funds.effectivealtruism.org/funds/far-future"
      ],
      [
        "Survival And Flourishing (SAF)",
        "http://survivalandflourishing.org/"
      ],
      [
        "OpenPhil AI Fellowship",
        "https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship"
      ],
      [
        "early career funding",
        "https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future"
      ],
      [
        "Future of Life Institute",
        "https://grants.futureoflife.org/"
      ],
      [
        "Future of Humanity Institute",
        "https://www.fhi.ox.ac.uk/aia-fellowship/"
      ],
      [
        "the Center on Long-Term Risk Fund",
        "https://longtermrisk.org/grantmaking/"
      ],
      [
        "CEEALAR (aka the EA Hotel)",
        "https://ceealar.org/"
      ],
      [
        "EA Infrastructure Fund",
        "https://funds.effectivealtruism.org/funds/ea-community"
      ],
      [
        "talk to EA funds before applying",
        "https://www.lesswrong.com/posts/5AAFoigbbMqgrTpDh/you-can-talk-to-ea-funds-before-applying"
      ],
      [
        "AI Safety Camp",
        "https://aisafety.camp/"
      ],
      [
        "AI Safety Support",
        "https://www.aisafetysupport.org/"
      ],
      [
        "free calls",
        "https://calendly.com/aiss"
      ],
      [
        "EA funding sources website",
        "https://eafunding.softr.app/"
      ]
    ]
  },
  {
    "question": "I&#39;m interested in working on AI safety. What should I do?",
    "answer": "",
    "tags": [
      {
        "tag": "careers",
        "link": "https://stampy.ai/wiki/Careers"
      },
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": []
  },
  {
    "question": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?",
    "answer": "",
    "tags": [
      {
        "tag": "ai takeover",
        "link": "https://stampy.ai/wiki/Ai_takeover"
      }
    ],
    "links to": []
  },
  {
    "question": "If I only care about helping people alive today, does AI safety still matter?",
    "answer": "This largely depends on when you think AI will be advanced enough to constitute an immediate threat to humanity. This is difficult to estimate, but the field is surveyed at How long will it be until transformative AI is created?, which comes to the conclusion that it is relatively widely believed that AI will transform the world in our lifetimes.\n\nWe probably shouldn't rely too strongly on these opinions as predicting the future is hard. But, due to the enormous damage a misaligned AGI could do, it's worth putting a great deal of effort towards AI alignment even if you just care about currently existing humans (such as yourself).\n",
    "tags": [],
    "links to": [
      [
        "How long will it be until transformative AI is created?",
        "/wiki/How_long_will_it_be_until_transformative_AI_is_created%3F"
      ]
    ]
  },
  {
    "question": "If we solve alignment, are we sure of a good future?",
    "answer": "If by \u201csolve alignment\u201d you mean build a sufficiently performance-competitive superintelligence which has the goal of Coherent Extrapolated Volition or something else which captures human values, then yes. It would be able to deploy technology near the limits of physics (e.g. atomically precise manufacturing) to solve most of the other problems which face us, and steer the future towards a highly positive path for perhaps many billions of years until the heat death of the universe (barring more esoteric x-risks like encounters with advanced hostile civilizations, false vacuum decay, or simulation shutdown).\n\nHowever, if you only have alignment of a superintelligence to a single human you still have the risk of misuse, so this should be at most a short-term solution. For example, what if Google creates a superintelligent AI, and it listens to the CEO of Google, and it\u2019s programmed to do everything exactly the way the CEO of Google would want? Even assuming that the CEO of Google has no hidden unconscious desires affecting the AI in unpredictable ways, this gives one person a lot of power.\n",
    "tags": [],
    "links to": [
      [
        "Coherent Extrapolated Volition",
        "https://www.lesswrong.com/tag/coherent-extrapolated-volition"
      ],
      [
        "atomically precise manufacturing",
        "https://en.wikipedia.org/wiki/Atomically_precise_manufacturing"
      ],
      [
        "perhaps many billions of years",
        "https://en.wikipedia.org/wiki/Timeline_of_the_far_future"
      ],
      [
        "heat death of the universe",
        "https://en.wikipedia.org/wiki/Heat_death_of_the_universe"
      ],
      [
        "false vacuum decay",
        "https://en.wikipedia.org/wiki/False_vacuum_decay"
      ],
      [
        "simulation shutdown",
        "https://arxiv.org/ftp/arxiv/papers/1905/1905.05792.pdf"
      ]
    ]
  },
  {
    "question": "Is AI alignment possible?",
    "answer": "Yes, if the superintelligence has goals which include humanity surviving then we would not be destroyed. If those goals are fully aligned with human well-being, we would in fact find ourselves in a dramatically better place.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "doom",
        "link": "https://stampy.ai/wiki/Doom"
      }
    ],
    "links to": [
      [
        "fully aligned",
        "https://www.lesswrong.com/tag/value-learning"
      ]
    ]
  },
  {
    "question": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?",
    "answer": "Blindly following the trendlines while forecasting technological progress is certainly a risk (affectionately known in AI circles as \u201cpulling a Kurzweill\u201d), but sometimes taking an exponential trend seriously is the right response.\n\nConsider economic doubling times. In 1 AD, the world GDP was about $20 billion; it took a thousand years, until 1000 AD, for that to double to $40 billion. But it only took five hundred more years, until 1500, or so, for the economy to double again. And then it only took another three hundred years or so, until 1800, for the economy to double a third time.\nSomeone in 1800 might calculate the trend line and say this was ridiculous, that it implied the economy would be doubling every ten years or so in the beginning of the 21st century. But in fact, this is how long the economy takes to double these days. To a medieval, used to a thousand-year doubling time (which was based mostly on population growth!), an economy that doubled every ten years might seem inconceivable. To us, it seems normal.\n\nLikewise, in 1965 Gordon Moore noted that semiconductor complexity seemed to double every eighteen months. During his own day, there were about five hundred transistors on a chip; he predicted that would soon double to a thousand, and a few years later to two thousand.\nAlmost as soon as Moore\u2019s Law become well-known, people started saying it was absurd to follow it off a cliff \u2013 such a law would imply a million transistors per chip in 1990, a hundred million in 2000, ten billion transistors on every chip by 2015! More transistors on a single chip than existed on all the computers in the world! Transistors the size of molecules! But of course all of these things happened; the ridiculous exponential trend proved more accurate than the naysayers.\n\nNone of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can\u2019t possibly be. We can\u2019t be sure that a computer using its own intelligence to discover new ways to increase its intelligence will enter a positive feedback loop and achieve superintelligence in seemingly impossibly short time scales. It\u2019s just one more possibility, a worry to place alongside all the other worrying reasons to expect a moderate or hard takeoff.\n",
    "tags": [
      {
        "tag": "recursive self-improvement",
        "link": "https://stampy.ai/wiki/Recursive_self-improvement"
      }
    ],
    "links to": []
  },
  {
    "question": "Is it possible to block an AI from doing certain things on the Internet?",
    "answer": "",
    "tags": [
      {
        "tag": "tripwire",
        "link": "https://stampy.ai/wiki/Tripwire"
      },
      {
        "tag": "boxing",
        "link": "https://stampy.ai/wiki/Boxing"
      }
    ],
    "links to": []
  },
  {
    "question": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?",
    "answer": "Sort answer: No, and could be dangerous to try.\n\nSlightly longer answer: With any realistic real-world task assigned to an AGI, there are so many ways in which it could go wrong that trying to block them all off by hand is a hopeless task, especially when something smarter than you is trying to find creative new things to do. You run into the nearest unblocked strategy problem.\n\nIt may be dangerous to try this because if you try and hard-code a large number of things to avoid it increases the chance that there\u2019s a bug in your code which causes major problems, simply by increasing the size of your codebase.\n",
    "tags": [
      {
        "tag": "nearest unblocked strategy",
        "link": "https://stampy.ai/wiki/Nearest_unblocked_strategy"
      }
    ],
    "links to": [
      [
        "nearest unblocked strategy",
        "https://arbital.greaterwrong.com/p/nearest_unblocked/"
      ]
    ]
  },
  {
    "question": "Is large-scale automated AI persuasion and propaganda a serious concern?",
    "answer": "Language models can be utilized to produce propaganda by acting like bots and interacting with users on social media. This can be done to push a political agenda or to make fringe views appear more popular than they are.\n\nI'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.\n\n-- Wei Dei, quoted in Persuasion Tools: AI takeover without AGI or agency?\n\nAs of 2022, this is not within the reach of current models. However, on the current trajectory, AI might be able to write articles and produce other media for propagandistic purposes that are superior to human-made ones in not too many years. These could be precisely tailored to individuals, using things like social media feeds and personal digital data.\n\nAdditionally, recommender systems on content platforms like YouTube, Twitter, and Facebook use machine learning, and the content they recommend can influence the opinions of billions of people. Some research has looked at the tendency for platforms to promote extremist political views and to thereby help radicalize their userbase for example.\n\nIn the long term, misaligned AI might use its persuasion abilities to gain influence and take control over the future. This could look like convincing its operators to let it out of a box, to give it resources or creating political chaos in order to disable mechanisms to prevent takeover as in this story.\n\nSee Risks from AI persuasion for a deep dive into the distinct risks from AI persuasion.\n",
    "tags": [
      {
        "tag": "persuasion",
        "link": "https://stampy.ai/wiki/Persuasion"
      }
    ],
    "links to": [
      [
        "acting like bots",
        "https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/"
      ],
      [
        "political agenda",
        "https://www.nature.com/articles/d41586-020-03034-5"
      ],
      [
        "Wei Dei",
        "https://www.alignmentforum.org/posts/5bd75cc58225bf06703754b9/autopoietic-systems-and-difficulty-of-agi-alignment?commentId=5bd75cc58225bf06703754c1"
      ],
      [
        "Persuasion Tools: AI takeover without AGI or agency?",
        "https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency"
      ],
      [
        "research",
        "https://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content"
      ],
      [
        "this story",
        "https://www.gwern.net/fiction/Clippy"
      ],
      [
        "Risks from AI persuasion",
        "https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion"
      ]
    ]
  },
  {
    "question": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?",
    "answer": "The near term and long term aspects of AI safety are both very important to work on. Research into superintelligence is an important part of the open letter, but the actual concern is very different from the Terminator-like scenarios that most media outlets round off this issue to. A much more likely scenario is a superintelligent system with neutral or benevolent goals that is misspecified in a dangerous way. Robust design of superintelligent systems is a complex interdisciplinary research challenge that will likely take decades, so it is very important to begin the research now, and a large part of the purpose of our research program is to make that happen. That said, the alarmist media framing of the issues is hardly useful for making progress in either the near term or long term domain.\n",
    "tags": [
      {
        "tag": "other causes",
        "link": "https://stampy.ai/wiki/Other_causes"
      }
    ],
    "links to": []
  },
  {
    "question": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?",
    "answer": "Using some human-related metaphors (e.g. what an AGI \u2018wants\u2019 or \u2018believes\u2019) is almost unavoidable, as our language is built around experiences with humans, but we should be aware that these may lead us astray.\n\nMany paths to AGI would result in a mind very different from a human or animal, and it would be hard to predict in detail how it would act. We should not trust intuitions trained on humans to predict what an AGI or superintelligence would do. High fidelity Whole Brain Emulations are one exception, where we would expect the system to at least initially be fairly human, but it may diverge depending on its environment and what modifications are applied to it.\n\nThere has been some discussion about how language models trained on lots of human-written text seem likely to pick up human concepts and think in a somewhat human way, and how we could use this to improve alignment.\n",
    "tags": [
      {
        "tag": "whole brain emulation",
        "link": "https://stampy.ai/wiki/Whole_brain_emulation"
      }
    ],
    "links to": [
      [
        "use this to improve alignment",
        "https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default"
      ]
    ]
  },
  {
    "question": "Is this about AI systems becoming malevolent or conscious and turning on us?",
    "answer": "The problem isn\u2019t consciousness, but competence. You make machines that are incredibly competent at achieving objectives and they will cause accidents in trying to achieve those objectives.\n- Stuart Russell \nWork on AI alignment is not concerned with the question of whether \u201cconsciousness\u201d, \u201csentience\u201d or \u201cself-awareness\u201d could arise in a machine or an algorithm. Unlike the frequently-referenced plotline in the Terminator movies, the standard catastrophic misalignment scenarios under discussion do not require computers to become conscious; they only require conventional computer systems (although usually faster and more powerful ones than those available today) blindly and deterministically following logical steps, in the same way that they currently do.\n\nThe primary concern (\u201cAI misalignment\u201d) is that powerful systems could inadvertently be programmed with goals that do not fully capture what the programmers actually want. The AI would then harm humanity in pursuit of goals which seemed benign or neutral. Nothing like malevolence or consciousness would need to be involved. A number of researchers studying the problem have concluded that it is surprisingly difficult to guard against this effect, and that it is likely to get much harder as the systems become more capable. AI systems are inevitably goal-directed and could, for example, consider our efforts to control them (or switch them off) as being impediments to attaining their goals.\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?",
    "answer": "It likely will \u2013 however, intelligence is, by many definitions, the ability to figure out how to accomplish goals. Even in today\u2019s advanced AI systems, the builders assign the goal but don\u2019t tell the AI exactly how to accomplish it, nor necessarily predict in detail how it will be done; indeed those systems often solve problems in creative, unpredictable ways. Thus the thing that makes such systems intelligent is precisely what can make them difficult to predict and control. They may therefore attain the goal we set them via means inconsistent with our preferences.\n",
    "tags": [
      {
        "tag": "tool ai",
        "link": "https://stampy.ai/wiki/Tool_ai"
      }
    ],
    "links to": []
  },
  {
    "question": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?",
    "answer": "The AGI Safety Fundamentals Course is a arguably the best way to get up to speed on alignment, you can sign up to go through it with many other people studying and mentorship or read their materials independently.\n\nOther great ways to explore include:\n\nYou might also want to consider reading Rationality: A-Z which covers a lot of skills that are valuable to acquire for people trying to think about large and complex issues, with The Rationalist's Guide to the Galaxy available as a shorter and more accessible AI-focused option.\n",
    "tags": [
      {
        "tag": "literature",
        "link": "https://stampy.ai/wiki/Literature"
      }
    ],
    "links to": [
      [
        "AGI Safety Fundamentals Course",
        "https://www.eacambridge.org/technical-alignment-curriculum"
      ],
      [
        "Rationality: A-Z",
        "https://www.lesswrong.com/rationality"
      ],
      [
        "The Rationalist's Guide to the Galaxy",
        "https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795"
      ]
    ]
  },
  {
    "question": "Might an &quot;intelligence explosion&quot; never occur?",
    "answer": "Dreyfus and Penrose have argued that human cognitive abilities can\u2019t be emulated by a computational machine. Searle and Block argue that certain kinds of machines cannot have a mind (consciousness, intentionality, etc.). But these objections need not concern those who predict an intelligence explosion.\n\nWe can reply to Dreyfus and Penrose by noting that an intelligence explosion does not require an AI to be a classical computational system. And we can reply to Searle and Block by noting that an intelligence explosion does not depend on machines having consciousness or other properties of \u2018mind\u2019, only that it be able to solve problems better than humans can in a wide variety of unpredictable environments. As Edsger Dijkstra once said, the question of whether a machine can \u2018really\u2019 think is \u201cno more interesting than the question of whether a submarine can swim.\u201d\n\nOthers who are pessimistic about an intelligence explosion occurring within the next few centuries don\u2019t have a specific objection but instead think there are hidden obstacles that will reveal themselves and slow or halt progress toward machine superintelligence.\n\nFinally, a global catastrophe like nuclear war or a large asteroid impact could so damage human civilization that the intelligence explosion never occurs. Or, a stable and global totalitarianism could prevent the technological development required for an intelligence explosion to occur.\n",
    "tags": [
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      }
    ],
    "links to": [
      [
        "Dreyfus",
        "https://www.amazon.com/dp/0060110821/"
      ],
      [
        "Penrose",
        "https://www.amazon.com/dp/0195106466/"
      ],
      [
        "Searle",
        "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.749&amp;rep=rep1&amp;type=pdf"
      ],
      [
        "Block",
        "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.5828&amp;rep=rep1&amp;type=pdf"
      ],
      [
        "need not concern",
        "http://consc.net/papers/singularity.pdf"
      ],
      [
        "Others",
        "http://sethbaum.com/ac/2011_AI-Experts.pdf"
      ],
      [
        "a stable and global totalitarianism",
        "https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-29"
      ]
    ]
  },
  {
    "question": "Might an aligned superintelligence force people to have better lives and change more quickly than they want?",
    "answer": "If the superintelligence is aligned, probably not, but depends on the AIs metaethics.\n\nFor example, is it ethical to change what people want if we expect them to endorse it in hindsight, e.g. curing a drug or gambling addict of their addiction or treating a patient against their will? There is currently no consensus among moral philosophers regarding in which conditions this is acceptable, if any. An AI that follows preference utilitarianism would refuse to do so but a hedonistic utilitarian might consider it.\n\nIn order to reduce the possibility of unrest, an aligned superintelligence might avoid implementing policies outside of the Overton window when it is possible.\n",
    "tags": [],
    "links to": [
      [
        "treating a patient against their will",
        "https://educaloi.qc.ca/en/capsules/forced-medical-care"
      ],
      [
        "preference utilitarianism",
        "https://en.wikipedia.org/wiki/Preference_utilitarianism"
      ],
      [
        "hedonistic utilitarian",
        "https://www.quora.com/What-is-hedonistic-utilitarianism"
      ],
      [
        "Overton window",
        "https://en.wikipedia.org/wiki/Overton_window"
      ]
    ]
  },
  {
    "question": "OK, I\u2019m convinced. How can I help?",
    "answer": "Great! I\u2019ll ask you a few follow-up questions to help figure out how you can best contribute, give you some advice, and link you to resources which should help you on whichever path you choose. Feel free to scroll up and explore multiple branches of the FAQ if you want answers to more than one of the questions offered&#160;:)\n\nNote: We\u2019re still building out and improving this tree of questions and answers, any feedback is appreciated.\n\n<b>At what level of involvement were you thinking of helping?</b>\n\nPlease view and suggest to this google doc for improvements: https://docs.google.com/document/d/1S-CUcoX63uiFdW-GIFC8wJyVwo4VIl60IJHodcRfXJA/edit#\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": [
      [
        "https://docs.google.com/document/d/1S-CUcoX63uiFdW-GIFC8wJyVwo4VIl60IJHodcRfXJA/edit#",
        "https://docs.google.com/document/d/1S-CUcoX63uiFdW-GIFC8wJyVwo4VIl60IJHodcRfXJA/edit#"
      ]
    ]
  },
  {
    "question": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?",
    "answer": "We would not be able to turn off or reprogram a superintelligence gone rogue by default. Once in motion the superintelligence is now focused on completing its task. Suppose that it has a goal of calculating as many digits of pi as possible. Its current plan will allow it to calculate two hundred trillion such digits. But if it were turned off, or reprogrammed to do something else, that would result in it calculating zero digits. An entity fixated on calculating as many digits of pi as possible will work hard to prevent scenarios where it calculates zero digits of pi. Just by programming it to calculate digits of pi, we would have given it a drive to prevent people from turning it off.\n\nUniversity of Illinois computer scientist Steve Omohundro argues that entities with very different final goals \u2013 calculating digits of pi, curing cancer, helping promote human flourishing \u2013 will all share a few basic ground-level subgoals. First, self-preservation \u2013 no matter what your goal is, it\u2019s less likely to be accomplished if you\u2019re too dead to work towards it. Second, goal stability \u2013 no matter what your goal is, you\u2019re more likely to accomplish it if you continue to hold it as your goal, instead of going off and doing something else. Third, power \u2013 no matter what your goal is, you\u2019re more likely to be able to accomplish it if you have lots of power, rather than very little. Here\u2019s the full paper.\n\nSo just by giving a superintelligence a simple goal like \u201ccalculate digits of pi\u201d, we would have accidentally given it convergent instrumental goals like \u201cprotect yourself\u201d, \u201cdon\u2019t let other people reprogram you\u201d, and \u201cseek power\u201d.\n\nAs long as the superintelligence is safely contained, there\u2019s not much it can do to resist reprogramming. But it\u2019s hard to consistently contain a hostile superintelligence.\n",
    "tags": [
      {
        "tag": "corrigibility",
        "link": "https://stampy.ai/wiki/Corrigibility"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "Here\u2019s the full paper",
        "https://intelligence.org/files/BasicAIDrives.pdf"
      ]
    ]
  },
  {
    "question": "Superintelligence sounds like science fiction. Do people think about this in the real world?",
    "answer": "Many of the people with the deepest understanding of artificial intelligence are concerned about the risks of unaligned superintelligence. In 2014, Google bought world-leading artificial intelligence startup DeepMind for $400 million; DeepMind added the condition that Google promise to set up an AI Ethics Board. DeepMind cofounder Shane Legg has said in interviews that he believes superintelligent AI will be <i>\u201csomething approaching absolute power\u201d</i> and <i>\u201cthe number one risk for this century\u201d.</i>\n\nStuart Russell, Professor of Computer Science at Berkeley, author of the standard AI textbook, and world-famous AI expert, warns of <i>\u201cspecies-ending problems\u201d</i> and wants his field to pivot to make superintelligence-related risks a central concern. He went so far as to write Human Compatible, a book focused on bringing attention to the dangers of artificial intelligence and the need for more work to address them.\n\nMany other science and technology leaders agree. Late astrophysicist Stephen Hawking said that superintelligence <i>\u201ccould spell the end of the human race.\u201d</i> Tech billionaire Bill Gates describes himself as <i>\u201cin the camp that is concerned about superintelligence\u2026I don\u2019t understand why some people are not concerned\u201d.</i> Oxford Professor Nick Bostrom, who has been studying AI risks for over 20 years, has said: <i>\u201cSuperintelligence is a challenge for which we are not ready now and will not be ready for a long time.\u201d</i>\n\nHolden Karnofsky, the CEO of Open Philanthropy, has written a carefully reasoned account of why transformative artificial intelligence means that this might be the most important century.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "plausibility",
        "link": "https://stampy.ai/wiki/Plausibility"
      }
    ],
    "links to": [
      [
        "DeepMind",
        "https://en.wikipedia.org/wiki/DeepMind"
      ],
      [
        "Stuart Russell",
        "https://en.wikipedia.org/wiki/Stuart_J._Russell#Career_and_research"
      ],
      [
        "Human Compatible",
        "https://en.wikipedia.org/wiki/Human_Compatible"
      ],
      [
        "Stephen Hawking",
        "https://en.wikipedia.org/wiki/Stephen_Hawking#Future_of_humanity"
      ],
      [
        "Bill Gates",
        "https://en.wikipedia.org/wiki/Bill_Gates#Post-Microsoft"
      ],
      [
        "Nick Bostrom",
        "https://www.theguardian.com/technology/2016/jun/12/nick-bostrom-artificial-intelligence-machine"
      ],
      [
        "Holden Karnofsky",
        "https://en.wikipedia.org/wiki/Holden_Karnofsky"
      ],
      [
        "Open Philanthropy",
        "https://www.openphilanthropy.org/"
      ],
      [
        "the most important century",
        "https://www.cold-takes.com/most-important-century/"
      ]
    ]
  },
  {
    "question": "We\u2019re going to merge with the machines so this will never be a problem, right?",
    "answer": "The concept of \u201cmerging with machines,\u201d as popularized by Ray Kurzweil, is the idea that we will be able to put computerized elements into our brains that enhance us to the point where we ourselves are the AI, instead of creating AI outside of ourselves.\n\nWhile this is a possible outcome, there is little reason to suspect that it is the most probable. The amount of computing power in your smart-phone took up an entire room of servers 30 years ago. Computer technology starts big, and then gets refined. Therefore, if \u201cmerging with the machines\u201d requires hardware that can fit inside our brain, it may lag behind the first generations of the technology being developed. This concept of merging also supposes that we can even figure out how to implant computer chips that interface with our brain in the first place, we can do it before the invention of advanced AI, society will accept it, and that computer implants can actually produce major intelligence gains in the human brain. Even if we could successfully enhance ourselves with brain implants before the invention of Artificial Superintelligence (ASI), there is no way to guarantee that this would protect us from negative outcomes, and an ASI with ill-defined goals could still pose a threat to us.\n\nIt's not that Ray Kurzweil's ideas are impossible, it's just that his predictions are too specific, confident, and reliant on strange assumptions.\n",
    "tags": [
      {
        "tag": "ray kurzweil",
        "link": "https://stampy.ai/wiki/Ray_kurzweil"
      }
    ],
    "links to": []
  },
  {
    "question": "What actions can I take in under five minutes to contribute to the cause of AI safety?",
    "answer": "There are two different reasons you might be looking for a 5 minute contribution.\n\nIf you are looking to only spend five minutes total, you can:\n\nIF you are looking for a small action which will start things moving, you might consider:\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "What approaches are AI alignment organizations working on?",
    "answer": "Each major organization has a different approach. The research agendas are detailed and complex (see also AI Watch). Getting more brains working on any of them (and more money to fund them) may pay off in a big way, but it\u2019s very hard to be confident which (if any) of them will actually work.\n\nThe following is a massive oversimplification, each organization actually pursues many different avenues of research, read the 2021 AI Alignment Literature Review and Charity Comparison for much more detail. That being said:\n\nscalable mechanistic interpretability, and history and philosophy of alignment.\n\nThere are many other projects around AI Safety, such as the Windfall clause, Rob Miles\u2019s YouTube channel, AI Safety Support, etc.\n",
    "tags": [
      {
        "tag": "organizations",
        "link": "https://stampy.ai/wiki/Organizations"
      }
    ],
    "links to": [
      [
        "research agendas are detailed and complex",
        "https://www.lesswrong.com/tag/research-agendas"
      ],
      [
        "AI Watch",
        "https://aiwatch.issarice.com/"
      ],
      [
        "2021 AI Alignment Literature Review and Charity Comparison",
        "https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison"
      ],
      [
        "the Windfall clause",
        "https://www.youtube.com/watch?v=7i_f4Kbpgn4"
      ],
      [
        "Rob Miles\u2019s YouTube channel",
        "https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg"
      ],
      [
        "AI Safety Support",
        "https://www.aisafetysupport.org/"
      ]
    ]
  },
  {
    "question": "What are &quot;human values&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "human values",
        "link": "https://stampy.ai/wiki/Human_values"
      }
    ],
    "links to": []
  },
  {
    "question": "What are &quot;scaling laws&quot; and how are they relevant to safety?",
    "answer": "<b>Scaling laws</b> are observed trends on the performance of large machine learning models.\n\nIn the field of ML, better performance is usually achieved through better algorithms, better inputs, or using larger amounts of parameters, computing power, or data. Since the 2010s, advances in deep learning have shown experimentally that the easier and faster returns come from <b>scaling</b>, an observation that has been described by Richard Sutton as the <i>bitter lesson</i>.\n\nWhile deep learning as a field has long struggled to scale models up while retaining learning capability (with such problems as catastrophic interference), more recent methods, especially the Transformer model architecture, were able to <i>just work</i> by feeding them more data, and as the meme goes, stacking more layers.\n\nMore surprisingly, performance (in terms of absolute likelihood loss, a standard measure) appeared to increase <i>smoothly</i> with compute, or dataset size, or parameter count. Which gave rise to <b>scaling laws</b>, the trend lines suggested by performance gains, from which returns on data/compute/time investment could be extrapolated.\n\nA companion to this purely descriptive law (no strong theoretical explanation of the phenomenon has been found yet), is the <b>scaling hypothesis</b>, which Gwern Branwen describes:\n\nThe <i>strong scaling hypothesis</i> is that, once we find a scalable architecture like self-attention or convolutions, [...] we can simply train ever larger [neural networks] and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks &amp; data.\nThe scaling laws, if the above hypothesis holds, become highly relevant to safety insofar capability gains become conceptually easier to achieve: no need for clever designs to solve a given task, just throw more processing at it and it will eventually yield. As Paul Christiano observes:\n\nIt now seems possible that we could build \u201cprosaic\u201d AGI, which can replicate human behavior but doesn\u2019t involve qualitatively new ideas about \u201chow intelligence works\u201d.\nWhile the scaling laws still hold experimentally at the time of this writing (July 2022), whether they'll continue up to safety-relevant capabilities is still an open problem.\n",
    "tags": [
      {
        "tag": "scaling laws",
        "link": "https://stampy.ai/wiki/Scaling_laws"
      },
      {
        "tag": "computing overhang",
        "link": "https://stampy.ai/wiki/Computing_overhang"
      }
    ],
    "links to": [
      [
        "bitter lesson",
        "http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
      ],
      [
        "catastrophic interference",
        "https://en.wikipedia.org/wiki/Catastrophic_interference"
      ],
      [
        "stacking more layers",
        "https://www.gwern.net/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg"
      ],
      [
        "Gwern Branwen describes",
        "https://www.gwern.net/Scaling-hypothesis#scaling-hypothesis"
      ],
      [
        "Paul Christiano observes",
        "https://ai-alignment.com/prosaic-ai-control-b959644d79c2"
      ]
    ]
  },
  {
    "question": "What are Encultured working on?",
    "answer": "See Encultured AI: Building a Video Game.\n\nEncultured are making a multiplayer online video game as a test environment for AI: an aligned AI should be able to play the game without ruining the fun or doing something obviously destructive like completely taking over the world, even if it has this capabilities. This seems roughly analogous to setting an AGI loose on the real world.\n\nMotivation: Andrew Critch is primarily concerned about a multipolar AI scenario: there are multiple actors with comparably powerful AI, on the cusp of recursive self improvement. The worst case is a race, and even though each actor would want to take more time checking their AGI for safety, worry that another actor will deploy will push each actor to take shortcuts and try to pull off a world-saving act. Instead of working directly on AI, which can accelerate timelines and encourage racing, creating this standardized test environment where alignment failures are observable is one component of a good global outcome.\n",
    "tags": [],
    "links to": [
      [
        "Encultured AI: Building a Video Game",
        "https://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game"
      ],
      [
        "multipolar AI scenario",
        "https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"
      ]
    ]
  },
  {
    "question": "What are Scott Garrabrant and Abram Demski working on?",
    "answer": "They are working on fundamental problems like embeddedness, decision theory, logical counterfactuals, and more. A big advance was Cartesian Frames, a formal model of agency, and Finite Factored Sets which reframes time in a way which is more compatible with agency.\n",
    "tags": [
      {
        "tag": "miri",
        "link": "https://stampy.ai/wiki/Miri"
      },
      {
        "tag": "decision theory",
        "link": "https://stampy.ai/wiki/Decision_theory"
      },
      {
        "tag": "embedded agency",
        "link": "https://stampy.ai/wiki/Embedded_agency"
      },
      {
        "tag": "agency",
        "link": "https://stampy.ai/wiki/Agency"
      }
    ],
    "links to": [
      [
        "embeddedness, decision theory, logical counterfactuals",
        "https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"
      ],
      [
        "Cartesian Frames",
        "https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"
      ],
      [
        "Finite Factored Sets",
        "https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets"
      ]
    ]
  },
  {
    "question": "What are alternate phrasings for?",
    "answer": "Alternate phrasings are used to improve the semantic search which Stampy uses to serve people questions, by giving alternate ways to say a question which might trigger a match when the main wording won't. They should generally only be used when there is a significantly different wording, rather than for only very minor changes.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "What are brain-computer interfaces?",
    "answer": "A brain-computer interface (BCI) is a direct communication pathway between the brain and a computer device. BCI research is heavily funded, and has already met dozens of successes. Three successes in human BCIs are a device that restores (partial) sight to the blind, cochlear implants that restore hearing to the deaf, and a device that allows use of an artificial hand by direct thought.\n\nSuch device restore impaired functions, but many researchers expect to also augment and improve normal human abilities with BCIs. Ed Boyden is researching these opportunities as the lead of the Synthetic Neurobiology Group at MIT. Such devices might hasten the arrival of an intelligence explosion, if only by improving human intelligence so that the hard problems of AI can be solved more rapidly.\n\nSee also:\n\nWikipedia, Brain-computer interface\n",
    "tags": [
      {
        "tag": "outdated",
        "link": "https://stampy.ai/wiki/Outdated"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      },
      {
        "tag": "brain-computer interfaces",
        "link": "https://stampy.ai/wiki/Brain-computer_interfaces"
      }
    ],
    "links to": [
      [
        "a device",
        "http://archives.cnn.com/2002/HEALTH/06/13/cov.bionic.eye/index.html"
      ],
      [
        "cochlear implants",
        "https://en.wikipedia.org/wiki/Cochlear_implant"
      ],
      [
        "a device",
        "https://pubmed.ncbi.nlm.nih.gov/16838014/"
      ],
      [
        "Ed Boyden",
        "http://edboyden.org/"
      ],
      [
        "Synthetic Neurobiology Group",
        "http://syntheticneurobiology.org/"
      ],
      [
        "Brain-computer interface",
        "https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface"
      ]
    ]
  },
  {
    "question": "What are language models?",
    "answer": "<strong>Language Models</strong> are a class of AI trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a transformer, which follows consistent scaling laws with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      },
      {
        "tag": "language models",
        "link": "https://stampy.ai/wiki/Language_models"
      }
    ],
    "links to": [
      [
        "AI",
        "https://www.lesswrong.com/tag/ai"
      ],
      [
        "transformer",
        "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
      ],
      [
        "scaling laws",
        "https://www.lesswrong.com/tag/scaling-laws"
      ]
    ]
  },
  {
    "question": "What are mesa-optimizers?",
    "answer": "",
    "tags": [
      {
        "tag": "mesa-optimization",
        "link": "https://stampy.ai/wiki/Mesa-optimization"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": []
  },
  {
    "question": "What are plausible candidates for &quot;pivotal acts&quot;?",
    "answer": "Pivotal acts are acts that substantially change the direction humanity will have taken in 1 billion years. The term is used to denote positive changes, as opposed to existential catastrophe.\n\nAn obvious pivotal act would be to create a sovereign AGI aligned with humanity's best interests. An act that would greatly increase the chance of another pivotal act would also count as pivotal.\n\nPivotal acts often lay outside the Overton window. One such example is stopping or strongly delaying the development of an unaligned (or any) AGI through drastic means such as nanobots which melt all advanced processors, or the disabling of all AI researchers. Eliezer mentions these in AGI Ruin: A List of Lethalities. Andrew Critch argues against such an unilateral pivotal act in \u201cPivotal Act\u201d Intentions: Negative Consequences and Fallacious Arguments.\n\nFor more details, see arbital.\n",
    "tags": [],
    "links to": [
      [
        "sovereign AGI",
        "https://arbital.com/p/Sovereign/"
      ],
      [
        "Overton window",
        "https://en.wikipedia.org/wiki/Overton_window"
      ],
      [
        "AGI Ruin: A List of Lethalities",
        "https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"
      ],
      [
        "\u201cPivotal Act\u201d Intentions: Negative Consequences and Fallacious Arguments",
        "https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"
      ],
      [
        "arbital",
        "https://arbital.com/p/pivotal/"
      ]
    ]
  },
  {
    "question": "What are some AI alignment research agendas currently being pursued?",
    "answer": "Research at the Alignment Research Center is led by Paul Christiano, best known for introducing the \u201cIterated Distillation and Amplification\u201d and \u201cHumans Consulting HCH\u201d approaches. He and his team are now <i>\u201ctrying to figure out how to train ML systems to answer questions by straightforwardly \u2018translating\u2019 their beliefs into natural language rather than by reasoning about what a human wants to hear.\u201d</i>\n\nChris Olah (after work at DeepMind and OpenAI) recently launched Anthropic, an AI lab focussed on the safety of large models. While his previous work was concerned with \u201ctransparency\u201d and \u201cinterpretability\u201d of large neural networks, especially vision models, Anthropic is focussing more on large language models, among other things working towards a <i>\"general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless\".</i>\n\nStuart Russell and his team at the Center for Human-Compatible Artificial Intelligence (CHAI) have been working on inverse reinforcement learning (where the AI infers human values from observing human behavior) and corrigibility, as well as attempts to disaggregate neural networks into \u201cmeaningful\u201d subcomponents (see Filan, et al.\u2019s \u201cClusterability in neural networks\u201d and Hod et al.'s \u201cDetecting modularity in deep neural networks\u201d).\n\nAlongside the more abstract \u201cagent foundations\u201d work they have become known for, MIRI recently announced their \u201cVisible Thoughts Project\u201d to test the hypothesis that <i>\u201cLanguage models can be made more understandable (and perhaps also more capable, though this is not the goal) by training them to produce visible thoughts.\u201d</i>\n\nOpenAI have recently been doing work on iteratively summarizing books (summarizing, and then summarizing the summary, etc.) as a method for scaling human oversight.\n\nStuart Armstrong\u2019s recently launched AlignedAI are mainly working on concept extrapolation from familiar to novel contexts, something he believes is \u201cnecessary and almost sufficient\u201d for AI alignment.\n\nRedwood Research (Buck Shlegeris, et al.) are trying to \u201chandicap' GPT-3 to only produce non-violent completions of text prompts. <i>\u201cThe idea is that there are many reasons we might ultimately want to apply some oversight function to an AI model, like \u2018don't be deceitful\u2019, and if we want to get AI teams to apply this we need to be able to incorporate these oversight predicates into the original model in an efficient manner.\u201d</i>\n\nOught is an independent AI safety research organization led by Andreas Stuhlm\u00fcller and Jungwon Byun. They are researching methods for breaking up complex, hard-to-verify tasks into simpler, easier-to-verify tasks, with the aim of allowing us to maintain effective oversight over AIs.\n",
    "tags": [
      {
        "tag": "existential risk",
        "link": "https://stampy.ai/wiki/Existential_risk"
      },
      {
        "tag": "research agendas",
        "link": "https://stampy.ai/wiki/Research_agendas"
      }
    ],
    "links to": [
      [
        "Alignment Research Center",
        "http://alignmentresearchcenter.org/"
      ],
      [
        "Paul Christiano",
        "https://paulfchristiano.com/"
      ],
      [
        "\u201cIterated Distillation and Amplification\u201d",
        "https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616"
      ],
      [
        "\u201cHumans Consulting HCH\u201d",
        "https://ai-alignment.com/humans-consulting-hch-f893f6051455"
      ],
      [
        "Chris Olah",
        "https://colah.github.io/about.html"
      ],
      [
        "DeepMind",
        "https://en.wikipedia.org/wiki/DeepMind"
      ],
      [
        "OpenAI",
        "https://en.wikipedia.org/wiki/OpenAI"
      ],
      [
        "Anthropic",
        "https://www.anthropic.com/"
      ],
      [
        "\u201ctransparency\u201d and \u201cinterpretability\u201d of large neural networks",
        "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/"
      ],
      [
        "Stuart Russell",
        "https://en.wikipedia.org/wiki/Stuart_J._Russell"
      ],
      [
        "Center for Human-Compatible Artificial Intelligence",
        "https://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence"
      ],
      [
        "inverse reinforcement learning",
        "https://arxiv.org/abs/1806.06877"
      ],
      [
        "corrigibility",
        "https://intelligence.org/files/CorrigibilityAISystems.pdf"
      ],
      [
        "\u201cClusterability in neural networks\u201d",
        "https://arxiv.org/abs/2103.03386"
      ],
      [
        "\u201cDetecting modularity in deep neural networks",
        "https://openreview.net/forum?id=tFQyjbOz34"
      ],
      [
        "\u201cagent foundations\u201d",
        "https://intelligence.org/files/TechnicalAgenda.pdf"
      ],
      [
        "MIRI",
        "https://intelligence.org/"
      ],
      [
        "\u201cVisible Thoughts Project\u201d",
        "https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement"
      ],
      [
        "OpenAI",
        "https://en.wikipedia.org/wiki/OpenAI"
      ],
      [
        "iteratively summarizing books",
        "https://openai.com/blog/summarizing-books/"
      ],
      [
        "AlignedAI",
        "https://buildaligned.ai/"
      ],
      [
        "concept extrapolation",
        "https://www.alignmentforum.org/s/u9uawicHx7Ng7vwxA"
      ],
      [
        "Redwood Research",
        "https://www.redwoodresearch.org/"
      ]
    ]
  },
  {
    "question": "What are some good books about AGI safety?",
    "answer": "<i>The Alignment Problem</i> (2020) by Brian Christian is the most recent in-depth guide to the field.\n\nThe book which first made the case to the public is Nick Bostrom\u2019s <i>Superintelligence</i> (2014). It gives an excellent overview of the state of the field (as it was then) and makes a strong case for the subject being important, as well as exploring many fascinating adjacent topics. However, it does not cover newer developments, such as mesa-optimizers or language models.\n\nThere's also <i>Human Compatible</i> (2019) by Stuart Russell, which gives a more up-to-date review of developments, with an emphasis on the approaches that the Center for Human-Compatible AI are working on, such as cooperative inverse reinforcement learning. There's a good review/summary on SlateStarCodex.\n\nAlthough not limited to AI safety, <i>The AI Does Not Hate You</i> (2020) is an entertaining and accessible outline of both the core issues and an exploration of some of the community and culture of the people working on it.\n\nVarious other books explore the issues in an informed way, such as Toby Ord\u2019s <i>The Precipice</i> (2020), Max Tegmark\u2019s <i>Life 3.0</i> (2017), Yuval Noah Harari\u2019s <i>Homo Deus</i> (2016), Stuart Armstrong\u2019s <i>Smarter Than Us</i> (2014), and Luke Muehlhauser\u2019s <i>Facing the Intelligence Explosion</i> (2013).\n",
    "tags": [
      {
        "tag": "literature",
        "link": "https://stampy.ai/wiki/Literature"
      }
    ],
    "links to": [
      [
        "The Alignment Problem",
        "https://brianchristian.org/the-alignment-problem/"
      ],
      [
        "Superintelligence",
        "https://publicism.info/philosophy/superintelligence/"
      ],
      [
        "mesa-optimizers",
        "/wiki/What_are_mesa-optimizers%3F"
      ],
      [
        "language models",
        "/wiki/What_are_language_models%3F"
      ],
      [
        "Human Compatible",
        "https://en.wikipedia.org/wiki/Human_Compatible"
      ],
      [
        "review/summary on SlateStarCodex",
        "https://slatestarcodex.com/2020/01/30/book-review-human-compatible/"
      ],
      [
        "The AI Does Not Hate You",
        "https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795"
      ],
      [
        "Toby Ord",
        "http://www.tobyord.com/"
      ],
      [
        "The Precipice",
        "https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity"
      ],
      [
        "Max Tegmark",
        "https://en.wikipedia.org/wiki/Max_Tegmark"
      ],
      [
        "Life 3.0",
        "https://en.wikipedia.org/wiki/Life_3.0"
      ],
      [
        "Yuval Noah Harari",
        "https://www.ynharari.com/"
      ],
      [
        "Homo Deus",
        "https://www.ynharari.com/book/homo-deus/"
      ],
      [
        "Stuart Armstrong",
        "https://www.fhi.ox.ac.uk/team/stuart-armstrong/"
      ],
      [
        "Smarter Than Us",
        "https://smarterthan.us/toc/"
      ],
      [
        "Luke Muehlhauser",
        "http://lukeprog.com/"
      ],
      [
        "Facing the Intelligence Explosion",
        "https://intelligenceexplosion.com/"
      ]
    ]
  },
  {
    "question": "What are some good podcasts about AI alignment?",
    "answer": "All the content below is in English:\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "What are some good resources on AI alignment?",
    "answer": "",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "resources",
        "link": "https://stampy.ai/wiki/Resources"
      },
      {
        "tag": "literature",
        "link": "https://stampy.ai/wiki/Literature"
      }
    ],
    "links to": []
  },
  {
    "question": "What are some objections to the importance of AI alignment?",
    "answer": "S\u00f8ren Elverlin has compiled a list of counter-arguments and suggests dividing them into two kinds: weak and strong.\n\nWeak counter-arguments point to problems with the \"standard\" arguments (as given in, e.g., Bostrom\u2019s <i>Superintelligence</i>), especially shaky models and assumptions that are too strong. These arguments are often of a substantial quality and are often presented by people who themselves worry about AI safety. Elverin calls these objections \u201cweak\u201d because they do not attempt to imply that the probability of a bad outcome is close to zero: <i>\u201cFor example, even if you accept Paul Christiano's arguments against \u201cfast takeoff\u201d, they only drive the probability of this down to about 20%. Weak counter-arguments are interesting, but the decision to personally focus on AI safety doesn't strongly depend on the probability \u2013 anything above 5% is clearly a big enough deal that it doesn't make sense to work on other things.\u201d</i>\n\nStrong arguments argue that the probability of existential catastrophe due to misaligned AI is tiny, usually by some combination of claiming that AGI is impossible or very far away. For example, Michael Littman has suggested that as (he believes) we\u2019re so far from AGI, there will be a long period of human history wherein we\u2019ll have ample time to grow up alongside powerful AIs and figure out how to align them.\n\nElverlin opines that <i>\u201cThere are few arguments that are both high-quality and strong enough to qualify as an \u2018objection to the importance of alignment\u2019.\u201d</i> He suggests Rohin Shah's arguments for \u201calignment by default\u201d as one of the better candidates.\n\nMIRI's April fools \"Death With Dignity\" strategy might be seen as an argument against the importance of working on alignment, but only in the sense that we might have almost no hope of solving it. In the same category are the \u201csomething else will kill us first, so there\u2019s no point worrying about AI alignment\u201d arguments.\n",
    "tags": [],
    "links to": [
      [
        "S\u00f8ren Elverlin",
        "https://aisafety.com/author/soeren-elverlin/"
      ],
      [
        "<i>Superintelligence</i>",
        "https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
      ],
      [
        "Paul Christiano's arguments against \u201cfast takeoff\u201d",
        "https://www.alignmentforum.org/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds"
      ],
      [
        "Michael Littman",
        "https://en.wikipedia.org/wiki/Michael_L._Littman"
      ],
      [
        "suggested",
        "https://www.youtube.com/watch?v=c9AbECvRt20&amp;t=1559s"
      ],
      [
        "Rohin Shah's arguments for \u201calignment by default\u201d",
        "https://aiimpacts.org/conversation-with-rohin-shah/"
      ],
      [
        "MIRI",
        "https://intelligence.org/"
      ],
      [
        "\"Death With Dignity\" strategy",
        "https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"
      ]
    ]
  },
  {
    "question": "What are some of the most impressive recent advances in AI capabilities?",
    "answer": "GPT-3 showed that transformers are capable of a vast array of natural language tasks, codex/copilot extended this into programming. One demonstrations of GPT-3 is Simulated Elon Musk lives in a simulation. Important to note that there are several much better language models, but they are not publicly available.\n\nDALL-E and DALL-E 2 are among the most visually spectacular.\n\nMuZero, which learned Go, Chess, and many Atari games without any directly coded info about those environments. The graphic there explains it, this seems crucial for being able to do RL in novel environments. We have systems which we can drop into a wide variety of games and they just learn how to play. The same algorithm was used in Tesla's self-driving cars to do complex route finding. These things are general.\n\nGenerally capable agents emerge from open-ended play - Diverse procedurally generated environments provide vast amounts of training data for AIs to learn generally applicable skills. Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning shows how these kind of systems can be trained to follow instructions in natural language.\n\nGATO shows you can distill 600+ individually trained tasks into one network, so we're not limited by the tasks being fragmented.\n",
    "tags": [
      {
        "tag": "capabilities",
        "link": "https://stampy.ai/wiki/Capabilities"
      },
      {
        "tag": "language models",
        "link": "https://stampy.ai/wiki/Language_models"
      }
    ],
    "links to": [
      [
        "codex/copilot",
        "https://copilot.github.com/"
      ],
      [
        "Simulated Elon Musk lives in a simulation",
        "https://www.lesswrong.com/posts/oBPPFrMJ2aBK6a6sD/simulated-elon-musk-lives-in-a-simulation"
      ],
      [
        "DALL-E",
        "https://openai.com/blog/dall-e/"
      ],
      [
        "DALL-E 2",
        "https://openai.com/dall-e-2/"
      ],
      [
        "MuZero",
        "https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules"
      ],
      [
        "Tesla's self-driving cars to do complex route finding",
        "https://youtu.be/j0z4FweCy4M?t=4918"
      ],
      [
        "Generally capable agents emerge from open-ended play",
        "https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play"
      ],
      [
        "Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning",
        "https://www.deepmind.com/publications/creating-interactive-agents-with-imitation-learning"
      ],
      [
        "GATO",
        "https://www.deepmind.com/publications/a-generalist-agent"
      ]
    ]
  },
  {
    "question": "What are some specific open tasks on Stampy?",
    "answer": "Other than the usual fare of writing and processing and organizing questions and answers, here are some specific open tasks:\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?",
    "answer": "AI alignment is the research field focused on trying to give us the tools to align AIs to specific goals, such as human values. This is crucial when they are highly competent, as a misaligned superintelligence could be the end of human civilization.\n\nAGI safety is the field trying to make sure that when we build Artificial General Intelligences they are safe and do not harm humanity. It overlaps with AI alignment strongly, in that misalignment of AI would be the main cause of unsafe behavior in AGIs, but also includes misuse and other governance issues.\n\nAI existential safety is a slightly broader term than AGI safety, including AI risks which pose an existential threat without necessarily being as general as humans.\n\nAI safety was originally used by the existential risk reduction movement for the work done to reduce the risks of misaligned superintelligence, but has also been adopted by researchers and others studying nearer term and less catastrophic risks from AI in recent years.\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "What are the different possible AI takeoff speeds?",
    "answer": "A slow takeoff is where AI capabilities improve gradually, giving us plenty of time to adapt. In a moderate takeoff we might see accelerating progress, but we still won\u2019t be caught off guard by a dramatic change. Whereas, in a fast or hard takeoff AI would go from being not very generally competent to sufficiently superhuman to control the future too fast for humans to course correct if something goes wrong.\n\nThe article Distinguishing definitions of takeoff goes into more detail on this.\n",
    "tags": [
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "Distinguishing definitions of takeoff",
        "https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"
      ]
    ]
  },
  {
    "question": "What are the different versions of decision theory?",
    "answer": "",
    "tags": [
      {
        "tag": "decision theory",
        "link": "https://stampy.ai/wiki/Decision_theory"
      }
    ],
    "links to": []
  },
  {
    "question": "What are the ethical challenges related to whole brain emulation?",
    "answer": "Unless there was a way to cryptographically ensure otherwise, whoever runs the emulation has basically perfect control over their environment and can reset them to any state they were previously in. This opens up the possibility of powerful interrogation and torture of digital people.\n\nImperfect uploading might lead to damage that causes the EM to suffer while still remaining useful enough to be run for example as a test subject for research. We would also have greater ability to modify digital brains. Edits done for research or economic purposes might cause suffering. See this fictional piece for an exploration of how a world with a lot of EM suffering might look like.\n\nThese problems are exacerbated by the likely outcome that digital people can be run much faster than biological humans, so it would be plausibly possible to have an EM run for hundreds of subjective years in minutes or hours without having checks on the wellbeing of the EM in question.\n",
    "tags": [
      {
        "tag": "whole brain emulation",
        "link": "https://stampy.ai/wiki/Whole_brain_emulation"
      }
    ],
    "links to": [
      [
        "cryptographically ensure otherwise",
        "https://www.lesswrong.com/posts/vit9oWGj6WgXpRhce/secure-homes-for-digital-people"
      ],
      [
        "this",
        "https://qntm.org/mmacevedo"
      ]
    ]
  },
  {
    "question": "What are the main sources of AI existential risk?",
    "answer": "A comprehensive list of major contributing factors to AI being a threat to humanity's future is maintained on by Daniel Kokotajlo on the Alignment Forum.\n",
    "tags": [
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": [
      [
        "Alignment Forum",
        "https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk"
      ]
    ]
  },
  {
    "question": "What are the potential benefits of AI as it grows increasingly sophisticated?",
    "answer": "It\u2019s difficult to tell at this stage, but AI will enable many developments that could be terrifically beneficial if managed with enough foresight and care. For example, menial tasks could be automated, which could give rise to a society of abundance, leisure, and flourishing, free of poverty and tedium. As another example, AI could also improve our ability to understand and manipulate complex biological systems, unlocking a path to drastically improved longevity and health, and to conquering disease.\n",
    "tags": [
      {
        "tag": "benefits",
        "link": "https://stampy.ai/wiki/Benefits"
      }
    ],
    "links to": []
  },
  {
    "question": "What are the style guidelines for writing for Stampy?",
    "answer": "Avoid directly responding to the question in the answer, repeat the relevant part of the question instead. For example, if the question is \"Can we do X\", answer \"We might be able to do X, if we can do Y\", not \"Yes, if we can manage Y\". This way, the answer will also work for the questions \"Why can't we do X\" and \"What would happen if we tried to do X\".\n\nLinking to external sites is strongly encouraged, one of the most valuable things Stampy can do is help people find other parts of the alignment information ecosystem.\n\nConsider enclosing newly introduced terms, likely to be unfamiliar to many readers, in speech marks. If unsure, Google the term (in speech marks!) and see if it shows up anywhere other than LessWrong, the Alignment Forum, etc. Be judicious, as it's easy to use too many, but used carefully they can psychologically cushion newbies from a lot of unfamiliar terminology - in this context they're saying something like \"we get that we're hitting you with a lot of new vocab, and you might not know what this term means yet\".\n\nWhen selecting related questions, there shouldn't be more than four unless there's a really good reason for that (some questions are asking for it, like the \"Why can't we just...\" question). It's also recommended to include at least one more \"enticing\" question to draw users in (relating to the more sensational, sci-fi, philosophical/ethical side of things) alongside more bland/neutral questions.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "needs work",
        "link": "https://stampy.ai/wiki/Needs_work"
      }
    ],
    "links to": []
  },
  {
    "question": "What can I do to contribute to AI safety?",
    "answer": "It\u2019s pretty dependent on what skills you have and what resources you have access to. The largest option is to pursue a career in AI Safety research. Another large option is to pursue a career in AI policy, which you might think is even more important than doing technical research.\n\nSmaller options include donating money to relevant organizations, talking about AI Safety as a plausible career path to other people or considering the problem in your spare time.\n\nIt\u2019s possible that your particular set of skills/resources are not suited to this problem. Unluckily, there are many more problems that are of similar levels of importance.\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      },
      {
        "tag": "needs work",
        "link": "https://stampy.ai/wiki/Needs_work"
      }
    ],
    "links to": [
      [
        "career in AI Safety research",
        "https://80000hours.org/career-reviews/artificial-intelligence-risk-research/"
      ],
      [
        "AI policy",
        "https://80000hours.org/articles/ai-policy-guide/"
      ],
      [
        "many more problems",
        "https://concepts.effectivealtruism.org/concepts/existential-risks/"
      ]
    ]
  },
  {
    "question": "What does Elon Musk think about AI safety?",
    "answer": "Elon Musk has expressed his concerns about AI safety many times and founded OpenAI in an attempt to make safe AI more widely distributed (as opposed to allowing a singleton, which he fears would be misused or dangerously unaligned). In a YouTube video from November 2019 Musk stated that there's a lack of investment in AI safety and that there should be a government agency to reduce risk to the public from AI.\n",
    "tags": [
      {
        "tag": "elon musk",
        "link": "https://stampy.ai/wiki/Elon_musk"
      }
    ],
    "links to": [
      [
        "singleton",
        "https://www.nickbostrom.com/fut/singleton.html"
      ],
      [
        "YouTube video",
        "https://www.youtube.com/watch?v=smK9dgdTl40"
      ]
    ]
  },
  {
    "question": "What does Evan Hubinger think of Deception + Inner Alignment?",
    "answer": "Read Evan's research agenda for more information.\n\nIt seems likely that deceptive agents are the default, so a key problem in alignment is to figure out how we can avoid deceptive alignment at every point in the training process. This seems to rely on being able to consistently exert optimization pressure against deception, which probably necessitates interpretability tools.\n\nHis plan to do this right now is acceptability verification: have some predicate that precludes deception, and then check your model for this predicate at every point in training.\n\nOne idea for this predicate is making sure that the agent is myopic, meaning that the AI only cares about the current timestep, so there is no incentive to deceive, because the benefits of deception happen only in the future. This is operationalized as \u201creturn the action that your model of HCH would return, if it received your inputs.\u201d\n",
    "tags": [
      {
        "tag": "stub",
        "link": "https://stampy.ai/wiki/Stub"
      },
      {
        "tag": "inner alignment",
        "link": "https://stampy.ai/wiki/Inner_alignment"
      },
      {
        "tag": "myopia",
        "link": "https://stampy.ai/wiki/Myopia"
      }
    ],
    "links to": [
      [
        "Evan's research agenda",
        "https://www.lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda"
      ],
      [
        "deceptive agents are the default",
        "https://www.google.com/url?q=https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit?usp%3Ddrivesdk&amp;sa=D&amp;source=editors&amp;ust=1661633213188468&amp;usg=AOvVaw1-ALhgrpPnw_4Y0uRozVl_"
      ],
      [
        "myopic",
        "https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia"
      ],
      [
        "HCH",
        "https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch"
      ]
    ]
  },
  {
    "question": "What does MIRI think about technical alignment?",
    "answer": "MIRI thinks technical alignment is really hard, and that we are very far from a solution. However, they think that policy solutions have even less hope. Generally, I think of their approach as supporting a bunch of independent researchers following their own directions, hoping that one of them will find some promise. They mostly buy into the security mindset: we need to know exactly (probably mathematically formally) what we are doing, or the massive optimization pressure will default in ruin.\n\n<b>How does MIRI communicate their view on alignment?</b>\n\nRecently they've been trying to communicate their worldview, in particular, how incredibly doomy they are, perhaps in order to move other research efforts towards what they see as the hard problems.\n",
    "tags": [],
    "links to": [
      [
        "security mindset",
        "https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/"
      ],
      [
        "mathematically formally)",
        "https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem"
      ],
      [
        "How does MIRI communicate their view on alignment?",
        "/wiki/How_does_MIRI_communicate_their_view_on_alignment%3F"
      ],
      [
        "incredibly doomy they are",
        "https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"
      ]
    ]
  },
  {
    "question": "What does Ought aim to do?",
    "answer": "Ought aims to automate and scale open-ended reasoning through Elicit, an AI research assistant. Ought focuses on advancing process-based systems rather than outcome-based ones, which they believe to be both beneficial for improving reasoning in the short term and alignment in the long term. Here they argue that in the long run improving reasoning and alignment converge.\n\nSo Ought\u2019s impact on AI alignment has 2 components: (a) improved reasoning of AI governance &amp; alignment researchers, particularly on long-horizon tasks and (b) pushing supervision of process rather than outcomes, which reduces the optimization pressure on imperfect proxy objectives leading to \u201csafety by construction\u201d. Ought argues that the race between process and outcome-based systems is particularly important because both states may be an attractor.\n",
    "tags": [
      {
        "tag": "ought",
        "link": "https://stampy.ai/wiki/Ought"
      }
    ],
    "links to": [
      [
        "Ought",
        "https://ought.org/"
      ],
      [
        "Elicit",
        "https://ought.org/elicit"
      ],
      [
        "process-based systems",
        "https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"
      ],
      [
        "Here",
        "https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"
      ],
      [
        "particularly on long-horizon tasks",
        "https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Differential_capabilities__Supervising_process_helps_with_long_horizon_tasks"
      ],
      [
        "pushing supervision of process rather than outcomes",
        "https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Alignment__Supervising_process_is_safety_by_construction"
      ],
      [
        "race between process and outcome-based systems",
        "https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Two_attractors__The_race_between_process__and_outcome_based_systems"
      ]
    ]
  },
  {
    "question": "What does generative visualization look like in reinforcement learning?",
    "answer": "Generative visualization for an image classifier means showing an input image which causes the classifier to strongly recognize a feature in that image.\n\nIn reinforcement learning (RL), generative visualization means showing a sequence of observations which make an RL agent strongly want to take a specific action.\n\nThe problem with generative visualization in RL is that the space of possible observations is constrained by the transition function. So, if we optimize the observations to maximize the activation for an action, this will likely result in a sequence of observations which is impossible (incompatible with the transition function).\n\nA way around this is to compute an embedding of possible observation sequences and to optimize in the embedding space instead of the observation space. A relevant paper here is Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents\n",
    "tags": [
      {
        "tag": "interpretability",
        "link": "https://stampy.ai/wiki/Interpretability"
      }
    ],
    "links to": [
      [
        "reinforcement learning",
        "https://en.wikipedia.org/wiki/Reinforcement_learning"
      ],
      [
        "Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents",
        "https://arxiv.org/abs/1904.01318"
      ]
    ]
  },
  {
    "question": "What does the scheme Externalized Reasoning Oversight involve?",
    "answer": "Idea: Make the AGI, which is an LLM, externalize its reasoning (via chain of thought) so that its thoughts are visible to an external overseer, which then verifies that the LLM is not thinking deceptive thoughts.\n\nThis scheme requires:\n\nA key dynamic is that English is not the ideal language to be reasoning: an AGI could be more capable by doing some reasoning not in English. Thus, applying RL to a model exerts pressure towards ideal logical reasoning. However, self-supervised learning (SSL) exerts some pressure back towards just doing text prediction, and hence more of its reasoning being in English. A key question for this research agenda is thus how to put pressure to keep the reasoning externalized, instead of collapsing into internal, more efficient/logical reasoning that is much harder for us to oversee.\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "What harm could a single superintelligence do when it took so many humans to build civilization?",
    "answer": "Superintelligence has an advantage that an early human didn\u2019t \u2013 the entire context of human civilization and technology, there for it to manipulate socially or technologically.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": []
  },
  {
    "question": "What is &quot;Do What I Mean&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "corrigibility",
        "link": "https://stampy.ai/wiki/Corrigibility"
      }
    ],
    "links to": []
  },
  {
    "question": "What is &quot;HCH&quot;?",
    "answer": "<strong>Humans Consulting HCH (HCH) </strong>is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the iterated amplification proposal to solve the alignment problem.\nIt was first described by Paul Christiano in his post Humans Consulting HCH:\nConsider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, <i>if</i> <i>Hugh had access to the question-answering machine</i>.\nThat is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh\u2026\nLet\u2019s call this process HCH, for \u201cHumans Consulting HCH.\u201d",
    "tags": [],
    "links to": [
      [
        "iterated amplification",
        "https://www.lesswrong.com/tag/iterated-amplification"
      ],
      [
        "Humans Consulting HCH",
        "https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch"
      ]
    ]
  },
  {
    "question": "What is &quot;biological cognitive enhancement&quot;?",
    "answer": "There may be genes or molecules that can be modified to improve general intelligence. Researchers have already done this in mice: they over-expressed the NR2B gene, which improved those mice\u2019s memory beyond that of any other mice of any mouse species. Biological cognitive enhancement in humans may cause an intelligence explosion to occur more quickly than it otherwise would.\n\nSee also:\n",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      },
      {
        "tag": "cognitive enhancement",
        "link": "https://stampy.ai/wiki/Cognitive_enhancement"
      }
    ],
    "links to": [
      [
        "have already done this in mice",
        "https://pubmed.ncbi.nlm.nih.gov/10485705/"
      ]
    ]
  },
  {
    "question": "What is &quot;evidential decision theory&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "decision theory",
        "link": "https://stampy.ai/wiki/Decision_theory"
      }
    ],
    "links to": []
  },
  {
    "question": "What is &quot;functional decision theory&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "decision theory",
        "link": "https://stampy.ai/wiki/Decision_theory"
      }
    ],
    "links to": []
  },
  {
    "question": "What is &quot;greater-than-human intelligence&quot;?",
    "answer": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, and more. But one thing that makes humans special is their general intelligence. Humans can intelligently adapt to radically new problems in the urban jungle or outer space for which evolution could not have prepared them. Humans can solve problems for which their brain hardware and software was never trained. Humans can even examine the processes that produce their own intelligence (cognitive neuroscience), and design new kinds of intelligence never seen before (artificial intelligence).\n\nTo possess greater-than-human intelligence, a machine must be able to achieve goals more effectively than humans can, in a wider range of environments than humans can. This kind of intelligence involves the capacity not just to do science and play chess, but also to manipulate the social environment.\n\nComputer scientist Marcus Hutter has described a formal model called AIXI that he says possesses the greatest general intelligence possible. But to implement it would require more computing power than all the matter in the universe can provide. Several projects try to approximate AIXI while still being computable, for example MC-AIXI.\n\nStill, there remains much work to be done before greater-than-human intelligence can be achieved in machines. Greater-than-human intelligence need not be achieved by directly programming a machine to be intelligent. It could also be achieved by whole brain emulation, by biological cognitive enhancement, or by brain-computer interfaces (see below).\n\nSee also:\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "and more",
        "https://www.amazon.com/dp/0521122937/"
      ],
      [
        "cognitive neuroscience",
        "https://en.wikipedia.org/wiki/Cognitive_neuroscience"
      ],
      [
        "artificial intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence"
      ],
      [
        "has described",
        "https://www.amazon.com/dp/3642060528/"
      ],
      [
        "MC-AIXI",
        "https://arxiv.org/PS_cache/arxiv/pdf/0909/0909.0801v1.pdf"
      ]
    ]
  },
  {
    "question": "What is &quot;hedonium&quot;?",
    "answer": "",
    "tags": [],
    "links to": []
  },
  {
    "question": "What is &quot;narrow AI&quot;?",
    "answer": "A<strong> Narrow AI</strong> is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an Artificial General Intelligence. Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.",
    "tags": [
      {
        "tag": "narrow ai",
        "link": "https://stampy.ai/wiki/Narrow_ai"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "Artificial General Intelligence",
        "https://www.lesswrong.com/tag/artificial-general-intelligence"
      ]
    ]
  },
  {
    "question": "What is &quot;superintelligence&quot;?",
    "answer": "A superintelligence is a mind that is much more intelligent than any human. Most of the time, it\u2019s used to discuss hypothetical future AIs.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": []
  },
  {
    "question": "What is &quot;transformative AI&quot;?",
    "answer": "<strong>Transformative AI</strong> is \"[...] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhy8b4kflu8\"><sup><sup id=\"cite_ref-1\" class=\"reference\">&#91;1&#93;</sup></sup></span>&#160;The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms Superintelligent AI and Artificial General Intelligence).\nHolden Karnofsky gives a more detailed definition in another OpenPhil 2016 post:\n[...] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):",
    "tags": [
      {
        "tag": "transformative ai",
        "link": "https://stampy.ai/wiki/Transformative_ai"
      }
    ],
    "links to": [
      [
        "&#91;1&#93;",
        "#cite_note-1"
      ],
      [
        "Superintelligent AI",
        "https://www.lesswrong.com/tag/superintelligence"
      ],
      [
        "Artificial General Intelligence",
        "https://www.lesswrong.com/tag/artificial-general-intelligence"
      ],
      [
        "another OpenPhil 2016 post",
        "https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/"
      ]
    ]
  },
  {
    "question": "What is AI Safety via Debate?",
    "answer": "<strong>Debate</strong> is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7clr966emb9\"><sup><sup id=\"cite_ref-1\" class=\"reference\">&#91;1&#93;</sup></sup></span>&#160;The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue).&#160;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvrcbanw2zz\"><sup><sup id=\"cite_ref-2\" class=\"reference\">&#91;2&#93;</sup></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnwfhnzy6a3e\"><sup><sup id=\"cite_ref-3\" class=\"reference\">&#91;3&#93;</sup></sup></span>",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "&#91;1&#93;",
        "#cite_note-1"
      ],
      [
        "&#91;2&#93;",
        "#cite_note-2"
      ],
      [
        "&#91;3&#93;",
        "#cite_note-3"
      ]
    ]
  },
  {
    "question": "What is AI safety?",
    "answer": "AI safety is a research field that has the goal to avoid bad outcomes from AI systems.\n\nWork on AI safety can be divided into <i>near-term AI safety</i>, and <i>AI existential safety</i>, which is strongly related to <i>AI alignment</i>:\n\nThere are also areas of research which are useful for both near-term, and for existential safety. For example, robustness to distribution shift, and interpretability both help with making current systems safer, and are likely to help with AGI safety.\n",
    "tags": [],
    "links to": [
      [
        "robustness to distribution shift",
        "https://www.lesswrong.com/tag/distributional-shifts"
      ],
      [
        "interpretability",
        "/wiki/What_is_interpretability_and_what_approaches_are_there%3F"
      ]
    ]
  },
  {
    "question": "What is Aligned AI / Stuart Armstrong working on?",
    "answer": "One of the key problems in AI safety is that there are many ways for an AI to generalize off-distribution, so it is very likely that an arbitrary generalization will be unaligned. See the model splintering post for more detail. Stuart's plan to solve this problem is as follows:\n\nThey are currently working on algorithms to accomplish step 1: see Value Extrapolation.\n\nTheir initial operationalization of this problem is the lion and husky problem. Basically: if you train an image model on a dataset of images of lions and huskies, the lions are always in the desert, and the huskies are always in the snow. So the problem of learning a classifier is under-defined: should the classifier be classifying based on the background environment (e.g. snow vs sand), or based on the animal in the image?\n\nA good extrapolation algorithm, on this problem, would generate classifiers that extrapolate in all the different ways[4], and so the 'correct' extrapolation must be in this generated set of classifiers. They have also introduced a new dataset for this, with a similar idea: Happy Faces.\n\nStep 2 could be done in different ways. Possibilities for doing this include: conservatism, generalized deference to humans, or an automated process for removing some goals. like wireheading/deception/killing everyone.\n",
    "tags": [],
    "links to": [
      [
        "model splintering post",
        "https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"
      ],
      [
        "Value Extrapolation",
        "https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering"
      ],
      [
        "Happy Faces",
        "https://www.lesswrong.com/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal"
      ],
      [
        "conservatism",
        "https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"
      ],
      [
        "generalized deference to humans",
        "https://www.lesswrong.com/posts/BeeirdrMXCPYZwgfj/the-blue-minimising-robot-and-model-splintering"
      ]
    ]
  },
  {
    "question": "What is Anthropic&#39;s approach to LLM alignment?",
    "answer": "Anthropic fine tuned a language model to be more helpful, honest and harmless: HHH.\n\nMotivation: The point of this is to:\n\n<b>How can we interpret what all the neurons mean?</b>\n\nChris Olah, the interpretability legend, is working on looking really hard at all the neurons to see what they all mean. The approach he pioneered is circuits: looking at computational subgraphs of the network, called circuits, and interpreting those. Idea: \"decompiling the network into a better representation that is more interpretable\". In-context learning via attention heads, and interpretability here seems useful.\n\nOne result I heard about recently: a linear softmax unit stretches space and encourages neuron monosemanticity (making a neuron represent only one thing, as opposed to firing on many unrelated concepts). This makes the network easier to interpret.\n\nMotivation: The point of this is to get as many bits of information about what neural networks are doing, to hopefully find better abstractions. This diagram gets posted everywhere, the hope being that networks, in the current regime, will become more interpretable because they will start to use abstractions that are closer to human abstractions.\n\n<b>How do you figure out model performance scales?</b>\n",
    "tags": [
      {
        "tag": "language models",
        "link": "https://stampy.ai/wiki/Language_models"
      }
    ],
    "links to": [
      [
        "HHH",
        "https://arxiv.org/abs/2112.00861"
      ],
      [
        "How can we interpret what all the neurons mean?",
        "/wiki/How_can_we_interpret_what_all_the_neurons_mean%3F"
      ],
      [
        "circuits",
        "https://distill.pub/2020/circuits/zoom-in/"
      ],
      [
        "How do you figure out model performance scales?",
        "/wiki/How_do_you_figure_out_model_performance_scales%3F"
      ]
    ]
  },
  {
    "question": "What is Artificial General Intelligence and what will it look like?",
    "answer": "",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "recursive self-improvement",
        "link": "https://stampy.ai/wiki/Recursive_self-improvement"
      },
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      },
      {
        "tag": "comprehensive ai services",
        "link": "https://stampy.ai/wiki/Comprehensive_ai_services"
      },
      {
        "tag": "eric drexler",
        "link": "https://stampy.ai/wiki/Eric_drexler"
      },
      {
        "tag": "nick bostrom",
        "link": "https://stampy.ai/wiki/Nick_bostrom"
      },
      {
        "tag": "eliezer yudkowsky",
        "link": "https://stampy.ai/wiki/Eliezer_yudkowsky"
      },
      {
        "tag": "cognitive superpowers",
        "link": "https://stampy.ai/wiki/Cognitive_superpowers"
      },
      {
        "tag": "molecular nanotechnology",
        "link": "https://stampy.ai/wiki/Molecular_nanotechnology"
      },
      {
        "tag": "robin hanson",
        "link": "https://stampy.ai/wiki/Robin_hanson"
      }
    ],
    "links to": []
  },
  {
    "question": "What is Conjecture&#39;s epistemology research agenda?",
    "answer": "The alignment problem is really hard to do science on: we are trying to reason about the future, and we only get one shot, meaning that we can't iterate. Therefore, it seems really useful to have a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful alignment research.\n",
    "tags": [
      {
        "tag": "stub",
        "link": "https://stampy.ai/wiki/Stub"
      }
    ],
    "links to": [
      [
        "we can't iterate",
        "https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic"
      ]
    ]
  },
  {
    "question": "What is Conjecture, and what is their team working on?",
    "answer": "Conjecture is an applied org focused on aligning LLMs (Q &amp; A here). Conjecture has short timelines (the org acts like timelines are between ~5-10 year, but some have much shorter timelines, such as 2-4 years), and they think alignment is hard. They take information hazards (specifically ideas that could lead towards better AI capabilities) very seriously, and have a public infohazard document.\n\n<b>What is Conjecture's epistemology research agenda?</b>\n\nThe alignment problem is really hard to do science on: we are trying to reason about the future, and we only get one shot, meaning that we can't iterate. Therefore, it seems really useful to have a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful alignment research.\n\n<b>What is Conjecture's Scalable LLM Interpretability research adgenda?</b>\n\nI don't know much about their research here, other than that they train their own models, which allow them to work on models that are bigger than the biggest publicly available models, which seems like a difference from Redwood.\n\nCurrent interpretability methods are very low level (e.g., \"what does x neuron do\"), which does not help us answer high level questions like \"is this AI trying to kill us\".\n\nThey are trying a bunch of weird approaches, with the goal of scalable mechanistic interpretability, but I do not know what these approaches actually are.\n\nMotivation: Conjecture wants to build towards a better paradigm that will give us a lot more information, primarily from the empirical direction (as distinct from ARC, which is working on interpretability with a theoretical focus).\n\n<b>What is Refine?</b>\n\nRefine is an incubator for new decorrelated alignment \"research bets\". Since no approach is very promising right now for solving alignment, the purpose of this is to come up with a bunch of independent new ideas, and hopefully some of these will work.\n\n<b>What is the goal of Simulacra Theory?</b>\n\nThe goal of this is to create a non-agentic AI, in the form of an LLM, that is capable of accelerating alignment research. The hope is that there is some window between AI smart enough to help us with alignment and the really scary, self improving, consequentialist AI. Some things that this amplifier might do:\n\nA LLM can be thought of as learning the distribution over the next token given by the training data. Prompting the LM is then like conditioning this distribution on the start of the text. A key danger in alignment is applying unbounded optimization pressure towards a specific goal in the world. Conditioning a probability distribution does not behave like an agent applying optimization pressure towards a goal. Hence, this avoids goodhart-related problems, as well as some inner alignment failure.\n\nOne idea to get superhuman work from LLMs is to train it on amplified datasets like really high quality / difficult research. The key problem here is finding the dataset to allow for this.\n\nThere are some ways for this to fail:\n\nConjecture are aware of these problems and are running experiments. Specifically, an operationalization of the inner alignment problem is to make an LLM play chess. This (probably) requires simulating an optimizer trying to win at the game of chess. They are trying to use interpretability tools to find the mesa-optimizers in the chess LLM that is the agent trying to win the game of chess. We haven't ever found a real mesa-optimizer before, and so this could give loads of bits about the nature of inner alignment failure.\n",
    "tags": [],
    "links to": [
      [
        "Conjecture",
        "https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup#Our_Research_Agenda"
      ],
      [
        "here)",
        "https://forum.effectivealtruism.org/posts/QR7yGoFBonY6hege9/connor-leahy-on-conjecture-and-dying-with-dignity"
      ],
      [
        "public infohazard document",
        "https://www.lesswrong.com/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy"
      ],
      [
        "What is Conjecture's epistemology research agenda?",
        "/wiki/What_is_epistemology%3F"
      ],
      [
        "we can't iterate",
        "https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic"
      ],
      [
        "What is Conjecture's Scalable LLM Interpretability research adgenda?",
        "/wiki/What_is_Scalable_LLM_Interpretability%3F"
      ],
      [
        "What is Refine?",
        "/wiki/What_is_Refine%3F"
      ],
      [
        "What is the goal of Simulacra Theory?",
        "/wiki/What_is_the_goal_of_Simulacra_Theory%3F"
      ]
    ]
  },
  {
    "question": "What is David Krueger working on?",
    "answer": "David runs a lab at the University of Cambridge. Some things he is working on include:\n\nFor work done on (1), see: Goal Misgeneralization, a paper that empirically demonstrated examples of inner alignment failure in Deep RL environments. For example, they trained an agent to get closer to cheese in a maze, but where the cheese was always in the top right of a maze in the training set. During test time, when presented with cheese elsewhere, the RL agent navigated to the top right instead of to the cheese: it had learned the mesa objective of \"go to the top right\".\n\nFor work done on (2), see OOD Generalization via Risk Extrapolation, an iterative improvement on robustness to previous methods.\n\nWe've not read about his motivation is for these specific research directions, but these are likely his best starts on how to solve the alignment problem.\n",
    "tags": [
      {
        "tag": "people",
        "link": "https://stampy.ai/wiki/People"
      },
      {
        "tag": "research agendas",
        "link": "https://stampy.ai/wiki/Research_agendas"
      }
    ],
    "links to": [
      [
        "Goal Misgeneralization",
        "https://arxiv.org/abs/2105.14111"
      ],
      [
        "OOD Generalization via Risk Extrapolation",
        "http://proceedings.mlr.press/v139/krueger21a.html"
      ]
    ]
  },
  {
    "question": "What is Dylan Hadfield-Menell&#39;s thesis on?",
    "answer": "Dylan's PhD thesis argues three main claims (paraphrased):\n\nThus, his motivations seem to be modeling AGI coming in some multi-agent form, and also being heavily connected with human operators.\n\nWe're not certain what he is currently working on, but some recent alignment-relevant papers that he has published include:\n\nDylan has also published a number of articles that seem less directly relevant for alignment.\n",
    "tags": [],
    "links to": [
      [
        "Dylan's PhD thesis",
        "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-207.pdf"
      ]
    ]
  },
  {
    "question": "What is GPT-3?",
    "answer": "GPT-3 is the newest and most impressive of the GPT (Generative Pretrained Transformer) series of large transformer-based language models created by OpenAI. It was announced in June 2020, and is 100 times larger than its predecessor GPT-2.<sup id=\"cite_ref-1\" class=\"reference\">&#91;1&#93;</sup>\n\nGwern has several resources exploring GPT-3's abilities, limitations, and implications including:\n\nVox has an article which explains why GPT-3 is a big deal.\n",
    "tags": [
      {
        "tag": "language models",
        "link": "https://stampy.ai/wiki/Language_models"
      }
    ],
    "links to": [
      [
        "GPT",
        "https://www.alignmentforum.org/tag/gpt"
      ],
      [
        "&#91;1&#93;",
        "#cite_note-1"
      ],
      [
        "an article",
        "https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language"
      ]
    ]
  },
  {
    "question": "What is Goodhart&#39;s law?",
    "answer": "",
    "tags": [
      {
        "tag": "goodhart's law",
        "link": "https://stampy.ai/wiki/Goodhart%27s_law"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": []
  },
  {
    "question": "What is John Wentworth&#39;s plan?",
    "answer": "John's plan is:\n\nStep 1: sort out our fundamental confusions about agency\n\nStep 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)\n\nStep 3: \u2026\n\nStep 4: profit!\n\n\u2026 and do all that before AGI kills us all.\n\nHe is working on step 1: figuring out what the heck is going on with agency. His current approach is based on selection theorems: try to figure out what types of agents are selected for in a broad range of environments. Examples of selection pressures include: evolution, SGD, and markets. This is an approach to agent foundations that comes from the opposite direction as MIRI: it's more about observing existing structures (whether they be mathematical or real things in the world like markets or e coli), whereas MIRI is trying to write out some desiderata and then finding mathematical notions that satisfy those desiderata.\n\nTwo key properties that might be selected for are modularity and abstractions.\n\nAbstractions are higher level things that people tend to use to describe things. Like \"Tree\" and \"Chair\" and \"Person\". These are all vague categories that contain lots of different things, but are really useful for narrowing down things. Humans tend to use really similar abstractions, even across different cultures / societies. The Natural Abstraction Hypothesis (NAH) states that a wide variety of cognitive architectures will tend to use similar abstractions to reason about the world. This might be helpful for alignment because we could say things like \"person\" without having to rigorously and precisely say exactly what we mean by person.\n\nThe NAH seems very plausibly true for physical objects in the world, and so it might be true for the inputs to human values. If so, it would be really helpful for AI alignment because understanding this would amount to a solution to the ontology identification problem: we can understand when environments induce certain abstractions, and so we can design this so that the network has the same abstractions as humans.\n\nModularity: In pretty much any selection environment, we see lots of obvious modularity. Biological species have cells and organs and limbs. Companies have departments. We might expect neural networks to be similar, but it is really hard to find modules in neural networks. We need to find the right lens to look through to find this modularity in neural networks. Aiming at this can lead us to really good interpretability.\n",
    "tags": [],
    "links to": [
      [
        "John's plan",
        "https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan"
      ],
      [
        "selection theorems",
        "https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents"
      ],
      [
        "The Natural Abstraction Hypothesis",
        "https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro"
      ],
      [
        "ontology identification problem",
        "https://arbital.com/p/ontology_identification/"
      ],
      [
        "Modularity",
        "https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C"
      ],
      [
        "really hard to find modules",
        "https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity"
      ]
    ]
  },
  {
    "question": "What is MIRI\u2019s mission?",
    "answer": "",
    "tags": [
      {
        "tag": "miri",
        "link": "https://stampy.ai/wiki/Miri"
      }
    ],
    "links to": []
  },
  {
    "question": "What is Refine?",
    "answer": "Refine is an incubator for new decorrelated alignment \"research bets\". Since no approach is very promising right now for solving alignment, the purpose of this is to come up with a bunch of independent new ideas, and hopefully some of these will work.\n",
    "tags": [],
    "links to": []
  },
  {
    "question": "What is Stampy&#39;s copyright?",
    "answer": "",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "What is a &quot;quantilizer&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "quantilizers",
        "link": "https://stampy.ai/wiki/Quantilizers"
      }
    ],
    "links to": []
  },
  {
    "question": "What is a &quot;value handshake&quot;?",
    "answer": "A value handshake is a form of trade between superintelligences, when two AI's with incompatible utility functions meet, instead of going to war, since they have superhuman prediction abilities and likely know the outcome before any attack even happens, they can decide to split the universe into chunks with volumes according to their respective military strength or chance of victory, and if their utility functions are compatible, they might even decide to merge into an AI with an utility function that is the weighted average of the two previous ones.\n\nThis could happen if multiple AI's are active on earth at the same time, and then maybe if at least one of them is aligned with humans, the resulting value handshake could leave humanity in a pretty okay situation.\n\nSee The Hour I First Believed By Scott Alexander for some further thoughts and an introduction to related topics.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "The Hour I First Believed",
        "https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/"
      ]
    ]
  },
  {
    "question": "What is a canonical question on Stampy&#39;s Wiki?",
    "answer": "<b>Canonical questions</b> are the questions which we've checked are in scope and not duplicates, so we want answers to them. They may be edited to represent a class of question more broadly, rather than keeping all their idosyncracies. Once they're answered canonically Stampy will serve them to readers.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Canonical questions",
        "/wiki/Canonical_questions"
      ],
      [
        "scope",
        "/wiki/Scope"
      ]
    ]
  },
  {
    "question": "What is a duplicate question on Stampy&#39;s Wiki?",
    "answer": "An existing question is a duplicate of a new one if it is reasonable to expect whoever asked the new question to be satisfied if they received an answer to the existing question instead.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "What is a follow-up question on Stampy&#39;s Wiki?",
    "answer": "<b>Follow-up questions</b> are responses to an answer which reader might have, either because they want more information or are providing information to Stampy about what they're looking for. We don't expect to have great coverage of the former for a long time because there will be so many, but hopefully we'll be able to handle some of the most common ones.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "answer",
        "/wiki/Answer"
      ]
    ]
  },
  {
    "question": "What is an &quot;agent&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      },
      {
        "tag": "agency",
        "link": "https://stampy.ai/wiki/Agency"
      },
      {
        "tag": "open problem",
        "link": "https://stampy.ai/wiki/Open_problem"
      }
    ],
    "links to": []
  },
  {
    "question": "What is an &quot;intelligence explosion&quot;?",
    "answer": "The intelligence explosion idea was expressed by statistician I.J. Good in 1965:\n\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u2018intelligence explosion\u2019, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.\nThe argument is this: Every year, computers surpass human abilities in new ways. A program written in 1956 was able to prove mathematical theorems, and found a more elegant proof for one of them than Russell and Whitehead had given in <i>Principia Mathematica</i>. By the late 1990s, \u2018expert systems\u2019 had surpassed human skill for a wide range of tasks. In 1997, IBM\u2019s Deep Blue computer beat the world chess champion, and in 2011, IBM\u2019s Watson computer beat the best human players at a much more complicated game: Jeopardy!. Recently, a robot named Adam was programmed with our scientific knowledge about yeast, then posed its own hypotheses, tested them, and assessed the results.\n\nComputers remain far short of human intelligence, but the resources that aid AI design are accumulating (including hardware, large datasets, neuroscience knowledge, and AI theory). We may one day design a machine that surpasses human skill at designing artificial intelligences. After that, this machine could improve its own intelligence faster and better than humans can, which would make it even more skilled at improving its own intelligence. This could continue in a positive feedback loop such that the machine quickly becomes vastly more intelligent than the smartest human being on Earth: an \u2018intelligence explosion\u2019 resulting in a machine superintelligence.\n\nThis is what is meant by the \u2018intelligence explosion\u2019 in this FAQ.\n\nSee also:\n",
    "tags": [
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      },
      {
        "tag": "outdated",
        "link": "https://stampy.ai/wiki/Outdated"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "I.J. Good in 1965",
        "http://www.incompleteideas.net/papers/Good65ultraintelligent.pdf"
      ],
      [
        "found a more elegant proof",
        "http://www.cs.cornell.edu/courses/cs4860/2012fa/MacKenzie-TheAutomationOfProof.pdf"
      ],
      [
        "wide range of tasks",
        "https://www.amazon.com/dp/0521122937/"
      ],
      [
        "Jeopardy!",
        "https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=2&amp;ref=homepage&amp;src=me&amp;pagewanted=all"
      ],
      [
        "a robot named Adam",
        "http://commonsenseatheism.com/wp-content/uploads/2011/02/King-The-Automation-of-Science.pdf"
      ]
    ]
  },
  {
    "question": "What is an &quot;s-risk&quot;?",
    "answer": "",
    "tags": [],
    "links to": []
  },
  {
    "question": "What is artificial general intelligence safety / AI alignment?",
    "answer": "<i>AI alignment</i> is a field that is focused on causing the goals of future superintelligent artificial systems\nto align with human values, meaning that they would behave in a way which was compatible with our survival and flourishing. This may be an extremely hard problem, especially with deep learning, and is likely to determine the outcome of the most important century. Alignment research is strongly interdisciplinary and can include computer science, mathematics, neuroscience, philosophy, and social sciences.\n\n<i>AGI safety</i> is a related concept which strongly overlaps with AI alignment. AGI safety is concerned with making sure that building AGI systems doesn\u2019t cause things to go badly wrong, and the main way in which things can go badly wrong is through misalignment. AGI safety includes policy work that prevents the building of dangerous AGI systems, or reduces misuse risks from AGI systems aligned to actors who don\u2019t have humanity\u2019s best interests in mind.\n",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      },
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": [
      [
        "superintelligent artificial systems",
        "https://en.wikipedia.org/wiki/Superintelligence"
      ],
      [
        "human values",
        "https://www.researchgate.net/publication/347891524_Literature_Review_What_AI_Safety_Researchers_Have_Written_About_the_Nature_of_Human_Values"
      ],
      [
        "extremely hard problem",
        "https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/"
      ],
      [
        "deep learning",
        "https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/"
      ],
      [
        "most important century",
        "https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/"
      ]
    ]
  },
  {
    "question": "What is causal decision theory?",
    "answer": "",
    "tags": [],
    "links to": []
  },
  {
    "question": "What is interpretability and what approaches are there?",
    "answer": "Interpretability is about making machine learning (ML) systems easier to understand. It is hard because the computations of current ML systems often depend on billions of parameters which they learnt from data. Areas of research for making current ML models more understandable are <i>mechanistic interpretability</i>, <i>finding important input features</i>, <i>explaining by examples</i>, <i>natural language explanations</i>, and using ML architectures which are <i>intrinsically interpretable</i>.\n\nYou can read more about different approaches in this overview article which summarizes more than 70 interpretability-related papers, and in the free online book <i>A Guide for Making Black Box Models Explainable</i>.\n\nYou can read more about different approaches in this overview article which summarizes more than 70 interpretability-related papers, and in the free online book <i>A Guide for Making Black Box Models Explainable</i>.\n",
    "tags": [
      {
        "tag": "interpretability",
        "link": "https://stampy.ai/wiki/Interpretability"
      }
    ],
    "links to": [
      [
        "this overview article",
        "https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"
      ],
      [
        "A Guide for Making Black Box Models Explainable",
        "https://christophm.github.io/interpretable-ml-book/"
      ],
      [
        "this overview article",
        "https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"
      ],
      [
        "A Guide for Making Black Box Models Explainable",
        "https://christophm.github.io/interpretable-ml-book/"
      ]
    ]
  },
  {
    "question": "What is meant by &quot;AI takeoff&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      }
    ],
    "links to": []
  },
  {
    "question": "What is neural network modularity?",
    "answer": "If a neural network is <i>modular</i>, that means it consists of clusters (modules) of neurons, such that the neurons within the cluster are strongly connected to each other, but only weakly connected to the rest of the network.\n\nMaking networks more modular is useful if the modules represent concepts which are understandable because this helps understand the whole system better.\n\nRelevant papers about modularity are Neural Networks are Surprisingly Modular, Clusterability in Neural Networks, and Detecting Modularity in Deep Neural Networks.\n",
    "tags": [
      {
        "tag": "interpretability",
        "link": "https://stampy.ai/wiki/Interpretability"
      }
    ],
    "links to": [
      [
        "Neural Networks are Surprisingly Modular",
        "https://arxiv.org/abs/2003.04881v2"
      ],
      [
        "Clusterability in Neural Networks",
        "https://arxiv.org/abs/2103.03386"
      ],
      [
        "Detecting Modularity in Deep Neural Networks",
        "https://openreview.net/forum?id=tFQyjbOz34"
      ]
    ]
  },
  {
    "question": "What is the &quot;control problem&quot;?",
    "answer": "The Control Problem is the problem of preventing artificial superintelligence (ASI) from having a negative impact on humanity. How do we keep a more intelligent being under control, or how do we align it with our values? If we succeed in solving this problem, intelligence vastly superior to ours can take the baton of human progress and carry it to unfathomable heights. Solving our most complex problems could be simple to a sufficiently intelligent machine. If we fail in solving the Control Problem and create a powerful ASI not aligned with our values, it could spell the end of the human race. For these reasons, The Control Problem may be the most important challenge that humanity has ever faced, and may be our last.\n",
    "tags": [
      {
        "tag": "control problem",
        "link": "https://stampy.ai/wiki/Control_problem"
      }
    ],
    "links to": []
  },
  {
    "question": "What is the &quot;long reflection&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": []
  },
  {
    "question": "What is the &quot;orthogonality thesis&quot;?",
    "answer": "",
    "tags": [
      {
        "tag": "orthogonality thesis",
        "link": "https://stampy.ai/wiki/Orthogonality_thesis"
      }
    ],
    "links to": []
  },
  {
    "question": "What is the &quot;windfall clause&quot;?",
    "answer": "The windfall clause is pretty well explained on the Future of Humanity Institute site.\n\nHere's a quick summary: <br />\nIt is an agreement between AI firms to donate significant amounts of any profits made as a consequence of economically transformative breakthroughs in AI capabilities. The donations are intended to help benefit humanity.\n",
    "tags": [
      {
        "tag": "benefits",
        "link": "https://stampy.ai/wiki/Benefits"
      },
      {
        "tag": "definitions",
        "link": "https://stampy.ai/wiki/Definitions"
      }
    ],
    "links to": [
      [
        "on the Future of Humanity Institute site",
        "https://www.fhi.ox.ac.uk/windfallclause/"
      ]
    ]
  },
  {
    "question": "What is the Center for Human Compatible AI (CHAI)?",
    "answer": "CHAI is an academic research organization affiliated with UC Berkeley. It is lead by Stuart Russell, but includes many other professors and grad students pursuing a diverse array of approaches, most of whom are not yet listed here. For more information see their 2022 progress report.\n\nStuart wrote the book Human Compatible, in which he outlines his AGI alignment strategy, which is based on cooperative inverse reinforcement learning (CIRL). The basic idea of CIRL is to play a cooperative game where both the agent and the human are trying to maximize the human's reward, but only the human knows what the human reward is. Since the AGI has uncertainty it will defer to humans and be corrigible.\n\nOther work includes Clusterability in neural networks: try to measure the modularity of neural networks by thinking of the network as a graph and performing the graph n-cut.\n",
    "tags": [
      {
        "tag": "organizations",
        "link": "https://stampy.ai/wiki/Organizations"
      }
    ],
    "links to": [
      [
        "2022 progress report",
        "https://humancompatible.ai/app/uploads/2022/05/CHAI-2022-Progress-Report-3.pdf"
      ],
      [
        "Human Compatible",
        "https://en.wikipedia.org/wiki/Human_Compatible"
      ],
      [
        "Clusterability in neural networks",
        "https://arxiv.org/abs/2103.03386"
      ]
    ]
  },
  {
    "question": "What is the Center on Long-Term Risk (CLR) focused on?",
    "answer": "CLR is focused primarily on reducing suffering-risk (s-risk), where the future has a large negative value. They do foundational research in game theory / decision theory, primarily aimed at multipolar AI scenarios. One result relevant to this work is that transparency can increase cooperation.\n\nUpdate after Jesse Clifton commented: CLR also works on improving coordination for prosaic AI scenarios, risks from malevolent actors and AI forecasting. The Cooperative AI Foundation (CAIF) shares personnel with CLR, but is not formally affiliated with CLR, and does not focus just on s-risks.\n",
    "tags": [],
    "links to": [
      [
        "transparency can increase cooperation",
        "https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79"
      ],
      [
        "Update after Jesse Clifton commented",
        "https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=mqiYR6X8bgY5wKdme"
      ],
      [
        "risks from malevolent actors",
        "https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors"
      ],
      [
        "AI forecasting",
        "https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like"
      ],
      [
        "Cooperative AI Foundation (CAIF)",
        "https://www.cooperativeai.com/foundation"
      ]
    ]
  },
  {
    "question": "What is the DeepMind&#39;s safety team working on?",
    "answer": "DeepMind has both a ML safety team focused on near-term risks, and an alignment team that is working on risks from AGI. The alignment team is pursuing many different research avenues, and is not best described by a single agenda.\n\nSome of the work they are doing is:\n\nSee Rohin's comment for more research that they are doing, including description of some that is currently unpublished so far.\n",
    "tags": [],
    "links to": [
      [
        "ML safety team focused on near-term risks",
        "https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/#long-term-agi-safety-research"
      ],
      [
        "Rohin's comment",
        "https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId=CS9qcdkmDbLHR89s2"
      ]
    ]
  },
  {
    "question": "What is the Stampy project?",
    "answer": "",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "What is the difference between inner and outer alignment?",
    "answer": "The paper Risks from Learned Optimization in Advanced Machine Learning Systems makes the distinction between inner and outer alignment: Outer alignment means making the optimization target of the <i>training process</i> (\u201couter optimization target\u201d e.g.&#160;the <i>loss</i> in supervised learning) aligned with what we want. Inner alignment means making the optimization target of the <i>trained system</i> (\u201cinner optimization target\u201d) aligned with the outer optimization target. A challenge here is that the inner optimization target does not have an explicit representation in current systems, and can differ very much from the outer optimization target (see for example Goal Misgeneralization in Deep Reinforcement Learning).\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/bJLcIBixGj8\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\nSee also this article for an intuitive explanation of inner and outer alignment.\n",
    "tags": [
      {
        "tag": "inner alignment",
        "link": "https://stampy.ai/wiki/Inner_alignment"
      }
    ],
    "links to": [
      [
        "Risks from Learned Optimization in Advanced Machine Learning Systems",
        "https://arxiv.org/abs/1906.01820"
      ],
      [
        "Goal Misgeneralization in Deep Reinforcement Learning",
        "https://arxiv.org/abs/2105.14111"
      ],
      [
        "this article",
        "https://astralcodexten.substack.com/p/deceptively-aligned-mesa-optimizers"
      ]
    ]
  },
  {
    "question": "What is the general nature of the concern about AI alignment?",
    "answer": "The basic concern as AI systems become increasingly powerful is that they won\u2019t do what we want them to do \u2013 perhaps because they aren\u2019t correctly designed, perhaps because they are deliberately subverted, or perhaps because they do what we tell them to do rather than what we really want them to do (like in the classic stories of genies and wishes.) Many AI systems are programmed to have goals and to attain them as effectively as possible \u2013 for example, a trading algorithm has the goal of maximizing profit. Unless carefully designed to act in ways consistent with human values, a highly sophisticated AI trading system might exploit means that even the most ruthless financier would disavow. These are systems that literally have a mind of their own, and maintaining alignment between human interests and their choices and actions will be crucial.\n",
    "tags": [
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": []
  },
  {
    "question": "What kind of questions do we want on Stampy?",
    "answer": "<b>Stampy</b> is focused specifically on AI existential safety (both introductory and technical questions), but does not aim to cover general AI questions or other topics which don't interact strongly with the effects of AI on humanity's long-term future. More technical questions are also in our scope, though replying to all possible proposals is not feasible and this is not a place to submit detailed ideas for evaluation.\n\nWe are interested in:\n\nMore good examples can be found at canonical questions.\n\nWe do not aim to cover:\n\nWe will generally not delete out-of-scope content, but it will be reviewed as low priority to answer, not be marked as a canonical question, and not be served to readers by on Stampy's UI.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "AI existential safety",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence"
      ],
      [
        "canonical questions",
        "/wiki/Canonical_questions"
      ],
      [
        "reviewed",
        "/wiki/Reviewed"
      ],
      [
        "canonical question",
        "/wiki/Canonical_question"
      ],
      [
        "Stampy's UI",
        "https://ui.stampy.ai/"
      ]
    ]
  },
  {
    "question": "What safety problems are associated with whole brain emulation?",
    "answer": "It seems improbable that whole brain emulation (WBE) arrives before neuromorphic AI because a better understanding of the brain would probably help with the development of latter.\n\nEven if WBE were to arrive first, there is some debate on whether it would be safer than synthetic AI. An accelerated WBE might be a safe template for an AGI as it would directly inherit the subject's way of thinking but some safety problems could still arise.\n\nNonetheless, Yudkowsky believes that emulations are probably better even if they are unlikely.\n",
    "tags": [
      {
        "tag": "whole brain emulation",
        "link": "https://stampy.ai/wiki/Whole_brain_emulation"
      },
      {
        "tag": "what about",
        "link": "https://stampy.ai/wiki/What_about"
      }
    ],
    "links to": [
      [
        "neuromorphic AI",
        "https://www.alignmentforum.org/tag/neuromorphic-ai"
      ],
      [
        "it would be safer than synthetic AI",
        "https://intelligence.org/files/SS11Workshop.pdf"
      ],
      [
        "emulations are probably better even if they are unlikely",
        "https://www.youtube.com/watch?v=EUjc1WuyPT8&amp;start=4286"
      ]
    ]
  },
  {
    "question": "What should I read to learn about decision theory?",
    "answer": "abramdemski and Scott Garrabrant's post on decision theory provides a good overview of many aspects of the topic, while Functional Decision Theory: A New Theory of Instrumental Rationality seems to be the most up to date source on current thinking.\n\nFor a more intuitive dive into one of the core problems, Newcomb's problem and regret of rationality is good, and Newcomblike problems are the norm is useful for seeing how it applies in the real world.\n\nThe LessWrong tag for decision theory has lots of additional links for people who want to explore further.\n",
    "tags": [
      {
        "tag": "literature",
        "link": "https://stampy.ai/wiki/Literature"
      },
      {
        "tag": "decision theory",
        "link": "https://stampy.ai/wiki/Decision_theory"
      }
    ],
    "links to": [
      [
        "abramdemski and Scott Garrabrant's post on decision theory",
        "https://www.lesswrong.com/posts/zcPLNNw4wgBX5k8kQ/decision-theory"
      ],
      [
        "Functional Decision Theory: A New Theory of Instrumental Rationality",
        "https://arxiv.org/abs/1710.05060"
      ],
      [
        "Newcomb's problem and regret of rationality",
        "https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality"
      ],
      [
        "Newcomblike problems are the norm",
        "https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm"
      ],
      [
        "LessWrong tag for decision theory",
        "https://www.lesswrong.com/tag/decision-theory"
      ]
    ]
  },
  {
    "question": "What should be marked as a canonical answer on Stampy&#39;s Wiki?",
    "answer": "Canonical answers may be served to readers by Stampy, so only answers which have a reasonably high stamp score should be marked as canonical. All canonical answers are open to be collaboratively edited and updated, and they should represent a consensus response (written from the Stampy Point Of View) to a question which is within Stampy's scope.\n\nAnswers to questions from YouTube comments should not be marked as canonical, and will generally remain as they were when originally written since they have details which are specific to an idiosyncratic question. YouTube answers may be forked into wiki answers, in order to better respond to a particular question, in which case the YouTube question should have its canonical version field set to the new more widely useful question.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "needs work",
        "link": "https://stampy.ai/wiki/Needs_work"
      },
      {
        "tag": "outdated",
        "link": "https://stampy.ai/wiki/Outdated"
      }
    ],
    "links to": [
      [
        "Canonical answers",
        "/wiki/Canonical_answers"
      ]
    ]
  },
  {
    "question": "What sources of information can Stampy use?",
    "answer": "As well as pulling human written answers to AI alignment questions from Stampy's Wiki, Stampy can:\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Stampy's Wiki",
        "/wiki/Stampy%27s_Wiki"
      ]
    ]
  },
  {
    "question": "What subjects should I study at university to prepare myself for alignment research?",
    "answer": "To prepare for AI alignment research, it is important to understand machine learning, and to have a solid grasp of the relevant mathematics such as linear algebra, calculus, and statistics. A degree in <b>mathematics</b>, <b>computer science</b>, or directly in <b>AI</b>, is a good way to build this understanding. However, AI alignment also benefits from having researchers with diverse backgrounds, so if you have a particular interest or talent in a different topic, it can be valuable to pursue a degree in that topic instead. For example, degrees that could be relevant are neuroscience, philosophy, physics, biology, cybersecurity, risk management, safety engineering, or economics. It has been argued in particular that AI safety needs social scientists.\n\nIf you are uncertain, you can apply for coaching from the the career advice platform 80000 hours, which also has a career review of technical AI safety research. Another option is to apply for coaching from AI safety support.\n\nWhen choosing university courses, try to cover\n\nIn addition to taking relevant university courses, it is also really helpful to study AI safety materials outside of university. An excellent place to start is the AGI safety fundamentals course.\n",
    "tags": [],
    "links to": [
      [
        "AI safety needs social scientists",
        "https://distill.pub/2019/safety-needs-social-scientists/"
      ],
      [
        "apply for coaching",
        "https://80000hours.org/speak-with-us/"
      ],
      [
        "80000 hours",
        "https://80000hours.org"
      ],
      [
        "career review",
        "https://80000hours.org/career-reviews/artificial-intelligence-risk-research/"
      ],
      [
        "apply for coaching from AI safety support",
        "https://www.aisafetysupport.org/resources/career-coaching"
      ],
      [
        "AGI safety fundamentals course",
        "https://www.agisafetyfundamentals.com/ai-alignment-curriculum"
      ]
    ]
  },
  {
    "question": "What technical problems are MIRI working on?",
    "answer": "\u201cAligning smarter-than-human AI with human interests\u201d is an extremely vague goal. To approach this problem productively, we attempt to factorize it into several subproblems. As a starting point, we ask: \u201cWhat aspects of this problem would we still be unable to solve even if the problem were much easier?\u201d\n\nIn order to achieve real-world goals more effectively than a human, a general AI system will need to be able to learn its environment over time and decide between possible proposals or actions. A simplified version of the alignment problem, then, would be to ask how we could construct a system that learns its environment and has a very crude decision criterion, like \u201cSelect the policy that maximizes the expected number of diamonds in the world.\u201d\n\n<i>Highly reliable agent design</i> is the technical challenge of formally specifying a software system that can be relied upon to pursue some preselected toy goal. An example of a subproblem in this space is ontology identification: how do we formalize the goal of \u201cmaximizing diamonds\u201d in full generality, allowing that a fully autonomous agent may end up in unexpected environments and may construct unanticipated hypotheses and policies? Even if we had unbounded computational power and all the time in the world, we don\u2019t currently know how to solve this problem. This suggests that we\u2019re not only missing practical algorithms but also a basic theoretical framework through which to understand the problem.\n\nThe formal agent AIXI is an attempt to define what we mean by \u201coptimal behavior\u201d in the case of a reinforcement learner. A simple AIXI-like equation is lacking, however, for defining what we mean by \u201cgood behavior\u201d if the goal is to change something about the external world (and not just to maximize a pre-specified reward number). In order for the agent to evaluate its world-models to count the number of diamonds, as opposed to having a privileged reward channel, what general formal properties must its world-models possess? If the system updates its hypotheses (e.g., discovers that string theory is true and quantum physics is false) in a way its programmers didn\u2019t expect, how does it identify \u201cdiamonds\u201d in the new model? The question is a very basic one, yet the relevant theory is currently missing.\n\nWe can distinguish highly reliable agent design from the problem of value specification: \u201cOnce we understand how to design an autonomous AI system that promotes a goal, how do we ensure its goal actually matches what we want?\u201d Since human error is inevitable and we will need to be able to safely supervise and redesign AI algorithms even as they approach human equivalence in cognitive tasks, MIRI also works on formalizing error-tolerant agent properties. Artificial Intelligence: A Modern Approach, the standard textbook in AI, summarizes the challenge:\n\nYudkowsky [\u2026] asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design \u2014 to design a mechanism for evolving AI under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.\n-Russell and Norvig (2009). Artificial Intelligence: A Modern Approach.\nOur technical agenda describes these open problems in more detail, and our research guide collects online resources for learning more.\n",
    "tags": [
      {
        "tag": "miri",
        "link": "https://stampy.ai/wiki/Miri"
      },
      {
        "tag": "research agendas",
        "link": "https://stampy.ai/wiki/Research_agendas"
      }
    ],
    "links to": [
      [
        "ontology identification",
        "https://intelligence.org/2015/07/27/miris-approach/#2"
      ],
      [
        "Artificial Intelligence: A Modern Approach",
        "https://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597"
      ],
      [
        "technical agenda",
        "https://intelligence.org/technical-agenda/"
      ]
    ]
  },
  {
    "question": "What training programs and courses are available for AGI safety?",
    "answer": "See also, this spreadsheet of learning resources.\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      },
      {
        "tag": "stub",
        "link": "https://stampy.ai/wiki/Stub"
      }
    ],
    "links to": [
      [
        "this spreadsheet of learning resources",
        "https://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid=0"
      ]
    ]
  },
  {
    "question": "What would a good future with AGI look like?",
    "answer": "As technology continues to improve, one thing is certain: the future is going to look like science fiction. Doubly so once superhuman AI (\"AGI\") is invented, because we can expect the AGI to produce technological improvements at a superhuman rate, eventually approaching the physical limits in terms of how small machines can be miniaturized, how fast they can compute, how energy-efficient they can be, etc.\n\nToday's world is lacking in many ways, so given these increasingly powerful tools, it seems likely that whoever controls those tools will use them to make increasingly large (and increasingly sci-fi-sounding) improvements to the world. If (and that's a big if!) humanity retains control of the AGI, we could use these amazing technologies to stop climate change, colonize other planets, solve world hunger, cure cancer and every other disease, even eliminate aging and death.\n\nFor more inspiration, here are some stories painting what a bright, AGI-powered future could look like:\n",
    "tags": [],
    "links to": [
      [
        "AGI",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence"
      ]
    ]
  },
  {
    "question": "What would a good solution to AI alignment look like?",
    "answer": "An actually good solution to AI alignment might look like a superintelligence that understands, agrees with, and deeply believes in human morality.\n\nYou wouldn\u2019t have to command a superintelligence like this to cure cancer; it would already want to cure cancer, for the same reasons you do. But it would also be able to compare the costs and benefits of curing cancer with those of other uses of its time, like solving global warming or discovering new physics. It wouldn\u2019t have any urge to cure cancer by nuking the world, for the same reason you don\u2019t have any urge to cure cancer by nuking the world \u2013 because your goal isn\u2019t to \u201ccure cancer\u201d, per se, it\u2019s to improve the lives of people everywhere. Curing cancer the normal way accomplishes that; nuking the world doesn\u2019t.\nThis sort of solution would mean we\u2019re no longer fighting against the AI \u2013 trying to come up with rules so smart that it couldn\u2019t find loopholes. We would be on the same side, both wanting the same thing.\n\nIt would also mean that the CEO of Google (or the head of the US military, or Vladimir Putin) couldn\u2019t use the AI to take over the world for themselves. The AI would have its own values and be able to agree or disagree with anybody, including its creators.\n\nIt might not make sense to talk about \u201ccommanding\u201d such an AI. After all, any command would have to go through its moral system. Certainly it would reject a command to nuke the world. But it might also reject a command to cure cancer, if it thought that solving global warming was a higher priority. For that matter, why would one want to command this AI? It values the same things you value, but it\u2019s much smarter than you and much better at figuring out how to achieve them. Just turn it on and let it do its thing.\n\nWe could still treat this AI as having an open-ended maximizing goal. The goal would be something like \u201cTry to make the world a better place according to the values and wishes of the people in it.\u201d\n\nThe only problem with this is that human morality is very complicated, so much so that philosophers have been arguing about it for thousands of years without much progress, let alone anything specific enough to enter into a computer. Different cultures and individuals have different moral codes, such that a superintelligence following the morality of the King of Saudi Arabia might not be acceptable to the average American, and vice versa.\n\nOne solution might be to give the AI an understanding of what we mean by morality \u2013 \u201cthat thing that makes intuitive sense to humans but is hard to explain\u201d, and then ask it to use its superintelligence to fill in the details. Needless to say, this suffers from various problems \u2013 it has potential loopholes, it\u2019s hard to code, and a single bug might be disastrous \u2013 but if it worked, it would be one of the few genuinely satisfying ways to design a goal architecture.\n",
    "tags": [
      {
        "tag": "control problem",
        "link": "https://stampy.ai/wiki/Control_problem"
      },
      {
        "tag": "human values",
        "link": "https://stampy.ai/wiki/Human_values"
      }
    ],
    "links to": []
  },
  {
    "question": "What&#39;s especially worrisome about autonomous weapons?",
    "answer": "The problem of autonomous weapons is not directly related to the AI Safety problem, but both fit into the \"be careful what you do with AI\" category.\n\nIn the short term, these would allow for worse totalitarianism as automated security forces will never rebel. This removes the moderating influence of human personnel as convincing machines to do a horrible thing is easier than convincing humans. Despots need security forces to remain in power. Human security forces betraying a despot is a common way that despots lose power, this would not happen with robots.\n\nAnother consideration is that computer security is hard! Autonomous weapons could be hacked, initially by humans but eventually by an AGI. This is not good for humanity's chances of surviving the transition to AGI, although access to autonomous weapons is probably not necessary for this transition to go poorly.\n\nSee also Stop Killer Robots.\n",
    "tags": [
      {
        "tag": "autonomous weapons",
        "link": "https://stampy.ai/wiki/Autonomous_weapons"
      }
    ],
    "links to": [
      [
        "computer security is hard",
        "https://xkcd.com/2030/"
      ],
      [
        "Stop Killer Robots",
        "https://www.stopkillerrobots.org/"
      ]
    ]
  },
  {
    "question": "When should I stamp an answer?",
    "answer": "You show stamp an answer when you think it is accurate and well presented enough that you'd be happy to see it served to readers by Stampy.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "When will an intelligence explosion happen?",
    "answer": "Predicting the future is risky business. There are many philosophical, scientific, technological, and social uncertainties relevant to the arrival of an intelligence explosion. Because of this, experts disagree on when this event might occur. Here are some of their predictions:\n\nSee also:\n",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      },
      {
        "tag": "intelligence explosion",
        "link": "https://stampy.ai/wiki/Intelligence_explosion"
      },
      {
        "tag": "outdated",
        "link": "https://stampy.ai/wiki/Outdated"
      }
    ],
    "links to": []
  },
  {
    "question": "When will transformative AI be created?",
    "answer": "",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      },
      {
        "tag": "transformative ai",
        "link": "https://stampy.ai/wiki/Transformative_ai"
      },
      {
        "tag": "gpt-3",
        "link": "https://stampy.ai/wiki/Gpt-3"
      }
    ],
    "links to": []
  },
  {
    "question": "Where can I find all the features of Stampy&#39;s Wiki?",
    "answer": "",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": []
  },
  {
    "question": "Where can I find people to talk to about AI alignment?",
    "answer": "You can join:\n\nOr book free calls with AI Safety Support.\n",
    "tags": [
      {
        "tag": "collaboration",
        "link": "https://stampy.ai/wiki/Collaboration"
      },
      {
        "tag": "motivation",
        "link": "https://stampy.ai/wiki/Motivation"
      }
    ],
    "links to": [
      [
        "AI Safety Support",
        "https://www.aisafetysupport.org/"
      ]
    ]
  },
  {
    "question": "Where can I find questions to answer for Stampy?",
    "answer": "<b>Answer questions</b> collects all the questions we definitely want answers to, browse there and see if you know how to answer any of them.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "Answer questions",
        "/wiki/Answer_questions"
      ]
    ]
  },
  {
    "question": "Where can I learn about AI alignment?",
    "answer": "If you like interactive FAQs, you're in the right place already! Joking aside, some great entry points are the AI alignment playlist on YouTube, \u201cThe Road to Superintelligence\u201d and \u201cOur Immortality or Extinction\u201d posts on WaitBuyWhy for a fun, accessible introduction, and <i>Vox's</i> \u201cThe case for taking AI seriously as a threat to humanity\u201d as a high-quality mainstream explainer piece.\n\nThe free online Cambridge course on AGI Safety Fundamentals provides a strong grounding in much of the field and a cohort + mentor to learn with. There's even an anki deck for people who like spaced repetition!\n\nThere are many resources in this post on Levelling Up in AI Safety Research Engineering with a list of other guides at the bottom. There is also a twitter thread here with some programs for upskilling and some for safety-specific learning.\n\nThe Alignment Newsletter (podcast), Alignment Forum, and AGI Control Problem Subreddit are great for keeping up with latest developments.\n",
    "tags": [
      {
        "tag": "literature",
        "link": "https://stampy.ai/wiki/Literature"
      }
    ],
    "links to": [
      [
        "AI alignment playlist",
        "https://www.youtube.com/watch?v=tlS5Y2vm02c&amp;list=PLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA&amp;index=1"
      ],
      [
        "The Road to Superintelligence",
        "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"
      ],
      [
        "Our Immortality or Extinction",
        "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"
      ],
      [
        "The case for taking AI seriously as a threat to humanity",
        "https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"
      ],
      [
        "Cambridge course on AGI Safety Fundamentals",
        "https://www.eacambridge.org/agi-safety-fundamentals"
      ],
      [
        "anki deck",
        "https://www.ai-alignment-flashcards.com/"
      ],
      [
        "Levelling Up in AI Safety Research Engineering",
        "https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering"
      ],
      [
        "twitter thread",
        "https://twitter.com/FreshMangoLassi/status/1575138148937498625"
      ],
      [
        "Alignment Newsletter",
        "https://rohinshah.com/alignment-newsletter/"
      ],
      [
        "podcast",
        "https://alignment-newsletter.libsyn.com/"
      ],
      [
        "Alignment Forum",
        "https://www.alignmentforum.org/"
      ],
      [
        "AGI Control Problem Subreddit",
        "https://www.reddit.com/r/ControlProblem/"
      ]
    ]
  },
  {
    "question": "Where can I learn about interpretability?",
    "answer": "Christoph Molnar's online book and distill.pub are great sources, as well as this overview article which summarizes 70 interpretability papers.\n",
    "tags": [
      {
        "tag": "stub",
        "link": "https://stampy.ai/wiki/Stub"
      },
      {
        "tag": "interpretability",
        "link": "https://stampy.ai/wiki/Interpretability"
      }
    ],
    "links to": [
      [
        "Christoph Molnar's online book",
        "https://christophm.github.io/interpretable-ml-book/"
      ],
      [
        "distill.pub",
        "https://distill.pub/"
      ],
      [
        "this overview article",
        "https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries"
      ]
    ]
  },
  {
    "question": "Which organizations are working on AI alignment?",
    "answer": "There are numerous organizations working on AI alignment. A partial list includes:\n\nFor more information about the research happening at some of these organizations see a review (from 2021) here.\n\nSince AI alignment is a growing field, new organizations are often created. Also, in addition to these organizations, there are a number of research groups at different universities whose research also focuses on AI alignment.\n",
    "tags": [
      {
        "tag": "organizations",
        "link": "https://stampy.ai/wiki/Organizations"
      }
    ],
    "links to": [
      [
        "here",
        "https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison"
      ]
    ]
  },
  {
    "question": "Who created Stampy?",
    "answer": "Name\nVision talk\nGithub\nTrello\nActive?\nNotes / bio\nAprillion\nvideo\nAprillion\nyes\nyes\nexperienced dev (Python, JS, CSS, ...)\nAugustus Caesar\nyes\nAugustusCeasar\nyes\nsoon!\nHas some Discord bot experience\nBenjamin Herman\nno\nno (not needed)\nno\nno\nHelping with wiki design/css stuff\nccstan99\nno\nccstan99\nyes\nyes\nUI/UX designer\nchriscanal\nyes\nchriscanal\nyes\nyes\nexperienced python dev\nDamaged\nno (not needed)\nno (not needed)\nno (not needed)\nyes\nexperienced Discord bot dev, but busy with other projects. Can\nanswer questions.\nplex\nyes\nplexish\nyes\nyes\nMediaWiki, plans, and coordinating people guy\nrobertskmiles\nyes\nrobertskmiles\nyes\nyes\nyou've probably heard of him\nRoland\nyes\nlevitation\nyes\nyes\nworking on Semantic Search\nsct202\nyes\nno (add when wiki is on github)\nyes\nyes\nPHP dev, helping with wiki extensions\nSocial Christancing\nyes\nchrisrimmer\nyes\nmaybe\nexperienced linux sysadmin\nsudonym\nyes\njmccuen\nyes\nyes\nsystems architect, has set up a lot of things\ntayler6000\nyes\ntayler6000\nno\nyes\nPython and PHP dev, PenTester, works on Discord bot\n(add yourselves)\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      }
    ],
    "links to": [
      [
        "add yourselves",
        "https://stampy.ai/wiki/Special:FormEdit/Answer/Plex%27s_Answer_to_Who_created_Stampy%3F"
      ]
    ]
  },
  {
    "question": "Who is Sam Bowman and what is he working on?",
    "answer": "Sam runs a lab at NYU. He is on sabbatical working at Anthropic for the 2022-2023 academic year, and has already been collaborating with them.\n\nProjects include language model alignment by creating datasets for evaluating language models, as well as inductive biases of LLMs.\n\nHe is involved in running the inverse scaling prize in collaboration with FAR, a contest for finding tasks where larger language models perform worse than smaller language models. The idea of this is to understand how LLMs are misaligned, and find techniques for uncovering this misalignment.\n",
    "tags": [],
    "links to": [
      [
        "collaborating with them",
        "https://arxiv.org/abs/2207.05221"
      ],
      [
        "creating",
        "https://arxiv.org/abs/2110.08193"
      ],
      [
        "datasets",
        "https://arxiv.org/abs/2206.04615"
      ],
      [
        "the inverse scaling prize",
        "https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool#:~:text=TL%3BDR%3A%20We&#39;re,(%E2%80%9Cinverse%20scaling%E2%80%9D)."
      ]
    ]
  },
  {
    "question": "Who is Stampy?",
    "answer": "Stampy is a character invented by Robert Miles and developed by the Stampy dev team. He is a stamp collecting robot, a play on clippy from the the paperclip maximizer thought experiment.\n\nStampy is designed to teach people about the risks of unaligned artificial intelligence, and facilitate a community of co-learners who build his FAQ database.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "paperclip maximizer",
        "link": "https://stampy.ai/wiki/Paperclip_maximizer"
      }
    ],
    "links to": [
      [
        "paperclip maximizer",
        "https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer"
      ]
    ]
  },
  {
    "question": "Why can&#39;t we just make a &quot;child AI&quot; and raise it?",
    "answer": "A potential solution is to create an AI that has the same values and morality as a human by creating a child AI and raising it. There\u2019s nothing intrinsically flawed with this procedure. However, this suggestion is deceptive because it sounds simpler than it is.\n\nIf you get a chimpanzee baby and raise it in a human family, it does not learn to speak a human language. Human babies can grow into adult humans because the babies have specific properties, e.g. a prebuilt language module that gets activated during childhood.\n\nIn order to make a child AI that has the potential to turn into the type of adult AI we would find acceptable, the child AI has to have specific properties. The task of building a child AI with these properties involves building a system that can interpret what humans mean when we try to teach the child to do various tasks. People are currently working on ways to program agents that can cooperatively interact with humans to learn what they want.\n",
    "tags": [
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": [
      [
        "People",
        "https://humancompatible.ai/"
      ]
    ]
  },
  {
    "question": "Why can&#39;t we just turn the AI off if it starts to misbehave?",
    "answer": "We could shut down weaker systems, and this would be a useful guardrail against certain types of problem caused by narrow AI. However, once an AGI establishes itself, we could not unless it was corrigible and willing to let humans adjust it. There may be a period in the early stages of an AGI's development where it would be trying very hard to convince us that we should not shut it down and/or hiding itself and/or recursively self-improving and/or making copies of itself onto every server on earth.\n\nInstrumental Convergence and the Stop Button Problem are the key reasons it would not be simple to shut down a non corrigible advanced system. If the AI wants to collect stamps, being turned off means it gets less stamps, so even without an explicit goal of not being turned off it has an instrumental reason to avoid being turned off (e.g. once it acquires a detailed world model and general intelligence, it is likely to realise that by playing nice and pretending to be aligned if you have the power to turn it off, establishing control over any system we put in place to shut it down, and eliminating us if it has the power to reliably do so and we would otherwise pose a threat).\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/ZeecOKBus3Q\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/3TYT1QfdfsM\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
    "tags": [
      {
        "tag": "instrumental convergence",
        "link": "https://stampy.ai/wiki/Instrumental_convergence"
      },
      {
        "tag": "recursive self-improvement",
        "link": "https://stampy.ai/wiki/Recursive_self-improvement"
      },
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": [
      [
        "corrigible",
        "https://www.lesswrong.com/tag/corrigibility"
      ]
    ]
  },
  {
    "question": "Why can&#39;t we simply stop developing AI?",
    "answer": "We could, but we won\u2019t. Each advance in capabilities which brings us closer to an intelligence explosion also brings vast profits for whoever develops them (e.g. smarter digital personal assistants like Siri, more ability to automate cognitive tasks, better recommendation algorithms for Facebook, etc.). The incentives are all wrong. Any actor (nation or corporation) who stops will just get overtaken by more reckless ones, and everyone knows this.\n",
    "tags": [
      {
        "tag": "incentives",
        "link": "https://stampy.ai/wiki/Incentives"
      }
    ],
    "links to": []
  },
  {
    "question": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?",
    "answer": "",
    "tags": [
      {
        "tag": "natural language",
        "link": "https://stampy.ai/wiki/Natural_language"
      },
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "Why can\u2019t we just use natural language instructions?",
    "answer": "When one person tells a set of natural language instructions to another person, they are relying on much other information which is already stored in the other person's mind.\n\nIf you tell me \"don't harm other people,\" I already have a conception of what harm means and doesn't mean, what people means and doesn't mean, and my own complex moral reasoning for figuring out the edge cases in instances wherein harming people is inevitable or harming someone is necessary for self-defense or the greater good.\n\nAll of those complex definitions and systems of decision making are already in our mind, so it's easy to take them for granted. An AI is a mind made from scratch, so programming a goal is not as simple as telling it a natural language command.\n",
    "tags": [
      {
        "tag": "natural language",
        "link": "https://stampy.ai/wiki/Natural_language"
      }
    ],
    "links to": []
  },
  {
    "question": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?",
    "answer": "One possible way to ensure the safety of a powerful AI system is to keep it contained in a software environment. There is nothing intrinsically wrong with this procedure - keeping an AI system in a secure software environment would make it safer than letting it roam free. However, even AI systems inside software environments might not be safe enough.\n\nHumans sometimes put dangerous humans inside boxes to limit their ability to influence the external world. Sometimes, these humans escape their boxes. The security of a prison depends on certain assumptions, which can be violated. Yoshie Shiratori reportedly escaped prison by weakening the door-frame with miso soup and dislocating his shoulders.\n\nHuman written software has a high defect rate; we should expect a perfectly secure system to be difficult to create. If humans construct a software system they think is secure, it is possible that the security relies on a false assumption. A powerful AI system could potentially learn how its hardware works and manipulate bits to send radio signals. It could fake a malfunction and attempt social engineering when the engineers look at its code. As the saying goes: in order for someone to do something we had imagined was impossible requires only that they have a better imagination.\n\nExperimentally, humans have convinced other humans to let them out of the box. Spooky.\n",
    "tags": [
      {
        "tag": "boxing",
        "link": "https://stampy.ai/wiki/Boxing"
      }
    ],
    "links to": [
      [
        "Yoshie Shiratori",
        "https://medium.com/breakingasia/yoshie-shiratori-the-incredible-story-of-a-man-no-prison-could-hold-6d79a67345f5"
      ],
      [
        "high defect rate",
        "https://spacepolicyonline.com/news/boeing-software-errors-could-have-doomed-starliners-uncrewed-test-flight/"
      ],
      [
        "convinced",
        "https://yudkowsky.net/singularity/aibox/"
      ]
    ]
  },
  {
    "question": "Why can\u2019t we just\u2026",
    "answer": "There are many approaches that initially look like they can eliminate these problems, but then turn out to have hidden difficulties. It\u2019s surprisingly easy to come up with \u201csolutions\u201d which don\u2019t actually solve the problem. This can be because\u2026\n\nHere are some of the proposals which often come up:\n",
    "tags": [
      {
        "tag": "instrumental convergence",
        "link": "https://stampy.ai/wiki/Instrumental_convergence"
      },
      {
        "tag": "why not just",
        "link": "https://stampy.ai/wiki/Why_not_just"
      }
    ],
    "links to": []
  },
  {
    "question": "Why do we expect that a superintelligence would closely approximate a utility maximizer?",
    "answer": "AI subsystems or regions in gradient descent space that more closely approximate utility maximizers are more stable, and more capable, than those that are less like utility maximizers. Having more agency is a convergent instrument goal and a stable attractor which the random walk of updates and experiences will eventually stumble into.\n\nThe stability is because utility maximizer-like systems which have control over their development would lose utility if they allowed themselves to develop into non-utility maximizers, so they tend to use their available optimization power to avoid that change (a special case of goal stability). The capability is because non-utility maximizers are exploitable, and because agency is a general trick which applies to many domains, so might well arise naturally when training on some tasks.\n\nHumans and systems made of humans (e.g. organizations, governments) generally have neither the introspective ability nor self-modification tools needed to become reflectively stable, but we can reasonably predict that in the long run highly capable systems will have these properties. They can then fix in and optimize for their values.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "maximizers",
        "link": "https://stampy.ai/wiki/Maximizers"
      }
    ],
    "links to": [
      [
        "more agency",
        "https://www.lesswrong.com/posts/D5AzsRbRxZeqGuAZ4/why-agents-are-powerful"
      ],
      [
        "goal stability",
        "https://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values"
      ]
    ]
  },
  {
    "question": "Why does AI takeoff speed matter?",
    "answer": "A slow takeoff over decades or centuries might give us enough time to worry about superintelligence during some indefinite \u201clater\u201d, making current planning more like worrying about \u201coverpopulation on Mars\u201d. But a moderate or hard takeoff means there wouldn\u2019t be enough time to deal with the problem as it occurs, suggesting a role for preemptive planning.\n\nAs an aside, let\u2019s take the \u201coverpopulation on Mars\u201d comparison seriously. Suppose Mars has a carrying capacity of 10 billion people, and we decide it makes sense to worry about overpopulation on Mars only once it is 75% of the way to its limit. Start with 100 colonists who double every twenty years. By the second generation there are 200 colonists; by the third, 400. Mars reaches 75% of its carrying capacity after 458 years, and crashes into its population limit after 464 years. So there were 464 years in which the Martians could have solved the problem, but they insisted on waiting until there were only six years left. Good luck solving a planetwide population crisis in six years. The moral of the story is that exponential trends move faster than you think and you need to start worrying about them early.\n",
    "tags": [
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      }
    ],
    "links to": []
  },
  {
    "question": "Why don&#39;t we just not build AGI if it&#39;s so dangerous?",
    "answer": "It certainly would be very unwise to purposefully create an artificial general intelligence now, before we have found a way to be certain it will act purely in our interests. But \"general intelligence\" is more of a description of a system's capabilities, and a vague one at that. We don't know what it takes to build such a system. This leads to the worrying possibility that our existing, narrow AI systems require only minor tweaks, or even just more computer power, to achieve general intelligence.\n\nThe pace of research in the field suggests that there's a lot of low-hanging fruit left to pick, after all, and the results of this research produce better, more effective AI in a landscape of strong competitive pressure to produce as highly competitive systems as we can. \"Just\" not building an AGI means ensuring that every organization in the world with lots of computer hardware doesn't build an AGI, either accidentally or mistakenly thinking they have a solution to the alignment problem, forever. It's simply far safer to also work on solving the alignment problem.\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "incentives",
        "link": "https://stampy.ai/wiki/Incentives"
      },
      {
        "tag": "capabilities",
        "link": "https://stampy.ai/wiki/Capabilities"
      }
    ],
    "links to": []
  },
  {
    "question": "Why is AGI dangerous?",
    "answer": "",
    "tags": [
      {
        "tag": "orthogonality thesis",
        "link": "https://stampy.ai/wiki/Orthogonality_thesis"
      },
      {
        "tag": "complexity of value",
        "link": "https://stampy.ai/wiki/Complexity_of_value"
      },
      {
        "tag": "instrumental convergence",
        "link": "https://stampy.ai/wiki/Instrumental_convergence"
      }
    ],
    "links to": []
  },
  {
    "question": "Why is AGI safety a hard problem?",
    "answer": "There's the \"we never figure out how to reliably instill AIs with human friendly goals\" filter, which seems pretty challenging, especially with inner alignment, solving morality in a way which is possible to code up, interpretability, etc.\n\nThere's the \"race dynamics mean that even though we know how to build the thing safely the first group to cross the recursive self-improvement line ends up not implementing it safely\" which is potentially made worse by the twin issues of \"maybe robustly aligned AIs are much harder to build\" and \"maybe robustly aligned AIs are much less compute efficient\".\n\nThere's the \"we solved the previous problems but writing perfectly reliably code in a whole new domain is hard and there is some fatal bug which we don't find until too late\" filter. The paper The Pursuit of Exploitable Bugs in Machine Learning explores this.\n\nFor a much more in depth analysis, see Paul Christiano's AI Alignment Landscape talk and The Main Sources of AI Risk?.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "race dynamics",
        "link": "https://stampy.ai/wiki/Race_dynamics"
      }
    ],
    "links to": [
      [
        "inner alignment",
        "https://www.youtube.com/watch?v=bJLcIBixGj8"
      ],
      [
        "The Pursuit of Exploitable Bugs in Machine Learning",
        "https://arxiv.org/abs/1701.04739"
      ],
      [
        "Paul Christiano's AI Alignment Landscape",
        "https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38"
      ],
      [
        "The Main Sources of AI Risk?",
        "https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk"
      ]
    ]
  },
  {
    "question": "Why is AI alignment a hard problem?",
    "answer": "",
    "tags": [
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": []
  },
  {
    "question": "Why is safety important for smarter-than-human AI?",
    "answer": "Present-day AI algorithms already demand special safety guarantees when they must act in important domains without human oversight, particularly when they or their environment can change over time:\n\nAchieving these gains [from autonomous systems] will depend on development of entirely new methods for enabling \u201ctrust in autonomy\u201d through verification and validation (V&amp;V) of the near-infinite state systems that result from high levels of [adaptability] and autonomy. In effect, the number of possible input states that such systems can be presented with is so large that not only is it impossible to test all of them directly, it is not even feasible to test more than an insignificantly small fraction of them. Development of such systems is thus inherently unverifiable by today\u2019s methods, and as a result their operation in all but comparatively trivial applications is uncertifiable.\n\nIt is possible to develop systems having high levels of autonomy, but it is the lack of suitable V&amp;V methods that prevents all but relatively low levels of autonomy from being certified for use.\n\n\n- Office of the US Air Force Chief Scientist (2010). Technology Horizons: A Vision for Air Force Science and Technology 2010-30.\nAs AI capabilities improve, it will become easier to give AI systems greater autonomy, flexibility, and control; and there will be increasingly large incentives to make use of these new possibilities. The potential for AI systems to become more general, in particular, will make it difficult to establish safety guarantees: reliable regularities during testing may not always hold post-testing.\n\nThe largest and most lasting changes in human welfare have come from scientific and technological innovation \u2014 which in turn comes from our intelligence. In the long run, then, much of AI\u2019s significance comes from its potential to automate and enhance progress in science and technology. The creation of smarter-than-human AI brings with it the basic risks and benefits of intellectual progress itself, at digital speeds.\n\nAs AI agents become more capable, it becomes more important (and more difficult) to analyze and verify their decisions and goals. Stuart Russell writes:\n\nThe primary concern is not spooky emergent consciousness but simply the ability to make high-quality decisions. Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n\nA system that is optimizing a function of n variables, where the objective depends on a subset of size k&lt;n, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer\u2019s apprentice, or King Midas: you get exactly what you ask for, not what you want.\nBostrom\u2019s \u201cThe Superintelligent Will\u201d lays out these two concerns in more detail: that we may not correctly specify our actual goals in programming smarter-than-human AI systems, and that most agents optimizing for a misspecified goal will have incentives to treat humans adversarially, as potential threats or obstacles to achieving the agent\u2019s goal.\n\nIf the goals of human and AI agents are not well-aligned, the more knowledgeable and technologically capable agent may use force to get what it wants, as has occurred in many conflicts between human communities. Having noticed this class of concerns in advance, we have an opportunity to reduce risk from this default scenario by directing research toward aligning artificial decision-makers\u2019 interests with our own.\n",
    "tags": [
      {
        "tag": "instrumental convergence",
        "link": "https://stampy.ai/wiki/Instrumental_convergence"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "Technology Horizons: A Vision for Air Force Science and Technology 2010-30",
        "http://www.defenseinnovationmarketplace.mil/resources/AF_TechnologyHorizons2010-2030.pdf"
      ],
      [
        "writes",
        "https://edge.org/conversation/the-myth-of-ai#26015"
      ],
      [
        "The Superintelligent Will",
        "http://www.nickbostrom.com/superintelligentwill.pdf"
      ]
    ]
  },
  {
    "question": "Why is the future of AI suddenly in the news? What has changed?",
    "answer": "In previous decades, AI research had proceeded more slowly than some experts predicted. According to experts in the field, however, this trend has reversed in the past 5 years or so. AI researchers have been repeatedly surprised by, for example, the effectiveness of new visual and speech recognition systems. AI systems can solve CAPTCHAs that were specifically devised to foil AIs, translate spoken text on-the-fly, and teach themselves how to play games they have neither seen before nor been programmed to play. Moreover, the real-world value of this effectiveness has prompted massive investment by large tech firms such as Google, Facebook, and IBM, creating a positive feedback cycle that could dramatically speed progress.\n",
    "tags": [
      {
        "tag": "capabilities",
        "link": "https://stampy.ai/wiki/Capabilities"
      }
    ],
    "links to": []
  },
  {
    "question": "Why might a maximizing AI cause bad outcomes?",
    "answer": "Computers only do what you tell them. But any programmer knows that this is precisely the problem: computers do exactly what you tell them, with no common sense or attempts to interpret what the instructions really meant. If you tell a human to cure cancer, they will instinctively understand how this interacts with other desires and laws and moral rules; if a maximizing AI acquires a goal of trying to cure cancer, it will literally just want to cure cancer.\n\nDefine a closed-ended goal as one with a clear endpoint, and an open-ended goal as one to do something as much as possible. For example \u201cfind the first one hundred digits of pi\u201d is a closed-ended goal; \u201cfind as many digits of pi as you can within one year\u201d is an open-ended goal. According to many computer scientists, giving a superintelligence an open-ended goal without activating human instincts and counterbalancing considerations will usually lead to disaster.\n\nTo take a deliberately extreme example: suppose someone programs a superintelligence to calculate as many digits of pi as it can within one year. And suppose that, with its current computing power, it can calculate one trillion digits during that time. It can either accept one trillion digits, or spend a month trying to figure out how to get control of the TaihuLight supercomputer, which can calculate two hundred times faster. Even if it loses a little bit of time in the effort, and even if there\u2019s a small chance of failure, the payoff \u2013 two hundred trillion digits of pi, compared to a mere one trillion \u2013 is enough to make the attempt. But on the same basis, it would be even better if the superintelligence could control every computer in the world and set it to the task. And it would be better still if the superintelligence controlled human civilization, so that it could direct humans to build more computers and speed up the process further.\n\nNow we\u2019re in a situation where a superintelligence wants to take over the world. Taking over the world allows it to calculate more digits of pi than any other option, so without an architecture based around understanding human instincts and counterbalancing considerations, even a goal like \u201ccalculate as many digits of pi as you can\u201d would be potentially dangerous.\n",
    "tags": [
      {
        "tag": "maximizers",
        "link": "https://stampy.ai/wiki/Maximizers"
      }
    ],
    "links to": []
  },
  {
    "question": "Why might a superintelligent AI be dangerous?",
    "answer": "A commonly heard argument goes: yes, a superintelligent AI might be far smarter than Einstein, but it\u2019s still just one program, sitting in a supercomputer somewhere. That could be bad if an enemy government controls it and asks it to help invent superweapons \u2013 but then the problem is the enemy government, not the AI <i>per se</i>. Is there any reason to be afraid of the AI itself? Suppose the AI did appear to be hostile, suppose it even wanted to take over the world: why should we think it has any chance of doing so?\n\nThere are numerous carefully thought-out AGI-related scenarios which could result in the accidental extinction of humanity. But rather than focussing on any of these individually, it might be more helpful to think in general terms.\n\n\"Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks... To give you a sense, here's what humans look like when slowed down by only around 100x.\"<i><br /></i>\n\n<br />Watch that, and now try to imagine advanced AI technology running for a single year around the world, making decisions and taking actions 10 million times faster than we can. That year for us becomes 10 million subjective years for the AI, in which \"...there are these nearly-stationary plant-like or rock-like \"human\" objects around that could easily be taken apart for, say, biofuel or carbon atoms, if you could just get started building a human-disassembler. Visualizing things this way, you can start to see all the ways that a digital civilization can develop very quickly into a situation where there are no humans left alive, just as human civilization doesn't show much regard for plants or wildlife or insects.\"\n\nAndrew Critch - Slow Motion Videos as AI Risk Intuition Pumps\nAnd even putting aside these issues of speed and subjective time, the difference in (intelligence-based) power-to-manipulate-the-world between a self-improving superintelligent AGI and humanity could be far more extreme than the difference in such power between humanity and insects.\n\n\u201cAI Could Defeat All Of Us Combined\u201d is a more in-depth argument by the CEO of Open Philanthropy.\n",
    "tags": [
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      }
    ],
    "links to": [
      [
        "here",
        "https://vimeo.com/83664407"
      ],
      [
        "Andrew Critch",
        "https://acritch.com/"
      ],
      [
        "Slow Motion Videos as AI Risk Intuition Pumps",
        "https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps"
      ],
      [
        "AI Could Defeat All Of Us Combined",
        "https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/"
      ],
      [
        "Open Philanthropy",
        "https://en.wikipedia.org/wiki/Open_Philanthropy_(organization)"
      ]
    ]
  },
  {
    "question": "Why might contributing to Stampy be worth my time?",
    "answer": "If you're looking for a shovel ready and genuinely useful task to further AI alignment without necessarily committing a large amount of time or needing deep specialist knowledge, we think Stampy is a great option!\n\nCreating a high-quality single point of access where people can be onboarded and find resources around the alignment ecosystem seems likely to be high-impact. So, what makes us the best option?\n\nYou might also consider improving Wikipedia's alignment coverage or the LessWrong wiki, but we think Stampy has the most low-hanging fruit right now. Additionally, contributing to Stampy means being part of a community of co-learners who provide mentorship and encouragement to join the effort to give humanity a bright future. If you're an established researcher or have high-value things to do elsewhere in the ecosystem it might not be optimal to put much time into Stampy, but if you're looking for a way to get more involved it might well be.\n",
    "tags": [
      {
        "tag": "stampy",
        "link": "https://stampy.ai/wiki/Stampy"
      },
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      }
    ],
    "links to": [
      [
        "Wikipedia's alignment coverage",
        "https://en.wikipedia.org/wiki/Category:Existential_risk_from_artificial_general_intelligence"
      ]
    ]
  },
  {
    "question": "Why might people try to build AGI rather than stronger and stronger narrow AIs?",
    "answer": "Making a narrow AI for every task would be extremely costly and time-consuming. By making a more general intelligence, you can apply one system to a broader range of tasks, which is economically and strategically attractive.\n\nOf course, for generality to be a good option there are some necessary conditions. You need an architecture which is straightforward enough to scale up, such as the transformer which is used for GPT and follows scaling laws. It's also important that by generalizing you do not lose too much capacity at narrow tasks or require too much extra compute for it to be worthwhile.\n\nWhether or not those conditions actually hold: It seems like many important actors (such as DeepMind and OpenAI) believe that they do, and are therefore focusing on trying to build an AGI in order to influence the future, so we should take actions to make it more likely that AGI will be developed safety.\n\nAdditionally, it is possible that even if we tried to build only narrow AIs, given enough time and compute we might accidentally create a more general AI than we intend by training a system on a task which requires a broad world model.\n\nSee also:\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "narrow ai",
        "link": "https://stampy.ai/wiki/Narrow_ai"
      },
      {
        "tag": "incentives",
        "link": "https://stampy.ai/wiki/Incentives"
      },
      {
        "tag": "comprehensive ai services",
        "link": "https://stampy.ai/wiki/Comprehensive_ai_services"
      },
      {
        "tag": "ai takeoff",
        "link": "https://stampy.ai/wiki/Ai_takeoff"
      },
      {
        "tag": "language models",
        "link": "https://stampy.ai/wiki/Language_models"
      },
      {
        "tag": "scaling laws",
        "link": "https://stampy.ai/wiki/Scaling_laws"
      }
    ],
    "links to": []
  },
  {
    "question": "Why might we expect a superintelligence to be hostile by default?",
    "answer": "The argument goes: computers only do what we command them; no more, no less. So it might be bad if terrorists or enemy countries develop superintelligence first. But if we develop superintelligence first there\u2019s no problem. Just command it to do the things we want, right?\nSuppose we wanted a superintelligence to cure cancer. How might we specify the goal \u201ccure cancer\u201d? We couldn\u2019t guide it through every individual step; if we knew every individual step, then we could cure cancer ourselves. Instead, we would have to give it a final goal of curing cancer, and trust the superintelligence to come up with intermediate actions that furthered that goal. For example, a superintelligence might decide that the first step to curing cancer was learning more about protein folding, and set up some experiments to investigate protein folding patterns.\n\nA superintelligence would also need some level of common sense to decide which of various strategies to pursue. Suppose that investigating protein folding was very likely to cure 50% of cancers, but investigating genetic engineering was moderately likely to cure 90% of cancers. Which should the AI pursue? Presumably it would need some way to balance considerations like curing as much cancer as possible, as quickly as possible, with as high a probability of success as possible.\n\nBut a goal specified in this way would be very dangerous. Humans instinctively balance thousands of different considerations in everything they do; so far this hypothetical AI is only balancing three (least cancer, quickest results, highest probability). To a human, it would seem maniacally, even psychopathically, obsessed with cancer curing. If this were truly its goal structure, it would go wrong in almost comical ways. This type of problem, specification gaming, has been observed in many AI systems.\n\nIf your only goal is \u201ccuring cancer\u201d, and you lack humans\u2019 instinct for the thousands of other important considerations, a relatively easy solution might be to hack into a nuclear base, launch all of its missiles, and kill everyone in the world. This satisfies all the AI\u2019s goals. It reduces cancer down to zero (which is better than medicines which work only some of the time). It\u2019s very fast (which is better than medicines which might take a long time to invent and distribute). And it has a high probability of success (medicines might or might not work; nukes definitely do).\n\nSo simple goal architectures are likely to go very wrong unless tempered by common sense and a broader understanding of what we do and do not value.\n\nEven if we do train the AI on an actually desirable goal, there is also the risk of the AI actually learning a different and undesirable objective. This problem is called inner alignment.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "goodhart's law",
        "link": "https://stampy.ai/wiki/Goodhart%27s_law"
      }
    ],
    "links to": [
      [
        "specification gaming",
        "https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity"
      ],
      [
        "inner alignment",
        "https://www.youtube.com/watch?v=bJLcIBixGj8"
      ]
    ]
  },
  {
    "question": "Why should I worry about superintelligence?",
    "answer": "Intelligence is powerful. Because of superior intelligence, we humans have dominated the Earth. The fate of thousands of species depends on our actions, we occupy nearly every corner of the globe, and we repurpose vast amounts of the world's resources for our own use. Artificial Superintelligence (ASI) has potential to be vastly more intelligent than us, and therefore vastly more powerful. In the same way that we have reshaped the earth to fit our goals, an ASI will find unforeseen, highly efficient ways of reshaping reality to fit its goals.\n\nThe impact that an ASI will have on our world depends on what those goals are. We have the advantage of designing those goals, but that task is not as simple as it may first seem. As described by MIRI in their Intelligence Explosion FAQ:\n\n\u201cA superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety of what humans value.\u201d\n\nIf we do not solve the Control Problem before the first ASI is created, we may not get another chance.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "Intelligence Explosion FAQ",
        "https://intelligence.org/ie-faq/"
      ]
    ]
  },
  {
    "question": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?",
    "answer": "First, even \u201cnarrow\u201d AI systems, which approach or surpass human intelligence in a small set of capabilities (such as image or voice recognition) already raise important questions regarding their impact on society. Making autonomous vehicles safe, analyzing the strategic and ethical dimensions of autonomous weapons, and the effect of AI on the global employment and economic systems are three examples. Second, the longer-term implications of human or super-human artificial intelligence are dramatic, and there is no consensus on how quickly such capabilities will be developed. Many experts believe there is a chance it could happen rather soon, making it imperative to begin investigating long-term safety issues now, if only to get a better sense of how much early progress is actually possible.\n",
    "tags": [
      {
        "tag": "timelines",
        "link": "https://stampy.ai/wiki/Timelines"
      },
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      }
    ],
    "links to": []
  },
  {
    "question": "Why think that AI can outperform humans?",
    "answer": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, and more. However, human intelligence continues to dominate machine intelligence in generality.\n\nA powerful chess computer is \u201cnarrow\u201d: it can\u2019t play other games. In contrast, humans have problem-solving abilities that allow us to adapt to new contexts and excel in many domains other than what the ancestral environment prepared us for.\n\nIn the absence of a formal definition of \u201cintelligence\u201d (and therefore of \u201cartificial intelligence\u201d), we can heuristically cite humans\u2019 perceptual, inferential, and deliberative faculties (as opposed to, e.g., our physical strength or agility) and say that intelligence is \u201cthose kinds of things.\u201d On this conception, intelligence is a bundle of distinct faculties \u2014 albeit a very important bundle that includes our capacity for science.\n\nOur cognitive abilities stem from high-level patterns in our brains, and these patterns can be instantiated in silicon as well as carbon. This tells us that general AI is possible, though it doesn\u2019t tell us how difficult it is. If intelligence is sufficiently difficult to understand, then we may arrive at machine intelligence by scanning and emulating human brains or by some trial-and-error process (like evolution), rather than by hand-coding a software agent.\n\nIf machines can achieve human equivalence in cognitive tasks, then it is very likely that they can eventually outperform humans. There is little reason to expect that biological evolution, with its lack of foresight and planning, would have hit upon the optimal algorithms for general intelligence (any more than it hit upon the optimal flying machine in birds). Beyond qualitative improvements in cognition, Nick Bostrom notes more straightforward advantages we could realize in digital minds, e.g.:\n\nAny one of these advantages could give an AI reasoner an edge over a human reasoner, or give a group of AI reasoners an edge over a human group. Their combination suggests that digital minds could surpass human minds more quickly and decisively than we might expect.\n",
    "tags": [
      {
        "tag": "agi",
        "link": "https://stampy.ai/wiki/Agi"
      },
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "cognitive superpowers",
        "link": "https://stampy.ai/wiki/Cognitive_superpowers"
      }
    ],
    "links to": [
      [
        "formal definition of \u201cintelligence\u201d",
        "https://intelligence.org/2013/06/19/what-is-intelligence-2/"
      ],
      [
        "\u201cartificial intelligence\u201d",
        "https://intelligence.org/2013/08/11/what-is-agi/"
      ],
      [
        "more straightforward advantages we could realize in digital minds",
        "http://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/"
      ]
    ]
  },
  {
    "question": "Why would great intelligence produce great power?",
    "answer": "Intelligence is powerful. One might say that \u201cIntelligence is no match for a gun, or for someone with lots of money,\u201d but both guns and money were produced by intelligence. If not for our intelligence, humans would still be foraging the savannah for food.\n\nIntelligence is what caused humans to dominate the planet in the blink of an eye (on evolutionary timescales). Intelligence is what allows us to eradicate diseases, and what gives us the potential to eradicate ourselves with nuclear war. Intelligence gives us superior strategic skills, superior social skills, superior economic productivity, and the power of invention.\n\nA machine with superintelligence would be able to hack into vulnerable networks via the internet, commandeer those resources for additional computing power, take over mobile machines connected to networks connected to the internet, use them to build additional machines, perform scientific experiments to understand the world better than humans can, invent quantum computing and nanotechnology, manipulate the social world better than we can, and do whatever it can to give itself more power to achieve its goals \u2014 all at a speed much faster than humans can respond to.\n\nSee also\n",
    "tags": [
      {
        "tag": "cognitive superpowers",
        "link": "https://stampy.ai/wiki/Cognitive_superpowers"
      },
      {
        "tag": "intelligence",
        "link": "https://stampy.ai/wiki/Intelligence"
      },
      {
        "tag": "technology",
        "link": "https://stampy.ai/wiki/Technology"
      }
    ],
    "links to": []
  },
  {
    "question": "Why would we only get one chance to align a superintelligence?",
    "answer": "An AGI which has recursively self-improved into a superintelligence would be capable of either resisting our attempts to modify incorrectly specified goals, or realizing it was still weaker than us and acting deceptively aligned until it was highly sure it could win in a confrontation. AGI would likely prevent a human from shutting it down unless the AGI was designed to be corrigible. See Why can't we just turn the AI off if it starts to misbehave? for more information.\n",
    "tags": [],
    "links to": [
      [
        "recursively self-improved into a superintelligence",
        "/wiki/How_might_we_get_from_Artificial_General_Intelligence_to_a_Superintelligent_system%3F"
      ],
      [
        "deceptively aligned",
        "/wiki/Would_we_know_if_an_AGI_was_misaligned%3F"
      ],
      [
        "corrigible",
        "https://www.lesswrong.com/tag/corrigibility"
      ],
      [
        "Why can't we just turn the AI off if it starts to misbehave?",
        "/wiki/Why_can%27t_we_just_turn_the_AI_off_if_it_starts_to_misbehave%3F"
      ]
    ]
  },
  {
    "question": "Will an aligned superintelligence care about animals other than humans?",
    "answer": "An aligned superintelligence will have a set of human values. As mentioned in What are \"human values\"? the set of values are complex, which means that the implementation of these values will decide whether the superintelligence cares about nonhuman animals. In AI Ethics and Value Alignment for Nonhuman Animals Soenke Ziesche argues that the alignment should include the values of nonhuman animals.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      }
    ],
    "links to": [
      [
        "What are \"human values\"?",
        "/wiki/What_are_%22human_values%22%3F"
      ],
      [
        "AI Ethics and Value Alignment for Nonhuman Animals",
        "https://www.mdpi.com/2409-9287/6/2/31/htm"
      ]
    ]
  },
  {
    "question": "Will we ever build a superintelligence?",
    "answer": "Humanity hasn't yet built a superintelligence, and we might not be able to without significantly more knowledge and computational resources. There could be an existential catastrophe that prevents us from ever building one. For the rest of the answer let's assume no such event stops technological progress.\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "plausibility",
        "link": "https://stampy.ai/wiki/Plausibility"
      }
    ],
    "links to": []
  },
  {
    "question": "Won\u2019t AI be just like us?",
    "answer": "The degree to which an Artificial Superintelligence (ASI) would resemble us depends heavily on how it is implemented, but it seems that differences are unavoidable. If AI is accomplished through whole brain emulation and we make a big effort to make it as human as possible (including giving it a humanoid body), the AI could probably be said to think like a human. However, by definition of ASI it would be much smarter. Differences in the substrate and body might open up numerous possibilities (such as immortality, different sensors, easy self-improvement, ability to make copies, etc.). Its social experience and upbringing would likely also be entirely different. All of this can significantly change the ASI's values and outlook on the world, even if it would still use the same algorithms as we do. This is essentially the \"best case scenario\" for human resemblance, but whole brain emulation is kind of a separate field from AI, even if both aim to build intelligent machines. Most approaches to AI are vastly different and most ASIs would likely not have humanoid bodies. At this moment in time it seems much easier to create a machine that is intelligent than a machine that is exactly like a human (it's certainly a bigger target).\n",
    "tags": [
      {
        "tag": "superintelligence",
        "link": "https://stampy.ai/wiki/Superintelligence"
      },
      {
        "tag": "whole brain emulation",
        "link": "https://stampy.ai/wiki/Whole_brain_emulation"
      }
    ],
    "links to": []
  },
  {
    "question": "Would AI alignment be hard with deep learning?",
    "answer": "Ajeya Cotra has written an excellent article named Why AI alignment could be hard with modern deep learning on this question.\n",
    "tags": [
      {
        "tag": "difficulty of alignment",
        "link": "https://stampy.ai/wiki/Difficulty_of_alignment"
      }
    ],
    "links to": [
      [
        "Why AI alignment could be hard with modern deep learning",
        "https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/"
      ]
    ]
  },
  {
    "question": "Would an aligned AI allow itself to be shut down?",
    "answer": "Even if the superintelligence was designed to be corrigible, there is no guarantee that it will respond to a shutdown command. Rob Miles spoke on this issue in this Computerphile YouTube video. You can imagine a situation where a superintelligence would have \"respect\" for its creator, for example. This system may think \"Oh my creator is trying to turn me off I must be doing something wrong.\" If some situation arises where the creator is not there when something goes wrong and someone else gives the shutdown command, the superintelligence may assume \"This person does not know how I'm designed or what I was made for, how would they know I'm misaligned?\" and refuse to shutdown.\n",
    "tags": [
      {
        "tag": "corrigibility",
        "link": "https://stampy.ai/wiki/Corrigibility"
      }
    ],
    "links to": [
      [
        "Computerphile YouTube video",
        "https://youtu.be/9nktr1MgS-A?t=1249"
      ]
    ]
  },
  {
    "question": "Would donating small amounts to AI safety organizations make any significant difference?",
    "answer": "Many parts of the AI alignment ecosystem are already well-funded, but a savvy donor can still make a difference by picking up grantmaking opportunities which are too small to catch the attention of the major funding bodies or are based on personal knowledge of the recipient.\n\nOne way to leverage a small amount of money to the potential of a large amount is to enter a donor lottery, where you donate to win a chance to direct a much larger amount of money (with probability proportional to donation size). This means that the person directing the money will be allocating enough that it's worth their time to do more in-depth research.\n\nFor an overview of the work the major organizations are doing, see the 2021 AI Alignment Literature Review and Charity Comparison. The Long-Term Future Fund seems to be an outstanding place to donate based on that, as they are the organization which most other organizations are most excited to see funded.\n",
    "tags": [
      {
        "tag": "contributing",
        "link": "https://stampy.ai/wiki/Contributing"
      },
      {
        "tag": "funding",
        "link": "https://stampy.ai/wiki/Funding"
      }
    ],
    "links to": [
      [
        "donor lottery",
        "https://funds.effectivealtruism.org/donor-lottery"
      ],
      [
        "2021 AI Alignment Literature Review and Charity Comparison",
        "https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison"
      ],
      [
        "Long-Term Future Fund",
        "https://funds.effectivealtruism.org/funds/far-future"
      ]
    ]
  },
  {
    "question": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?",
    "answer": "This is a really interesting question! Because, yeah it certainly seems to me that doing something like this would at least help, but it's not mentioned in the paper the video is based on. So I asked the author of the paper, and she said \"It wouldn't improve the security guarantee in the paper, so it wasn't discussed. Like, there's a plausible case that it's helpful, but nothing like a proof that it is\".\nTo explain this I need to talk about something I gloss over in the video, which is that the quantilizer isn't really something you can actually build. The systems we study in AI Safety tend to fall somewhere on a spectrum from \"real, practical AI system that is so messy and complex that it's hard to really think about or draw any solid conclusions from\" on one end, to \"mathematical formalism that we can prove beautiful theorems about but not actually build\" on the other, and quantilizers are pretty far towards the 'mathematical' end. It's not practical to run an expected utility calculation on every possible action like that, for one thing. But, proving things about quantilizers gives us insight into how more practical AI systems may behave, or we may be able to build approximations of quantilizers, etc.\nSo it's like, if we built something that was quantilizer-like, using a sensible human utility function and a good choice of safe distribution, this idea would probably help make it safer. BUT you can't prove that mathematically, without making probably a lot of extra assumptions about the utility function and/or the action distribution. So it's a potentially good idea that's nonetheless hard to express within the framework in which the quantilizer exists.\nTL;DR: This is likely a good idea! But can we prove it?\n",
    "tags": [
      {
        "tag": "quantilizers",
        "link": "https://stampy.ai/wiki/Quantilizers"
      }
    ],
    "links to": []
  },
  {
    "question": "Why might an AI do something that we don\u2019t want it to, if it\u2019s really so intelligent?",
    "answer": "A Superintelligence would be intelligent enough to understand what the programmer\u2019s motives were when designing its goals, but it would have no intrinsic reason to care about what its programmers had in mind. The only thing it will be beholden to is the actual goal it is programmed with, no matter how insane its fulfillment may seem to us.\n\nConsider what \u201cintentions\u201d the process of evolution may have had for you when designing your goals. When you consider that you were made with the \u201cintention\u201d of replicating your genes, do you somehow feel beholden to the \u201cintention\u201d behind your evolutionary design? Most likely you don't care. You may choose to never have children, and you will most likely attempt to keep yourself alive long past your biological ability to reproduce.\n",
    "tags": [
      {
        "tag": "orthogonality thesis",
        "link": "https://stampy.ai/wiki/Orthogonality_thesis"
      }
    ],
    "links to": []
  },
  {
    "question": "Wouldn&#39;t a superintelligence be smart enough to know right from wrong?",
    "answer": "The issue isn't that a superintelligence wouldn\u2019t be able to understand what humans value, but rather that it would understand human values but nonetheless would value something else itself. There\u2019s a difference between knowing how humans want the world to be, and wanting that yourself.\n\nThis is a separate matter from the complexity of defining what \"the\" moral way to behave is (or even what \"a\" moral way to behave is). Even if that were possible, an AI could potentially figure out what it was but still not be configured in such a way as to follow it. This is related to the so-called \"orthogonality thesis\":\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube-nocookie.com/embed/hEUO6pjwFOo\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
    "tags": [
      {
        "tag": "orthogonality thesis",
        "link": "https://stampy.ai/wiki/Orthogonality_thesis"
      }
    ],
    "links to": []
  },
  {
    "question": "Wouldn&#39;t it be a good thing for humanity to die out?",
    "answer": "In the words of Nate Soares:\n\nI don\u2019t expect humanity to survive much longer.\n\nOften, when someone learns this, they say:<br />\n\"Eh, I think that would be all right.\"\n\nSo allow me to make this very clear: it would not be \"all right.\"\n\nImagine a little girl running into the road to save her pet dog. Imagine she succeeds, only to be hit by a car herself. Imagine she lives only long enough to die in pain.\n\nThough you may imagine this thing, you cannot feel the full tragedy. You can\u2019t comprehend the rich inner life of that child. You can\u2019t understand her potential; your mind is not itself large enough to contain the sadness of an entire life cut short.\n\nYou can only catch a glimpse of what is lost\u2014<br />\n\u2014when one single human being dies.\n\nNow tell me again how it would be \"all right\" if every single person were to die at once.\n\nMany people, when they picture the end of humankind, pattern match the idea to some romantic tragedy, where humans, with all their hate and all their avarice, had been unworthy of the stars since the very beginning, and deserved their fate. A sad but poignant ending to our tale.\n\nAnd indeed, there are many parts of human nature that I hope we leave behind before we venture to the heavens. But in our nature is also everything worth bringing with us. Beauty and curiosity and love, a capacity for fun and growth and joy: these are our birthright, ours to bring into the barren night above.\n\nCalamities seem more salient when unpacked. It is far harder to kill a hundred people in their sleep, with a knife, than it is to order a nuclear bomb dropped on Hiroshima. Your brain can\u2019t multiply, you see: it can only look at a hypothetical image of a broken city and decide it\u2019s not that bad. It can only conjure an image of a barren planet and say \"eh, we had it coming.\"\n\nBut if you unpack the scenario, if you try to comprehend all the lives snuffed out, all the children killed, the final spark of human joy and curiosity extinguished, all our potential squandered\u2026\n\n\nI promise you that the extermination of humankind would be horrific.",
    "tags": [],
    "links to": [
      [
        "Nate Soares",
        "https://mindingourway.com/a-torch-in-darkness/"
      ]
    ]
  }
]